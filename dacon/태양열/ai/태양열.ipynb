{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3888, 10)\n",
      "(52464, 1, 10) (52464, 1) (52464, 1)\n",
      "(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
      "\n",
      "\n",
      "  0.1 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_21 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "2296/2296 [==============================] - 37s 16ms/step - loss: 1.4445 - <lambda>: 1.4445 - val_loss: 1.5479 - val_<lambda>: 1.5479\n",
      "Epoch 2/500\n",
      "2296/2296 [==============================] - 36s 16ms/step - loss: 1.4126 - <lambda>: 1.4126 - val_loss: 1.5175 - val_<lambda>: 1.5175\n",
      "Epoch 3/500\n",
      "2296/2296 [==============================] - 36s 16ms/step - loss: 1.4050 - <lambda>: 1.4050 - val_loss: 1.5490 - val_<lambda>: 1.5490\n",
      "Epoch 4/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3986 - <lambda>: 1.3986 - val_loss: 1.5353 - val_<lambda>: 1.5353\n",
      "Epoch 5/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3871 - <lambda>: 1.3871 - val_loss: 1.6655 - val_<lambda>: 1.6655\n",
      "Epoch 6/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3839 - <lambda>: 1.3839 - val_loss: 1.5304 - val_<lambda>: 1.5304\n",
      "Epoch 7/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3735 - <lambda>: 1.3735 - val_loss: 1.5266 - val_<lambda>: 1.5266\n",
      "Epoch 8/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3736 - <lambda>: 1.3736 - val_loss: 1.5220 - val_<lambda>: 1.5220\n",
      "Epoch 9/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3652 - <lambda>: 1.3652 - val_loss: 1.5161 - val_<lambda>: 1.5161\n",
      "Epoch 10/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3620 - <lambda>: 1.3620 - val_loss: 1.5123 - val_<lambda>: 1.5123\n",
      "Epoch 11/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3603 - <lambda>: 1.3603 - val_loss: 1.4871 - val_<lambda>: 1.4871\n",
      "Epoch 12/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3545 - <lambda>: 1.3545 - val_loss: 1.5214 - val_<lambda>: 1.5214\n",
      "Epoch 13/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3553 - <lambda>: 1.3553 - val_loss: 1.5531 - val_<lambda>: 1.5531\n",
      "Epoch 14/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3479 - <lambda>: 1.3479 - val_loss: 1.5052 - val_<lambda>: 1.5052\n",
      "Epoch 15/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3436 - <lambda>: 1.3436 - val_loss: 1.5044 - val_<lambda>: 1.5044\n",
      "Epoch 16/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3367 - <lambda>: 1.3367 - val_loss: 1.5343 - val_<lambda>: 1.5343\n",
      "Epoch 17/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3333 - <lambda>: 1.3333 - val_loss: 1.5491 - val_<lambda>: 1.5491\n",
      "Epoch 18/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3262 - <lambda>: 1.3262 - val_loss: 1.6312 - val_<lambda>: 1.6312\n",
      "Epoch 19/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3258 - <lambda>: 1.3258 - val_loss: 1.5801 - val_<lambda>: 1.5801\n",
      "Epoch 20/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3217 - <lambda>: 1.3217 - val_loss: 1.5461 - val_<lambda>: 1.5461\n",
      "Epoch 21/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3155 - <lambda>: 1.3155 - val_loss: 1.5287 - val_<lambda>: 1.5287\n",
      "Epoch 22/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3175 - <lambda>: 1.3175 - val_loss: 1.5086 - val_<lambda>: 1.5086\n",
      "Epoch 23/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3112 - <lambda>: 1.3112 - val_loss: 1.5255 - val_<lambda>: 1.5255\n",
      "Epoch 24/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3075 - <lambda>: 1.3075 - val_loss: 1.5264 - val_<lambda>: 1.5264\n",
      "Epoch 25/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3049 - <lambda>: 1.3049 - val_loss: 1.5301 - val_<lambda>: 1.5301\n",
      "Epoch 26/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2973 - <lambda>: 1.2973 - val_loss: 1.5968 - val_<lambda>: 1.5968\n",
      "Epoch 27/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2938 - <lambda>: 1.2938 - val_loss: 1.5637 - val_<lambda>: 1.5637\n",
      "Epoch 28/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2910 - <lambda>: 1.2910 - val_loss: 1.5548 - val_<lambda>: 1.5548\n",
      "Epoch 29/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2855 - <lambda>: 1.2855 - val_loss: 1.6219 - val_<lambda>: 1.6219\n",
      "Epoch 30/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2853 - <lambda>: 1.2853 - val_loss: 1.5517 - val_<lambda>: 1.5517\n",
      "Epoch 31/500\n",
      "2294/2296 [============================>.] - ETA: 0s - loss: 1.2764 - <lambda>: 1.2764\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2762 - <lambda>: 1.2762 - val_loss: 1.5530 - val_<lambda>: 1.5530\n",
      "Epoch 32/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2719 - <lambda>: 1.2719 - val_loss: 1.5757 - val_<lambda>: 1.5757\n",
      "Epoch 33/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2690 - <lambda>: 1.2690 - val_loss: 1.9584 - val_<lambda>: 1.9584\n",
      "Epoch 34/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2590 - <lambda>: 1.2590 - val_loss: 1.6495 - val_<lambda>: 1.6495\n",
      "Epoch 35/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2559 - <lambda>: 1.2559 - val_loss: 1.5577 - val_<lambda>: 1.5577\n",
      "Epoch 36/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2533 - <lambda>: 1.2533 - val_loss: 1.5500 - val_<lambda>: 1.5500\n",
      "Epoch 37/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2490 - <lambda>: 1.2490 - val_loss: 1.5808 - val_<lambda>: 1.5808\n",
      "Epoch 38/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2465 - <lambda>: 1.2465 - val_loss: 1.6122 - val_<lambda>: 1.6122\n",
      "Epoch 39/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2387 - <lambda>: 1.2387 - val_loss: 1.5933 - val_<lambda>: 1.5933\n",
      "Epoch 40/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2403 - <lambda>: 1.2403 - val_loss: 1.6752 - val_<lambda>: 1.6752\n",
      "Epoch 41/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2345 - <lambda>: 1.2345 - val_loss: 1.5858 - val_<lambda>: 1.5858\n",
      "\n",
      "\n",
      "  0.2 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_22 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "2296/2296 [==============================] - 36s 16ms/step - loss: 2.4091 - <lambda>: 2.4091 - val_loss: 2.5343 - val_<lambda>: 2.5343\n",
      "Epoch 2/500\n",
      "2296/2296 [==============================] - 36s 15ms/step - loss: 2.3293 - <lambda>: 2.3293 - val_loss: 2.4607 - val_<lambda>: 2.4607\n",
      "Epoch 3/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3001 - <lambda>: 2.3001 - val_loss: 2.5738 - val_<lambda>: 2.5738\n",
      "Epoch 4/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2847 - <lambda>: 2.2847 - val_loss: 2.4927 - val_<lambda>: 2.4927\n",
      "Epoch 5/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2744 - <lambda>: 2.2744 - val_loss: 2.4995 - val_<lambda>: 2.4995\n",
      "Epoch 6/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2552 - <lambda>: 2.2552 - val_loss: 2.6663 - val_<lambda>: 2.6663\n",
      "Epoch 7/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2421 - <lambda>: 2.2421 - val_loss: 2.5280 - val_<lambda>: 2.5280\n",
      "Epoch 8/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2266 - <lambda>: 2.2266 - val_loss: 2.6640 - val_<lambda>: 2.6640\n",
      "Epoch 9/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2266 - <lambda>: 2.2266 - val_loss: 2.4716 - val_<lambda>: 2.4716\n",
      "Epoch 10/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2117 - <lambda>: 2.2117 - val_loss: 2.5402 - val_<lambda>: 2.5402\n",
      "Epoch 11/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2077 - <lambda>: 2.2077 - val_loss: 2.5348 - val_<lambda>: 2.5348\n",
      "Epoch 12/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2003 - <lambda>: 2.2003 - val_loss: 2.5000 - val_<lambda>: 2.5000\n",
      "Epoch 13/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1910 - <lambda>: 2.1910 - val_loss: 2.5142 - val_<lambda>: 2.5142\n",
      "Epoch 14/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1797 - <lambda>: 2.1797 - val_loss: 2.5137 - val_<lambda>: 2.5137\n",
      "Epoch 15/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1754 - <lambda>: 2.1754 - val_loss: 2.5550 - val_<lambda>: 2.5550\n",
      "Epoch 16/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1786 - <lambda>: 2.1786 - val_loss: 2.4702 - val_<lambda>: 2.4702\n",
      "Epoch 17/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1605 - <lambda>: 2.1605 - val_loss: 2.5277 - val_<lambda>: 2.5277\n",
      "Epoch 18/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1524 - <lambda>: 2.1524 - val_loss: 2.5078 - val_<lambda>: 2.5078\n",
      "Epoch 19/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1425 - <lambda>: 2.1425 - val_loss: 2.5466 - val_<lambda>: 2.5466\n",
      "Epoch 20/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1301 - <lambda>: 2.1301 - val_loss: 2.5085 - val_<lambda>: 2.5085\n",
      "Epoch 21/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1244 - <lambda>: 2.1244 - val_loss: 2.5095 - val_<lambda>: 2.5095\n",
      "Epoch 22/500\n",
      "2293/2296 [============================>.] - ETA: 0s - loss: 2.1178 - <lambda>: 2.1178\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1176 - <lambda>: 2.1176 - val_loss: 2.5148 - val_<lambda>: 2.5148\n",
      "Epoch 23/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0951 - <lambda>: 2.0951 - val_loss: 2.6296 - val_<lambda>: 2.6296\n",
      "Epoch 24/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0948 - <lambda>: 2.0948 - val_loss: 2.4926 - val_<lambda>: 2.4926\n",
      "Epoch 25/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0823 - <lambda>: 2.0823 - val_loss: 2.5585 - val_<lambda>: 2.5585\n",
      "Epoch 26/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0726 - <lambda>: 2.0726 - val_loss: 2.6398 - val_<lambda>: 2.6398\n",
      "Epoch 27/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0659 - <lambda>: 2.0659 - val_loss: 2.5582 - val_<lambda>: 2.5582\n",
      "Epoch 28/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0518 - <lambda>: 2.0518 - val_loss: 2.5350 - val_<lambda>: 2.5350\n",
      "Epoch 29/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0559 - <lambda>: 2.0559 - val_loss: 2.5558 - val_<lambda>: 2.5558\n",
      "Epoch 30/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0479 - <lambda>: 2.0479 - val_loss: 2.7350 - val_<lambda>: 2.7350\n",
      "Epoch 31/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0344 - <lambda>: 2.0344 - val_loss: 2.6216 - val_<lambda>: 2.6216\n",
      "Epoch 32/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0186 - <lambda>: 2.0186 - val_loss: 2.5835 - val_<lambda>: 2.5835\n",
      "\n",
      "\n",
      "  0.3 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_23 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.9462 - <lambda>: 2.9462 - val_loss: 3.2321 - val_<lambda>: 3.2321\n",
      "Epoch 2/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8025 - <lambda>: 2.8025 - val_loss: 3.0434 - val_<lambda>: 3.0434\n",
      "Epoch 3/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7685 - <lambda>: 2.7685 - val_loss: 3.1366 - val_<lambda>: 3.1366\n",
      "Epoch 4/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7487 - <lambda>: 2.7487 - val_loss: 2.9573 - val_<lambda>: 2.9573\n",
      "Epoch 5/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7338 - <lambda>: 2.7338 - val_loss: 3.0424 - val_<lambda>: 3.0424\n",
      "Epoch 6/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7069 - <lambda>: 2.7069 - val_loss: 3.0490 - val_<lambda>: 3.0490\n",
      "Epoch 7/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7012 - <lambda>: 2.7012 - val_loss: 3.0227 - val_<lambda>: 3.0227\n",
      "Epoch 8/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6856 - <lambda>: 2.6856 - val_loss: 3.0095 - val_<lambda>: 3.0095\n",
      "Epoch 9/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6735 - <lambda>: 2.6735 - val_loss: 3.1017 - val_<lambda>: 3.1017\n",
      "Epoch 10/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6697 - <lambda>: 2.6697 - val_loss: 3.0536 - val_<lambda>: 3.0536\n",
      "Epoch 11/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6504 - <lambda>: 2.6504 - val_loss: 3.0038 - val_<lambda>: 3.0038\n",
      "Epoch 12/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6469 - <lambda>: 2.6469 - val_loss: 3.0380 - val_<lambda>: 3.0380\n",
      "Epoch 13/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6400 - <lambda>: 2.6400 - val_loss: 3.0588 - val_<lambda>: 3.0588\n",
      "Epoch 14/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6304 - <lambda>: 2.6304 - val_loss: 3.1036 - val_<lambda>: 3.1036\n",
      "Epoch 15/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6097 - <lambda>: 2.6097 - val_loss: 3.0411 - val_<lambda>: 3.0411\n",
      "Epoch 16/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6013 - <lambda>: 2.6013 - val_loss: 3.0737 - val_<lambda>: 3.0737\n",
      "Epoch 17/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5974 - <lambda>: 2.5974 - val_loss: 3.0584 - val_<lambda>: 3.0584\n",
      "Epoch 18/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5845 - <lambda>: 2.5845 - val_loss: 3.0559 - val_<lambda>: 3.0559\n",
      "Epoch 19/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5700 - <lambda>: 2.5700 - val_loss: 3.0485 - val_<lambda>: 3.0485\n",
      "Epoch 20/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5655 - <lambda>: 2.5655 - val_loss: 3.0239 - val_<lambda>: 3.0239\n",
      "Epoch 21/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5564 - <lambda>: 2.5564 - val_loss: 3.0841 - val_<lambda>: 3.0841\n",
      "Epoch 22/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5418 - <lambda>: 2.5418 - val_loss: 3.1762 - val_<lambda>: 3.1762\n",
      "Epoch 23/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5407 - <lambda>: 2.5407 - val_loss: 3.0929 - val_<lambda>: 3.0929\n",
      "Epoch 24/500\n",
      "2296/2296 [==============================] - ETA: 0s - loss: 2.5271 - <lambda>: 2.5271\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5271 - <lambda>: 2.5271 - val_loss: 3.1200 - val_<lambda>: 3.1200\n",
      "Epoch 25/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5201 - <lambda>: 2.5201 - val_loss: 3.1190 - val_<lambda>: 3.1190\n",
      "Epoch 26/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4986 - <lambda>: 2.4986 - val_loss: 3.0840 - val_<lambda>: 3.0840\n",
      "Epoch 27/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4826 - <lambda>: 2.4826 - val_loss: 3.0645 - val_<lambda>: 3.0645\n",
      "Epoch 28/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4921 - <lambda>: 2.4921 - val_loss: 3.2018 - val_<lambda>: 3.2018\n",
      "Epoch 29/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4739 - <lambda>: 2.4739 - val_loss: 3.1594 - val_<lambda>: 3.1594\n",
      "Epoch 30/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4664 - <lambda>: 2.4664 - val_loss: 3.1168 - val_<lambda>: 3.1168\n",
      "Epoch 31/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4561 - <lambda>: 2.4561 - val_loss: 3.0756 - val_<lambda>: 3.0756\n",
      "Epoch 32/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4487 - <lambda>: 2.4487 - val_loss: 3.1318 - val_<lambda>: 3.1318\n",
      "Epoch 33/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4350 - <lambda>: 2.4350 - val_loss: 3.0932 - val_<lambda>: 3.0932\n",
      "Epoch 34/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4263 - <lambda>: 2.4263 - val_loss: 3.0818 - val_<lambda>: 3.0818\n",
      "\n",
      "\n",
      "  0.4 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_24 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 3.1310 - <lambda>: 3.1310 - val_loss: 3.3535 - val_<lambda>: 3.3535\n",
      "Epoch 2/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.9611 - <lambda>: 2.9611 - val_loss: 3.2803 - val_<lambda>: 3.2803\n",
      "Epoch 3/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.9055 - <lambda>: 2.9055 - val_loss: 3.1176 - val_<lambda>: 3.1176\n",
      "Epoch 4/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8712 - <lambda>: 2.8712 - val_loss: 3.1736 - val_<lambda>: 3.1736\n",
      "Epoch 5/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8567 - <lambda>: 2.8567 - val_loss: 3.2231 - val_<lambda>: 3.2231\n",
      "Epoch 6/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8620 - <lambda>: 2.8620 - val_loss: 3.0791 - val_<lambda>: 3.0791\n",
      "Epoch 7/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8369 - <lambda>: 2.8369 - val_loss: 3.1237 - val_<lambda>: 3.1237\n",
      "Epoch 8/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8235 - <lambda>: 2.8235 - val_loss: 3.1324 - val_<lambda>: 3.1324\n",
      "Epoch 9/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8124 - <lambda>: 2.8124 - val_loss: 3.1840 - val_<lambda>: 3.1840\n",
      "Epoch 10/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8021 - <lambda>: 2.8021 - val_loss: 3.1257 - val_<lambda>: 3.1257\n",
      "Epoch 11/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7930 - <lambda>: 2.7930 - val_loss: 3.1089 - val_<lambda>: 3.1089\n",
      "Epoch 12/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7895 - <lambda>: 2.7895 - val_loss: 3.1129 - val_<lambda>: 3.1129\n",
      "Epoch 13/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7722 - <lambda>: 2.7722 - val_loss: 3.1475 - val_<lambda>: 3.1475\n",
      "Epoch 14/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7699 - <lambda>: 2.7699 - val_loss: 3.1056 - val_<lambda>: 3.1056\n",
      "Epoch 15/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7589 - <lambda>: 2.7589 - val_loss: 3.1670 - val_<lambda>: 3.1670\n",
      "Epoch 16/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7496 - <lambda>: 2.7496 - val_loss: 3.1737 - val_<lambda>: 3.1737\n",
      "Epoch 17/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7350 - <lambda>: 2.7350 - val_loss: 3.2577 - val_<lambda>: 3.2577\n",
      "Epoch 18/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7297 - <lambda>: 2.7297 - val_loss: 3.1732 - val_<lambda>: 3.1732\n",
      "Epoch 19/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7151 - <lambda>: 2.7151 - val_loss: 3.1864 - val_<lambda>: 3.1864\n",
      "Epoch 20/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7099 - <lambda>: 2.7099 - val_loss: 3.1826 - val_<lambda>: 3.1826\n",
      "Epoch 21/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6955 - <lambda>: 2.6955 - val_loss: 3.1859 - val_<lambda>: 3.1859\n",
      "Epoch 22/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6907 - <lambda>: 2.6907 - val_loss: 3.2123 - val_<lambda>: 3.2123\n",
      "Epoch 23/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6676 - <lambda>: 2.6676 - val_loss: 3.2775 - val_<lambda>: 3.2775\n",
      "Epoch 24/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6683 - <lambda>: 2.6683 - val_loss: 3.1666 - val_<lambda>: 3.1666\n",
      "Epoch 25/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6587 - <lambda>: 2.6587 - val_loss: 3.2574 - val_<lambda>: 3.2574\n",
      "Epoch 26/500\n",
      "2294/2296 [============================>.] - ETA: 0s - loss: 2.6588 - <lambda>: 2.6588\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6592 - <lambda>: 2.6592 - val_loss: 3.1820 - val_<lambda>: 3.1820\n",
      "Epoch 27/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6345 - <lambda>: 2.6345 - val_loss: 3.2016 - val_<lambda>: 3.2016\n",
      "Epoch 28/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6166 - <lambda>: 2.6166 - val_loss: 3.3053 - val_<lambda>: 3.3053\n",
      "Epoch 29/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6159 - <lambda>: 2.6159 - val_loss: 3.2274 - val_<lambda>: 3.2274\n",
      "Epoch 30/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6062 - <lambda>: 2.6062 - val_loss: 3.2723 - val_<lambda>: 3.2723\n",
      "Epoch 31/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6017 - <lambda>: 2.6017 - val_loss: 3.3072 - val_<lambda>: 3.3072\n",
      "Epoch 32/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5906 - <lambda>: 2.5906 - val_loss: 3.1930 - val_<lambda>: 3.1930\n",
      "Epoch 33/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5861 - <lambda>: 2.5861 - val_loss: 3.2234 - val_<lambda>: 3.2234\n",
      "Epoch 34/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5726 - <lambda>: 2.5726 - val_loss: 3.3119 - val_<lambda>: 3.3119\n",
      "Epoch 35/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5570 - <lambda>: 2.5570 - val_loss: 3.2261 - val_<lambda>: 3.2261\n",
      "Epoch 36/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5584 - <lambda>: 2.5584 - val_loss: 3.2360 - val_<lambda>: 3.2360\n",
      "\n",
      "\n",
      "  0.5 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_25 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "2296/2296 [==============================] - 36s 16ms/step - loss: 3.0746 - <lambda>: 3.0746 - val_loss: 3.1376 - val_<lambda>: 3.1376\n",
      "Epoch 2/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8591 - <lambda>: 2.8591 - val_loss: 3.0729 - val_<lambda>: 3.0729\n",
      "Epoch 3/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8040 - <lambda>: 2.8040 - val_loss: 3.9153 - val_<lambda>: 3.9153\n",
      "Epoch 4/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7923 - <lambda>: 2.7923 - val_loss: 3.1443 - val_<lambda>: 3.1443\n",
      "Epoch 5/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7616 - <lambda>: 2.7616 - val_loss: 3.1232 - val_<lambda>: 3.1232\n",
      "Epoch 6/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7558 - <lambda>: 2.7558 - val_loss: 3.3166 - val_<lambda>: 3.3166\n",
      "Epoch 7/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7294 - <lambda>: 2.7294 - val_loss: 3.0932 - val_<lambda>: 3.0932\n",
      "Epoch 8/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7265 - <lambda>: 2.7265 - val_loss: 2.9959 - val_<lambda>: 2.9959\n",
      "Epoch 9/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7261 - <lambda>: 2.7261 - val_loss: 2.9974 - val_<lambda>: 2.9974\n",
      "Epoch 10/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7096 - <lambda>: 2.7096 - val_loss: 3.1021 - val_<lambda>: 3.1021\n",
      "Epoch 11/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6975 - <lambda>: 2.6975 - val_loss: 2.9827 - val_<lambda>: 2.9827\n",
      "Epoch 12/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6871 - <lambda>: 2.6871 - val_loss: 3.1050 - val_<lambda>: 3.1050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6801 - <lambda>: 2.6801 - val_loss: 3.0087 - val_<lambda>: 3.0087\n",
      "Epoch 14/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6719 - <lambda>: 2.6719 - val_loss: 2.9955 - val_<lambda>: 2.9955\n",
      "Epoch 15/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6643 - <lambda>: 2.6643 - val_loss: 3.0585 - val_<lambda>: 3.0585\n",
      "Epoch 16/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6531 - <lambda>: 2.6531 - val_loss: 3.0735 - val_<lambda>: 3.0735\n",
      "Epoch 17/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6525 - <lambda>: 2.6525 - val_loss: 2.9867 - val_<lambda>: 2.9867\n",
      "Epoch 18/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6378 - <lambda>: 2.6378 - val_loss: 2.9931 - val_<lambda>: 2.9931\n",
      "Epoch 19/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6333 - <lambda>: 2.6333 - val_loss: 2.9879 - val_<lambda>: 2.9879\n",
      "Epoch 20/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6298 - <lambda>: 2.6298 - val_loss: 3.0782 - val_<lambda>: 3.0782\n",
      "Epoch 21/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6200 - <lambda>: 2.6200 - val_loss: 3.0095 - val_<lambda>: 3.0095\n",
      "Epoch 22/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6085 - <lambda>: 2.6085 - val_loss: 3.1110 - val_<lambda>: 3.1110\n",
      "Epoch 23/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6017 - <lambda>: 2.6017 - val_loss: 3.0792 - val_<lambda>: 3.0792\n",
      "Epoch 24/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5925 - <lambda>: 2.5925 - val_loss: 2.9975 - val_<lambda>: 2.9975\n",
      "Epoch 25/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5860 - <lambda>: 2.5860 - val_loss: 3.1187 - val_<lambda>: 3.1187\n",
      "Epoch 26/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5752 - <lambda>: 2.5752 - val_loss: 3.0727 - val_<lambda>: 3.0727\n",
      "Epoch 27/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5717 - <lambda>: 2.5717 - val_loss: 3.0839 - val_<lambda>: 3.0839\n",
      "Epoch 28/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5659 - <lambda>: 2.5659 - val_loss: 3.0525 - val_<lambda>: 3.0525\n",
      "Epoch 29/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5519 - <lambda>: 2.5519 - val_loss: 3.1826 - val_<lambda>: 3.1826\n",
      "Epoch 30/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5580 - <lambda>: 2.5580 - val_loss: 3.1019 - val_<lambda>: 3.1019\n",
      "Epoch 31/500\n",
      "2294/2296 [============================>.] - ETA: 0s - loss: 2.5495 - <lambda>: 2.5495\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5494 - <lambda>: 2.5494 - val_loss: 3.1133 - val_<lambda>: 3.1133\n",
      "Epoch 32/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5290 - <lambda>: 2.5290 - val_loss: 3.0733 - val_<lambda>: 3.0733\n",
      "Epoch 33/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5171 - <lambda>: 2.5171 - val_loss: 3.1215 - val_<lambda>: 3.1215\n",
      "Epoch 34/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5094 - <lambda>: 2.5094 - val_loss: 3.0746 - val_<lambda>: 3.0746\n",
      "Epoch 35/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5022 - <lambda>: 2.5022 - val_loss: 3.0995 - val_<lambda>: 3.0995\n",
      "Epoch 36/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4874 - <lambda>: 2.4874 - val_loss: 3.0682 - val_<lambda>: 3.0682\n",
      "Epoch 37/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4884 - <lambda>: 2.4884 - val_loss: 3.0690 - val_<lambda>: 3.0690\n",
      "Epoch 38/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4778 - <lambda>: 2.4778 - val_loss: 3.1229 - val_<lambda>: 3.1229\n",
      "Epoch 39/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4761 - <lambda>: 2.4761 - val_loss: 3.1299 - val_<lambda>: 3.1299\n",
      "Epoch 40/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4741 - <lambda>: 2.4741 - val_loss: 3.0983 - val_<lambda>: 3.0983\n",
      "Epoch 41/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4638 - <lambda>: 2.4638 - val_loss: 3.2018 - val_<lambda>: 3.2018\n",
      "\n",
      "\n",
      "  0.6 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_26 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "2296/2296 [==============================] - 36s 16ms/step - loss: 2.7967 - <lambda>: 2.7967 - val_loss: 2.9073 - val_<lambda>: 2.9073\n",
      "Epoch 2/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5651 - <lambda>: 2.5651 - val_loss: 2.9497 - val_<lambda>: 2.9497\n",
      "Epoch 3/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5036 - <lambda>: 2.5036 - val_loss: 2.7281 - val_<lambda>: 2.7281\n",
      "Epoch 4/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4843 - <lambda>: 2.4843 - val_loss: 2.7279 - val_<lambda>: 2.7279\n",
      "Epoch 5/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4559 - <lambda>: 2.4559 - val_loss: 2.6678 - val_<lambda>: 2.6678\n",
      "Epoch 6/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4527 - <lambda>: 2.4527 - val_loss: 2.6841 - val_<lambda>: 2.6841\n",
      "Epoch 7/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4396 - <lambda>: 2.4396 - val_loss: 2.6629 - val_<lambda>: 2.6629\n",
      "Epoch 8/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4283 - <lambda>: 2.4283 - val_loss: 2.7576 - val_<lambda>: 2.7576\n",
      "Epoch 9/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4125 - <lambda>: 2.4125 - val_loss: 2.7591 - val_<lambda>: 2.7591\n",
      "Epoch 10/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4143 - <lambda>: 2.4143 - val_loss: 2.6490 - val_<lambda>: 2.6490\n",
      "Epoch 11/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4054 - <lambda>: 2.4054 - val_loss: 2.6763 - val_<lambda>: 2.6763\n",
      "Epoch 12/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3951 - <lambda>: 2.3951 - val_loss: 2.7150 - val_<lambda>: 2.7150\n",
      "Epoch 13/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3869 - <lambda>: 2.3869 - val_loss: 2.7208 - val_<lambda>: 2.7208\n",
      "Epoch 14/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3745 - <lambda>: 2.3745 - val_loss: 2.7292 - val_<lambda>: 2.7292\n",
      "Epoch 15/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3642 - <lambda>: 2.3642 - val_loss: 2.6764 - val_<lambda>: 2.6764\n",
      "Epoch 16/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3652 - <lambda>: 2.3652 - val_loss: 2.6803 - val_<lambda>: 2.6803\n",
      "Epoch 17/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3671 - <lambda>: 2.3671 - val_loss: 2.6770 - val_<lambda>: 2.6770\n",
      "Epoch 18/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3549 - <lambda>: 2.3549 - val_loss: 2.7239 - val_<lambda>: 2.7239\n",
      "Epoch 19/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3435 - <lambda>: 2.3435 - val_loss: 2.6669 - val_<lambda>: 2.6669\n",
      "Epoch 20/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3332 - <lambda>: 2.3332 - val_loss: 2.7245 - val_<lambda>: 2.7245\n",
      "Epoch 21/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3355 - <lambda>: 2.3355 - val_loss: 2.6901 - val_<lambda>: 2.6901\n",
      "Epoch 22/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3305 - <lambda>: 2.3305 - val_loss: 2.7383 - val_<lambda>: 2.7383\n",
      "Epoch 23/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3180 - <lambda>: 2.3180 - val_loss: 2.6928 - val_<lambda>: 2.6928\n",
      "Epoch 24/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3037 - <lambda>: 2.3037 - val_loss: 2.6717 - val_<lambda>: 2.6717\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3069 - <lambda>: 2.3069 - val_loss: 2.7001 - val_<lambda>: 2.7001\n",
      "Epoch 26/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2979 - <lambda>: 2.2979 - val_loss: 2.7456 - val_<lambda>: 2.7456\n",
      "Epoch 27/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2919 - <lambda>: 2.2919 - val_loss: 2.7322 - val_<lambda>: 2.7322\n",
      "Epoch 28/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2943 - <lambda>: 2.2943 - val_loss: 2.7319 - val_<lambda>: 2.7319\n",
      "Epoch 29/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2829 - <lambda>: 2.2829 - val_loss: 2.7252 - val_<lambda>: 2.7252\n",
      "Epoch 30/500\n",
      "2295/2296 [============================>.] - ETA: 0s - loss: 2.2793 - <lambda>: 2.2793\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2792 - <lambda>: 2.2792 - val_loss: 2.7072 - val_<lambda>: 2.7072\n",
      "Epoch 31/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2579 - <lambda>: 2.2579 - val_loss: 2.7552 - val_<lambda>: 2.7552\n",
      "Epoch 32/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2559 - <lambda>: 2.2559 - val_loss: 2.7322 - val_<lambda>: 2.7322\n",
      "Epoch 33/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2499 - <lambda>: 2.2499 - val_loss: 2.7137 - val_<lambda>: 2.7137\n",
      "Epoch 34/500\n",
      "2296/2296 [==============================] - 39s 17ms/step - loss: 2.2479 - <lambda>: 2.2479 - val_loss: 2.6869 - val_<lambda>: 2.6869\n",
      "Epoch 35/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2413 - <lambda>: 2.2413 - val_loss: 2.7267 - val_<lambda>: 2.7267\n",
      "Epoch 36/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2340 - <lambda>: 2.2340 - val_loss: 2.7711 - val_<lambda>: 2.7711\n",
      "Epoch 37/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2324 - <lambda>: 2.2324 - val_loss: 2.7317 - val_<lambda>: 2.7317\n",
      "Epoch 38/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2204 - <lambda>: 2.2204 - val_loss: 2.7994 - val_<lambda>: 2.7994\n",
      "Epoch 39/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2330 - <lambda>: 2.2330 - val_loss: 2.7115 - val_<lambda>: 2.7115\n",
      "Epoch 40/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2153 - <lambda>: 2.2153 - val_loss: 2.7604 - val_<lambda>: 2.7604\n",
      "\n",
      "\n",
      "  0.7 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_27 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "2296/2296 [==============================] - 36s 16ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 2/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 3/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 4/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 5/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 6/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 7/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 8/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 9/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 10/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 11/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 12/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 13/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 14/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 15/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 16/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 17/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 18/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 19/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 20/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 21/500\n",
      "2296/2296 [==============================] - ETA: 0s - loss: 12.2223 - <lambda>: 12.2223\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 22/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 23/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 24/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 25/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 26/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 27/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 28/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 29/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 30/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 31/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "\n",
      "\n",
      "  0.9 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_28 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "2296/2296 [==============================] - 36s 15ms/step - loss: 1.2355 - <lambda>: 1.2355 - val_loss: 1.0072 - val_<lambda>: 1.0072\n",
      "Epoch 2/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.9423 - <lambda>: 0.9423 - val_loss: 0.9538 - val_<lambda>: 0.9538\n",
      "Epoch 3/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.9090 - <lambda>: 0.9090 - val_loss: 0.9145 - val_<lambda>: 0.9145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8861 - <lambda>: 0.8861 - val_loss: 0.9712 - val_<lambda>: 0.9712\n",
      "Epoch 5/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8677 - <lambda>: 0.8677 - val_loss: 0.9468 - val_<lambda>: 0.9468\n",
      "Epoch 6/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8521 - <lambda>: 0.8521 - val_loss: 0.9640 - val_<lambda>: 0.9640\n",
      "Epoch 7/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8585 - <lambda>: 0.8585 - val_loss: 0.9001 - val_<lambda>: 0.9001\n",
      "Epoch 8/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8379 - <lambda>: 0.8379 - val_loss: 0.9089 - val_<lambda>: 0.9089\n",
      "Epoch 9/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8309 - <lambda>: 0.8309 - val_loss: 0.9308 - val_<lambda>: 0.9308\n",
      "Epoch 10/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8328 - <lambda>: 0.8328 - val_loss: 1.0083 - val_<lambda>: 1.0083\n",
      "Epoch 11/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8235 - <lambda>: 0.8235 - val_loss: 0.8654 - val_<lambda>: 0.8654\n",
      "Epoch 12/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8254 - <lambda>: 0.8254 - val_loss: 0.8719 - val_<lambda>: 0.8719\n",
      "Epoch 13/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8176 - <lambda>: 0.8176 - val_loss: 0.8733 - val_<lambda>: 0.8733\n",
      "Epoch 14/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8122 - <lambda>: 0.8122 - val_loss: 0.9301 - val_<lambda>: 0.9301\n",
      "Epoch 15/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8053 - <lambda>: 0.8053 - val_loss: 0.9027 - val_<lambda>: 0.9027\n",
      "Epoch 16/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8061 - <lambda>: 0.8061 - val_loss: 0.8806 - val_<lambda>: 0.8806\n",
      "Epoch 17/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8003 - <lambda>: 0.8003 - val_loss: 0.9193 - val_<lambda>: 0.9193\n",
      "Epoch 18/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8028 - <lambda>: 0.8028 - val_loss: 0.8861 - val_<lambda>: 0.8861\n",
      "Epoch 19/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7907 - <lambda>: 0.7907 - val_loss: 0.9116 - val_<lambda>: 0.9116\n",
      "Epoch 20/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7941 - <lambda>: 0.7941 - val_loss: 0.9154 - val_<lambda>: 0.9154\n",
      "Epoch 21/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7894 - <lambda>: 0.7894 - val_loss: 0.8777 - val_<lambda>: 0.8777\n",
      "Epoch 22/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7901 - <lambda>: 0.7901 - val_loss: 0.9332 - val_<lambda>: 0.9332\n",
      "Epoch 23/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7840 - <lambda>: 0.7840 - val_loss: 0.9032 - val_<lambda>: 0.9032\n",
      "Epoch 24/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7836 - <lambda>: 0.7836 - val_loss: 0.8695 - val_<lambda>: 0.8695\n",
      "Epoch 25/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7807 - <lambda>: 0.7807 - val_loss: 0.8623 - val_<lambda>: 0.8623\n",
      "Epoch 26/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7792 - <lambda>: 0.7792 - val_loss: 0.9225 - val_<lambda>: 0.9225\n",
      "Epoch 27/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7750 - <lambda>: 0.7750 - val_loss: 0.8670 - val_<lambda>: 0.8670\n",
      "Epoch 28/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7769 - <lambda>: 0.7769 - val_loss: 0.8729 - val_<lambda>: 0.8729\n",
      "Epoch 29/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7760 - <lambda>: 0.7760 - val_loss: 0.9536 - val_<lambda>: 0.9536\n",
      "Epoch 30/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7716 - <lambda>: 0.7716 - val_loss: 0.8866 - val_<lambda>: 0.8866\n",
      "Epoch 31/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7730 - <lambda>: 0.7730 - val_loss: 0.8957 - val_<lambda>: 0.8957\n",
      "Epoch 32/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7733 - <lambda>: 0.7733 - val_loss: 0.9244 - val_<lambda>: 0.9244\n",
      "Epoch 33/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7691 - <lambda>: 0.7691 - val_loss: 0.8761 - val_<lambda>: 0.8761\n",
      "Epoch 34/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7730 - <lambda>: 0.7730 - val_loss: 0.8604 - val_<lambda>: 0.8604\n",
      "Epoch 35/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7663 - <lambda>: 0.7663 - val_loss: 0.9249 - val_<lambda>: 0.9249\n",
      "Epoch 36/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7657 - <lambda>: 0.7657 - val_loss: 0.8713 - val_<lambda>: 0.8713\n",
      "Epoch 37/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7653 - <lambda>: 0.7653 - val_loss: 0.8678 - val_<lambda>: 0.8678\n",
      "Epoch 38/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7589 - <lambda>: 0.7589 - val_loss: 0.8924 - val_<lambda>: 0.8924\n",
      "Epoch 39/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7635 - <lambda>: 0.7635 - val_loss: 0.8685 - val_<lambda>: 0.8685\n",
      "Epoch 40/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7568 - <lambda>: 0.7568 - val_loss: 0.9575 - val_<lambda>: 0.9575\n",
      "Epoch 41/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7650 - <lambda>: 0.7650 - val_loss: 0.8779 - val_<lambda>: 0.8779\n",
      "Epoch 42/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7586 - <lambda>: 0.7586 - val_loss: 0.8636 - val_<lambda>: 0.8636\n",
      "Epoch 43/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7514 - <lambda>: 0.7514 - val_loss: 0.9216 - val_<lambda>: 0.9216\n",
      "Epoch 44/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7529 - <lambda>: 0.7529 - val_loss: 0.8878 - val_<lambda>: 0.8878\n",
      "Epoch 45/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7586 - <lambda>: 0.7586 - val_loss: 0.8838 - val_<lambda>: 0.8838\n",
      "Epoch 46/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7504 - <lambda>: 0.7504 - val_loss: 0.9811 - val_<lambda>: 0.9811\n",
      "Epoch 47/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7529 - <lambda>: 0.7529 - val_loss: 0.8614 - val_<lambda>: 0.8614\n",
      "Epoch 48/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7513 - <lambda>: 0.7513 - val_loss: 0.8673 - val_<lambda>: 0.8673\n",
      "Epoch 49/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7502 - <lambda>: 0.7502 - val_loss: 0.9134 - val_<lambda>: 0.9134\n",
      "Epoch 50/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7476 - <lambda>: 0.7476 - val_loss: 0.9237 - val_<lambda>: 0.9237\n",
      "Epoch 51/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7519 - <lambda>: 0.7519 - val_loss: 0.8794 - val_<lambda>: 0.8794\n",
      "Epoch 52/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7454 - <lambda>: 0.7454 - val_loss: 0.8822 - val_<lambda>: 0.8822\n",
      "Epoch 53/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7476 - <lambda>: 0.7476 - val_loss: 0.8810 - val_<lambda>: 0.8810\n",
      "Epoch 54/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7453 - <lambda>: 0.7453 - val_loss: 0.8586 - val_<lambda>: 0.8586\n",
      "Epoch 55/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7431 - <lambda>: 0.7431 - val_loss: 0.8648 - val_<lambda>: 0.8648\n",
      "Epoch 56/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7427 - <lambda>: 0.7427 - val_loss: 0.8648 - val_<lambda>: 0.8648\n",
      "Epoch 57/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7466 - <lambda>: 0.7466 - val_loss: 0.8982 - val_<lambda>: 0.8982\n",
      "Epoch 58/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7467 - <lambda>: 0.7467 - val_loss: 0.8734 - val_<lambda>: 0.8734\n",
      "Epoch 59/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7438 - <lambda>: 0.7438 - val_loss: 0.9229 - val_<lambda>: 0.9229\n",
      "Epoch 60/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7442 - <lambda>: 0.7442 - val_loss: 0.9274 - val_<lambda>: 0.9274\n",
      "Epoch 61/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7410 - <lambda>: 0.7410 - val_loss: 0.9350 - val_<lambda>: 0.9350\n",
      "Epoch 62/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7383 - <lambda>: 0.7383 - val_loss: 0.8773 - val_<lambda>: 0.8773\n",
      "Epoch 63/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7376 - <lambda>: 0.7376 - val_loss: 0.8781 - val_<lambda>: 0.8781\n",
      "Epoch 64/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7377 - <lambda>: 0.7377 - val_loss: 0.9101 - val_<lambda>: 0.9101\n",
      "Epoch 65/500\n",
      "2296/2296 [==============================] - 38s 17ms/step - loss: 0.7375 - <lambda>: 0.7375 - val_loss: 0.9113 - val_<lambda>: 0.9113\n",
      "Epoch 66/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7338 - <lambda>: 0.7338 - val_loss: 0.8820 - val_<lambda>: 0.8820\n",
      "Epoch 67/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7341 - <lambda>: 0.7341 - val_loss: 0.9009 - val_<lambda>: 0.9009\n",
      "Epoch 68/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7352 - <lambda>: 0.7352 - val_loss: 0.8760 - val_<lambda>: 0.8760\n",
      "Epoch 69/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7342 - <lambda>: 0.7342 - val_loss: 0.8801 - val_<lambda>: 0.8801\n",
      "Epoch 70/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7335 - <lambda>: 0.7335 - val_loss: 0.8917 - val_<lambda>: 0.8917\n",
      "Epoch 71/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7327 - <lambda>: 0.7327 - val_loss: 0.9052 - val_<lambda>: 0.9052\n",
      "Epoch 72/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7297 - <lambda>: 0.7297 - val_loss: 0.8906 - val_<lambda>: 0.8906\n",
      "Epoch 73/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7310 - <lambda>: 0.7310 - val_loss: 0.9089 - val_<lambda>: 0.9089\n",
      "Epoch 74/500\n",
      "2295/2296 [============================>.] - ETA: 0s - loss: 0.7324 - <lambda>: 0.7324\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7325 - <lambda>: 0.7325 - val_loss: 0.9005 - val_<lambda>: 0.9005\n",
      "Epoch 75/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7253 - <lambda>: 0.7253 - val_loss: 0.9224 - val_<lambda>: 0.9224\n",
      "Epoch 76/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7239 - <lambda>: 0.7239 - val_loss: 0.8949 - val_<lambda>: 0.8949\n",
      "Epoch 77/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7219 - <lambda>: 0.7219 - val_loss: 0.8850 - val_<lambda>: 0.8850\n",
      "Epoch 78/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7216 - <lambda>: 0.7216 - val_loss: 0.8718 - val_<lambda>: 0.8718\n",
      "Epoch 79/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7201 - <lambda>: 0.7201 - val_loss: 0.9207 - val_<lambda>: 0.9207\n",
      "Epoch 80/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7202 - <lambda>: 0.7202 - val_loss: 0.9732 - val_<lambda>: 0.9732\n",
      "Epoch 81/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7212 - <lambda>: 0.7212 - val_loss: 0.9142 - val_<lambda>: 0.9142\n",
      "Epoch 82/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7201 - <lambda>: 0.7201 - val_loss: 0.8993 - val_<lambda>: 0.8993\n",
      "Epoch 83/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7197 - <lambda>: 0.7197 - val_loss: 0.8808 - val_<lambda>: 0.8808\n",
      "Epoch 84/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7180 - <lambda>: 0.7180 - val_loss: 0.8847 - val_<lambda>: 0.8847\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must have equal len keys and value when setting with an ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-dc2cf828a259>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[0mdf_temp2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_temp2\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[0mnum_temp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_temp2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m \u001b[0msubmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Day8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"q_0.1\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_temp2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;31m##??????????????     ?????????   \u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[0miloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"iloc\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m         \u001b[0miloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   1727\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1728\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0milocs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1729\u001b[1;33m                         raise ValueError(\n\u001b[0m\u001b[0;32m   1730\u001b[0m                             \u001b[1;34m\"Must have equal len keys and value \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m                             \u001b[1;34m\"when setting with an ndarray\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Must have equal len keys and value when setting with an ndarray"
     ]
    }
   ],
   "source": [
    "##### ~~~!!!!! >o< ^o^\n",
    "###T-Td  (GHI )\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "import tensorflow.keras.backend as K\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model,Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPool1D,Flatten, Reshape, LSTM,Input,concatenate,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from lightgbm import LGBMRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#  \n",
    "train = pd.read_csv('./train/train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "#1. DATA\n",
    "\n",
    "# GHI column  \n",
    "def Add_features(data):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "# train data column\n",
    "def preprocess_data(data, is_train=True):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "    temp = data.copy()\n",
    "    temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "    if is_train==True:          \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')   #  Target\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill') #  Target\n",
    "        temp = temp.dropna()    #  \n",
    "        return temp.iloc[:-96]  #   . (   )\n",
    "\n",
    "    elif is_train == False:\n",
    "        temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "        return temp.iloc[-48:, :]\n",
    "\n",
    "df_train = preprocess_data(train)\n",
    "# print(df_train.shape)   # (52464, 10)\n",
    "# print(df_train[:48])\n",
    "\n",
    "#-------------test \n",
    "df_test = []\n",
    "for i in range(81):\n",
    "    file_path = 'test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    df_test.append(temp)\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "test = np.array(df_test)\n",
    "print(df_test.shape) # (3888, 8)\n",
    "x_test = df_test.to_numpy()\n",
    "\n",
    "###=====================================================================\n",
    "#  () --> y1, y2    \n",
    "def split_xy(dataset, time_steps) :\n",
    "    x, y1, y2 = [],[],[]\n",
    "    for i in range(len(dataset)) :\n",
    "        x_end = i + time_steps\n",
    "        if x_end > len(dataset) :\n",
    "            break\n",
    "        tmp_x = dataset[i:x_end, :-2] # ['Hour' ~ 'T']\n",
    "        tmp_y1 = dataset[x_end-1:x_end,-2] # Target1\n",
    "        tmp_y2 = dataset[x_end-1:x_end,-1]   # Target2\n",
    "        x.append(tmp_x)\n",
    "        y1.append(tmp_y1)\n",
    "        y2.append(tmp_y2)\n",
    "    return np.array(x), np.array(y1), np.array(y2)\n",
    "\n",
    "X = df_train.to_numpy()\n",
    "# print(X.shape)      # (52464, 10)\n",
    "x,y1,y2 = split_xy(X,1)\n",
    "print(x.shape, y1.shape, y2.shape) #(52464, 1, 8) (52464, 1) (52464, 1)\n",
    "\n",
    "#####===========\n",
    "x_train, x_val, y1_train, y1_val, y2_train, y2_val = train_test_split(x,y1,y2,\n",
    "                           train_size = 0.7,shuffle = False, random_state = 0)\n",
    "\n",
    "# print(x_train.shape,x_val.shape) #(36724, 1, 8) (15740, 1, 8)\n",
    "print(y1_train.shape, y1_val.shape, y2_train.shape, y2_val.shape) #(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
    "x_train= x_train.reshape(36724*1, 10)\n",
    "x_val= x_val.reshape(15740*1, 10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "x_train= x_train.reshape(36724, 1, 10)\n",
    "x_val= x_val.reshape(15740, 1, 10)\n",
    "x_test= x_test.reshape(3888, 1, 10)\n",
    "# print(x_train.shape,x_val.shape,x_test.shape) #(36724, 1, 8) (15740, 1, 8) (3888, 1, 8)\n",
    "\n",
    "#  : Quantile loss definition\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "    err = (y_true - y_pred)\n",
    "    return K.mean(K.maximum(q*err, (q-1)*err), axis=-1)\n",
    "\n",
    "q = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "#2. \n",
    "\n",
    "def modeling() :\n",
    "    inputs = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    adam = Adam(learning_rate=0.003)\n",
    "    conv = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Flatten()(conv)\n",
    "\n",
    "    conv1 = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Flatten()(conv1)\n",
    "\n",
    "    conv2 = Conv1D(filters = 256,kernel_size=3,activation='relu',padding='same')(inputs)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 64,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 32,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Flatten()(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters = 256,kernel_size=1,activation='relu',padding='same')(inputs)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 64,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 32,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Flatten()(conv3)\n",
    "    \n",
    "    lstm = GRU(256,activation='relu')(inputs)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    concat = concatenate([conv,conv1,conv2,conv3,lstm])\n",
    "    dense = Dense(128,activation='relu')(concat)\n",
    "    dense = Dense(64,activation='relu')(dense)\n",
    "    dense = Dense(32,activation='relu')(dense)\n",
    "    dense = Dense(8,activation='relu')(dense)\n",
    "    outputs = Dense(1,activation='relu')(dense)\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model\n",
    "# 3. , \n",
    "#####========,  , predict\n",
    "#y1, y2  \n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 30)\n",
    "lr = ReduceLROnPlateau(monitor = 'val_loss', patience = 20, factor = 0.9, verbose = 1)\n",
    "epochs = 500\n",
    "bs = 16\n",
    "\n",
    "# !!\n",
    "# x = []\n",
    "for i in q:\n",
    "    if q!=0.5:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models2/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = lambda y_true,y_pred: quantile_loss(i,y_true,y_pred), optimizer = 'adam', metrics = [lambda y,y_pred: quantile_loss(i,y,y_pred)])\n",
    "        model.fit(x_train,y1_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y1_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "    else:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models2/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = 'mae', optimizer = 'adam')\n",
    "        model.fit(x_train,y1_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y1_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "df_temp1 = pd.concat(x, axis = 1)\n",
    "df_temp1[df_temp1<0] = 0\n",
    "num_temp1 = df_temp1.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = num_temp1\n",
    "\n",
    "x = []\n",
    "# !!\n",
    "for i in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.9]:\n",
    "    if q!=0.5:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models2/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = lambda y_true,y_pred: quantile_loss(i,y_true,y_pred), optimizer = 'adam', metrics = [lambda y,y_pred: quantile_loss(i,y,y_pred)])\n",
    "        model.fit(x_train,y2_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y2_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "    else:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models2/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = 'mae', optimizer = 'adam')\n",
    "        model.fit(x_train,y2_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y2_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "df_temp2 = pd.concat(x, axis = 1)\n",
    "df_temp2[df_temp2<0] = 0\n",
    "num_temp2 = df_temp2.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = num_temp2\n",
    "\n",
    "##??????????????     ?????????   \n",
    "submission.to_csv('models2/upgrade_sub_0125__2.csv', index = False)\n",
    "##  135  -- 1.94653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3888, 10)\n",
      "(52464, 1, 10) (52464, 1) (52464, 1)\n",
      "(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
      "\n",
      "\n",
      "  0.1 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_11 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.2 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_12 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.3 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_13 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.4 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_14 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.5 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_15 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.6 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_16 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.7 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_17 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.8 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_18 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.9 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_19 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.1 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_21 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.2 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_22 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.3 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_23 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.4 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_24 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.5 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_25 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.6 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_26 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.7 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_27 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.8 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_20 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.9 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_28 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "##### ~~~!!!!! >o< ^o^\n",
    "###T-Td  (GHI )\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "import tensorflow.keras.backend as K\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model,Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPool1D,Flatten, Reshape, LSTM,Input,concatenate,BatchNormalization,LayerNormalization,GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from lightgbm import LGBMRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#  \n",
    "train = pd.read_csv('./train/train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "#1. DATA\n",
    "\n",
    "# GHI column  \n",
    "def Add_features(data):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "# train data column\n",
    "def preprocess_data(data, is_train=True):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "    temp = data.copy()\n",
    "    temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "    if is_train==True:          \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')   #  Target\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill') #  Target\n",
    "        temp = temp.dropna()    #  \n",
    "        return temp.iloc[:-96]  #   . (   )\n",
    "\n",
    "    elif is_train == False:\n",
    "        temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "        return temp.iloc[-48:, :]\n",
    "\n",
    "df_train = preprocess_data(train)\n",
    "# print(df_train.shape)   # (52464, 10)\n",
    "# print(df_train[:48])\n",
    "\n",
    "#-------------test \n",
    "df_test = []\n",
    "for i in range(81):\n",
    "    file_path = 'test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    df_test.append(temp)\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "test = np.array(df_test)\n",
    "print(df_test.shape) # (3888, 8)\n",
    "x_test = df_test.to_numpy()\n",
    "\n",
    "###=====================================================================\n",
    "#  () --> y1, y2    \n",
    "def split_xy(dataset, time_steps) :\n",
    "    x, y1, y2 = [],[],[]\n",
    "    for i in range(len(dataset)) :\n",
    "        x_end = i + time_steps\n",
    "        if x_end > len(dataset) :\n",
    "            break\n",
    "        tmp_x = dataset[i:x_end, :-2] # ['Hour' ~ 'T']\n",
    "        tmp_y1 = dataset[x_end-1:x_end,-2] # Target1\n",
    "        tmp_y2 = dataset[x_end-1:x_end,-1]   # Target2\n",
    "        x.append(tmp_x)\n",
    "        y1.append(tmp_y1)\n",
    "        y2.append(tmp_y2)\n",
    "    return np.array(x), np.array(y1), np.array(y2)\n",
    "\n",
    "X = df_train.to_numpy()\n",
    "# print(X.shape)      # (52464, 10)\n",
    "x,y1,y2 = split_xy(X,1)\n",
    "print(x.shape, y1.shape, y2.shape) #(52464, 1, 8) (52464, 1) (52464, 1)\n",
    "\n",
    "#####===========\n",
    "x_train, x_val, y1_train, y1_val, y2_train, y2_val = train_test_split(x,y1,y2,\n",
    "                           train_size = 0.7,shuffle = False, random_state = 0)\n",
    "\n",
    "# print(x_train.shape,x_val.shape) #(36724, 1, 8) (15740, 1, 8)\n",
    "print(y1_train.shape, y1_val.shape, y2_train.shape, y2_val.shape) #(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
    "x_train= x_train.reshape(36724*1, 10)\n",
    "x_val= x_val.reshape(15740*1, 10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "x_train= x_train.reshape(36724, 1, 10)\n",
    "x_val= x_val.reshape(15740, 1, 10)\n",
    "x_test= x_test.reshape(3888, 1, 10)\n",
    "# print(x_train.shape,x_val.shape,x_test.shape) #(36724, 1, 8) (15740, 1, 8) (3888, 1, 8)\n",
    "\n",
    "#  : Quantile loss definition\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "    err = (y_true - y_pred)\n",
    "    return K.mean(K.maximum(q*err, (q-1)*err), axis=-1)\n",
    "\n",
    "q = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "#2. \n",
    "\n",
    "def modeling() :\n",
    "    inputs = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    adam = Adam(learning_rate=0.003)\n",
    "    conv = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Flatten()(conv)\n",
    "\n",
    "    conv1 = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Flatten()(conv1)\n",
    "\n",
    "    conv2 = Conv1D(filters = 256,kernel_size=3,activation='relu',padding='same')(inputs)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 64,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 32,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Flatten()(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters = 256,kernel_size=1,activation='relu',padding='same')(inputs)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 64,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 32,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Flatten()(conv3)\n",
    "    \n",
    "    lstm = GRU(256,activation='relu')(inputs)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    concat = concatenate([conv,conv1,conv2,conv3,lstm])\n",
    "    dense = Dense(128,activation='relu')(concat)\n",
    "    dense = Dense(64,activation='relu')(dense)\n",
    "    dense = Dense(32,activation='relu')(dense)\n",
    "    dense = Dense(8,activation='relu')(dense)\n",
    "    outputs = Dense(1,activation='relu')(dense)\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# 3. , \n",
    "#####========,  , predict\n",
    "#y1, y2  \n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "lr = ReduceLROnPlateau(monitor = 'val_loss', patience = 5, factor = 0.9, verbose = 1)\n",
    "epochs = 200\n",
    "bs = 512\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = []\n",
    "for i in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "    filepath_cp = f'models2/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "    model = load_model(filepath_cp,compile=False)\n",
    "    pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "    x.append(pred)\n",
    "df_temp1 = pd.concat(x, axis = 1)\n",
    "df_temp1[df_temp1<0] = 0\n",
    "num_temp1 = df_temp1.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = num_temp1\n",
    "\n",
    "x = []\n",
    "# !!\n",
    "for i in q:\n",
    "    print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "    filepath_cp = f'models2/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "    model = load_model(filepath_cp,compile=False)\n",
    "    pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "    x.append(pred)\n",
    "df_temp2 = pd.concat(x, axis = 1)\n",
    "df_temp2[df_temp2<0] = 0\n",
    "num_temp2 = df_temp2.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = num_temp2\n",
    "submission.to_csv('models2/upgrade_sub_0125_.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3888, 10)\n",
      "(52464, 1, 10) (52464, 1) (52464, 1)\n",
      "(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
      "\n",
      "\n",
      "  0.1 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "287/287 [==============================] - 7s 23ms/step - loss: 1.4104 - <lambda>: 1.4104 - val_loss: 1.5489 - val_<lambda>: 1.5489\n",
      "Epoch 2/500\n",
      "287/287 [==============================] - 6s 19ms/step - loss: 1.3663 - <lambda>: 1.3663 - val_loss: 1.5271 - val_<lambda>: 1.5271\n",
      "Epoch 3/500\n",
      "287/287 [==============================] - 6s 19ms/step - loss: 1.3508 - <lambda>: 1.3508 - val_loss: 1.5218 - val_<lambda>: 1.5218\n",
      "Epoch 4/500\n",
      "287/287 [==============================] - 6s 19ms/step - loss: 1.3474 - <lambda>: 1.3474 - val_loss: 1.5084 - val_<lambda>: 1.5084\n",
      "Epoch 5/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3398 - <lambda>: 1.3398 - val_loss: 1.6022 - val_<lambda>: 1.6022\n",
      "Epoch 6/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3336 - <lambda>: 1.3336 - val_loss: 1.5363 - val_<lambda>: 1.5363\n",
      "Epoch 7/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3225 - <lambda>: 1.3225 - val_loss: 1.5131 - val_<lambda>: 1.5131\n",
      "Epoch 8/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3251 - <lambda>: 1.3251 - val_loss: 1.5677 - val_<lambda>: 1.5677\n",
      "Epoch 9/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3193 - <lambda>: 1.3193 - val_loss: 1.5229 - val_<lambda>: 1.5229\n",
      "Epoch 10/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3159 - <lambda>: 1.3159 - val_loss: 1.5131 - val_<lambda>: 1.5131\n",
      "Epoch 11/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3070 - <lambda>: 1.3070 - val_loss: 1.5586 - val_<lambda>: 1.5586\n",
      "Epoch 12/500\n",
      "287/287 [==============================] - 6s 20ms/step - loss: 1.3089 - <lambda>: 1.3089 - val_loss: 1.5554 - val_<lambda>: 1.5554\n",
      "Epoch 13/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3002 - <lambda>: 1.3002 - val_loss: 1.5267 - val_<lambda>: 1.5267\n",
      "Epoch 14/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2942 - <lambda>: 1.2942 - val_loss: 1.5503 - val_<lambda>: 1.5503\n",
      "Epoch 15/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2881 - <lambda>: 1.2881 - val_loss: 1.5281 - val_<lambda>: 1.5281\n",
      "Epoch 16/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2846 - <lambda>: 1.2846 - val_loss: 1.5435 - val_<lambda>: 1.5435\n",
      "Epoch 17/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2851 - <lambda>: 1.2851 - val_loss: 1.5556 - val_<lambda>: 1.5556\n",
      "Epoch 18/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2817 - <lambda>: 1.2817 - val_loss: 1.5462 - val_<lambda>: 1.5462\n",
      "Epoch 19/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2774 - <lambda>: 1.2774 - val_loss: 1.5250 - val_<lambda>: 1.5250\n",
      "Epoch 20/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2696 - <lambda>: 1.2696 - val_loss: 1.5292 - val_<lambda>: 1.5292\n",
      "Epoch 21/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2655 - <lambda>: 1.2655 - val_loss: 1.5400 - val_<lambda>: 1.5400\n",
      "Epoch 22/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2563 - <lambda>: 1.2563 - val_loss: 1.5305 - val_<lambda>: 1.5305\n",
      "Epoch 23/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2473 - <lambda>: 1.2473 - val_loss: 1.5479 - val_<lambda>: 1.5479\n",
      "Epoch 24/500\n",
      "287/287 [==============================] - ETA: 0s - loss: 1.2498 - <lambda>: 1.2498\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2498 - <lambda>: 1.2498 - val_loss: 1.5838 - val_<lambda>: 1.5838\n",
      "Epoch 25/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2391 - <lambda>: 1.2391 - val_loss: 1.5884 - val_<lambda>: 1.5884\n",
      "Epoch 26/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2268 - <lambda>: 1.2268 - val_loss: 1.5560 - val_<lambda>: 1.5560\n",
      "Epoch 27/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2230 - <lambda>: 1.2230 - val_loss: 1.6358 - val_<lambda>: 1.6358\n",
      "Epoch 28/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2160 - <lambda>: 1.2160 - val_loss: 1.6168 - val_<lambda>: 1.6168\n",
      "Epoch 29/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2093 - <lambda>: 1.2093 - val_loss: 1.6011 - val_<lambda>: 1.6011\n",
      "Epoch 30/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2104 - <lambda>: 1.2104 - val_loss: 1.5859 - val_<lambda>: 1.5859\n",
      "Epoch 31/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2028 - <lambda>: 1.2028 - val_loss: 1.5716 - val_<lambda>: 1.5716\n",
      "Epoch 32/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.1938 - <lambda>: 1.1938 - val_loss: 1.5839 - val_<lambda>: 1.5839\n",
      "Epoch 33/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.1847 - <lambda>: 1.1847 - val_loss: 1.6227 - val_<lambda>: 1.6227\n",
      "Epoch 34/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.1726 - <lambda>: 1.1726 - val_loss: 1.5716 - val_<lambda>: 1.5716\n",
      "\n",
      "\n",
      "  0.2 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "287/287 [==============================] - 6s 22ms/step - loss: 2.3981 - <lambda>: 2.3981 - val_loss: 2.6578 - val_<lambda>: 2.6578\n",
      "Epoch 2/500\n",
      "287/287 [==============================] - 6s 20ms/step - loss: 2.2047 - <lambda>: 2.2047 - val_loss: 2.4706 - val_<lambda>: 2.4706\n",
      "Epoch 3/500\n",
      "287/287 [==============================] - 6s 20ms/step - loss: 2.1870 - <lambda>: 2.1870 - val_loss: 2.4427 - val_<lambda>: 2.4427\n",
      "Epoch 4/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.1631 - <lambda>: 2.1631 - val_loss: 2.5169 - val_<lambda>: 2.5169\n",
      "Epoch 5/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.1595 - <lambda>: 2.1595 - val_loss: 2.5193 - val_<lambda>: 2.5193\n",
      "Epoch 6/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.1446 - <lambda>: 2.1446 - val_loss: 2.4640 - val_<lambda>: 2.4640\n",
      "Epoch 7/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.1238 - <lambda>: 2.1238 - val_loss: 2.4472 - val_<lambda>: 2.4472\n",
      "Epoch 8/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.1115 - <lambda>: 2.1115 - val_loss: 2.5812 - val_<lambda>: 2.5812\n",
      "Epoch 9/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.1131 - <lambda>: 2.1131 - val_loss: 2.5021 - val_<lambda>: 2.5021\n",
      "Epoch 10/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.1063 - <lambda>: 2.1063 - val_loss: 2.4507 - val_<lambda>: 2.4507\n",
      "Epoch 11/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0876 - <lambda>: 2.0876 - val_loss: 2.4895 - val_<lambda>: 2.4895\n",
      "Epoch 12/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0714 - <lambda>: 2.0714 - val_loss: 2.4551 - val_<lambda>: 2.4551\n",
      "Epoch 13/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.0623 - <lambda>: 2.0623 - val_loss: 2.5269 - val_<lambda>: 2.5269\n",
      "Epoch 14/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0541 - <lambda>: 2.0541 - val_loss: 2.4823 - val_<lambda>: 2.4823\n",
      "Epoch 15/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0425 - <lambda>: 2.0425 - val_loss: 2.4975 - val_<lambda>: 2.4975\n",
      "Epoch 16/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0379 - <lambda>: 2.0379 - val_loss: 2.4738 - val_<lambda>: 2.4738\n",
      "Epoch 17/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0147 - <lambda>: 2.0147 - val_loss: 2.5500 - val_<lambda>: 2.5500\n",
      "Epoch 18/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0131 - <lambda>: 2.0131 - val_loss: 2.5094 - val_<lambda>: 2.5094\n",
      "Epoch 19/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0110 - <lambda>: 2.0110 - val_loss: 2.5368 - val_<lambda>: 2.5368\n",
      "Epoch 20/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.9885 - <lambda>: 1.9885 - val_loss: 2.4730 - val_<lambda>: 2.4730\n",
      "Epoch 21/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9886 - <lambda>: 1.9886 - val_loss: 2.6484 - val_<lambda>: 2.6484\n",
      "Epoch 22/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9541 - <lambda>: 1.9541 - val_loss: 2.5527 - val_<lambda>: 2.5527\n",
      "Epoch 23/500\n",
      "285/287 [============================>.] - ETA: 0s - loss: 1.9501 - <lambda>: 1.9501\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9515 - <lambda>: 1.9515 - val_loss: 2.5477 - val_<lambda>: 2.5477\n",
      "Epoch 24/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9401 - <lambda>: 1.9401 - val_loss: 2.5507 - val_<lambda>: 2.5507\n",
      "Epoch 25/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9285 - <lambda>: 1.9285 - val_loss: 2.5260 - val_<lambda>: 2.5260\n",
      "Epoch 26/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9041 - <lambda>: 1.9041 - val_loss: 2.5392 - val_<lambda>: 2.5392\n",
      "Epoch 27/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9015 - <lambda>: 1.9015 - val_loss: 2.5193 - val_<lambda>: 2.5193\n",
      "Epoch 28/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8891 - <lambda>: 1.8891 - val_loss: 2.5982 - val_<lambda>: 2.5982\n",
      "Epoch 29/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.8778 - <lambda>: 1.8778 - val_loss: 2.6441 - val_<lambda>: 2.6441\n",
      "Epoch 30/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8706 - <lambda>: 1.8706 - val_loss: 2.5549 - val_<lambda>: 2.5549\n",
      "Epoch 31/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8606 - <lambda>: 1.8606 - val_loss: 2.5461 - val_<lambda>: 2.5461\n",
      "Epoch 32/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8465 - <lambda>: 1.8465 - val_loss: 2.5951 - val_<lambda>: 2.5951\n",
      "Epoch 33/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.8343 - <lambda>: 1.8343 - val_loss: 2.6168 - val_<lambda>: 2.6168\n",
      "\n",
      "\n",
      "  0.3 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "287/287 [==============================] - 6s 23ms/step - loss: 3.0922 - <lambda>: 3.0922 - val_loss: 3.5219 - val_<lambda>: 3.5219\n",
      "Epoch 2/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.6460 - <lambda>: 2.6460 - val_loss: 2.9699 - val_<lambda>: 2.9699\n",
      "Epoch 3/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.6160 - <lambda>: 2.6160 - val_loss: 2.8978 - val_<lambda>: 2.8978\n",
      "Epoch 4/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5777 - <lambda>: 2.5777 - val_loss: 2.9300 - val_<lambda>: 2.9300\n",
      "Epoch 5/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5721 - <lambda>: 2.5721 - val_loss: 2.9348 - val_<lambda>: 2.9348\n",
      "Epoch 6/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5471 - <lambda>: 2.5471 - val_loss: 2.9371 - val_<lambda>: 2.9371\n",
      "Epoch 7/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5466 - <lambda>: 2.5466 - val_loss: 2.9049 - val_<lambda>: 2.9049\n",
      "Epoch 8/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5395 - <lambda>: 2.5395 - val_loss: 2.9023 - val_<lambda>: 2.9023\n",
      "Epoch 9/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5130 - <lambda>: 2.5130 - val_loss: 2.9435 - val_<lambda>: 2.9435\n",
      "Epoch 10/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5132 - <lambda>: 2.5132 - val_loss: 2.9737 - val_<lambda>: 2.9737\n",
      "Epoch 11/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4997 - <lambda>: 2.4997 - val_loss: 2.9246 - val_<lambda>: 2.9246\n",
      "Epoch 12/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4816 - <lambda>: 2.4816 - val_loss: 2.9045 - val_<lambda>: 2.9045\n",
      "Epoch 13/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4545 - <lambda>: 2.4545 - val_loss: 2.9587 - val_<lambda>: 2.9587\n",
      "Epoch 14/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4554 - <lambda>: 2.4554 - val_loss: 2.9403 - val_<lambda>: 2.9403\n",
      "Epoch 15/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4362 - <lambda>: 2.4362 - val_loss: 2.9689 - val_<lambda>: 2.9689\n",
      "Epoch 16/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4314 - <lambda>: 2.4314 - val_loss: 2.9605 - val_<lambda>: 2.9605\n",
      "Epoch 17/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4230 - <lambda>: 2.4230 - val_loss: 3.0061 - val_<lambda>: 3.0061\n",
      "Epoch 18/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4053 - <lambda>: 2.4053 - val_loss: 3.0692 - val_<lambda>: 3.0692\n",
      "Epoch 19/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3959 - <lambda>: 2.3959 - val_loss: 2.9751 - val_<lambda>: 2.9751\n",
      "Epoch 20/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3801 - <lambda>: 2.3801 - val_loss: 2.9620 - val_<lambda>: 2.9620\n",
      "Epoch 21/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3551 - <lambda>: 2.3551 - val_loss: 2.9420 - val_<lambda>: 2.9420\n",
      "Epoch 22/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3776 - <lambda>: 2.3776 - val_loss: 2.9637 - val_<lambda>: 2.9637\n",
      "Epoch 23/500\n",
      "287/287 [==============================] - ETA: 0s - loss: 2.3427 - <lambda>: 2.3427\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3427 - <lambda>: 2.3427 - val_loss: 2.9524 - val_<lambda>: 2.9524\n",
      "Epoch 24/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3225 - <lambda>: 2.3225 - val_loss: 3.0001 - val_<lambda>: 3.0001\n",
      "Epoch 25/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3154 - <lambda>: 2.3154 - val_loss: 2.9322 - val_<lambda>: 2.9322\n",
      "Epoch 26/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2971 - <lambda>: 2.2971 - val_loss: 2.9826 - val_<lambda>: 2.9826\n",
      "Epoch 27/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2940 - <lambda>: 2.2940 - val_loss: 2.9457 - val_<lambda>: 2.9457\n",
      "Epoch 28/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2724 - <lambda>: 2.2724 - val_loss: 3.0978 - val_<lambda>: 3.0978\n",
      "Epoch 29/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2532 - <lambda>: 2.2532 - val_loss: 2.9323 - val_<lambda>: 2.9323\n",
      "Epoch 30/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2379 - <lambda>: 2.2379 - val_loss: 2.9627 - val_<lambda>: 2.9627\n",
      "Epoch 31/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2477 - <lambda>: 2.2477 - val_loss: 3.0225 - val_<lambda>: 3.0225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2189 - <lambda>: 2.2189 - val_loss: 3.0039 - val_<lambda>: 3.0039\n",
      "Epoch 33/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2157 - <lambda>: 2.2157 - val_loss: 3.0028 - val_<lambda>: 3.0028\n",
      "\n",
      "\n",
      "  0.4 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "287/287 [==============================] - 6s 22ms/step - loss: 3.1125 - <lambda>: 3.1125 - val_loss: 3.8604 - val_<lambda>: 3.8604\n",
      "Epoch 2/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.8019 - <lambda>: 2.8019 - val_loss: 3.1161 - val_<lambda>: 3.1161\n",
      "Epoch 3/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.7449 - <lambda>: 2.7449 - val_loss: 3.1280 - val_<lambda>: 3.1280\n",
      "Epoch 4/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.7410 - <lambda>: 2.7410 - val_loss: 3.1555 - val_<lambda>: 3.1555\n",
      "Epoch 5/500\n",
      "287/287 [==============================] - 8s 27ms/step - loss: 2.7014 - <lambda>: 2.7014 - val_loss: 3.0616 - val_<lambda>: 3.0616\n",
      "Epoch 6/500\n",
      "287/287 [==============================] - 6s 19ms/step - loss: 2.6955 - <lambda>: 2.6955 - val_loss: 3.0136 - val_<lambda>: 3.0136\n",
      "Epoch 7/500\n",
      "287/287 [==============================] - 6s 19ms/step - loss: 2.6593 - <lambda>: 2.6593 - val_loss: 2.9938 - val_<lambda>: 2.9938\n",
      "Epoch 8/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6537 - <lambda>: 2.6537 - val_loss: 3.0606 - val_<lambda>: 3.0606\n",
      "Epoch 9/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6390 - <lambda>: 2.6390 - val_loss: 3.0860 - val_<lambda>: 3.0860\n",
      "Epoch 10/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6223 - <lambda>: 2.6223 - val_loss: 3.0156 - val_<lambda>: 3.0156\n",
      "Epoch 11/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6110 - <lambda>: 2.6110 - val_loss: 3.2797 - val_<lambda>: 3.2797\n",
      "Epoch 12/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6037 - <lambda>: 2.6037 - val_loss: 2.9942 - val_<lambda>: 2.9942\n",
      "Epoch 13/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5805 - <lambda>: 2.5805 - val_loss: 2.9983 - val_<lambda>: 2.9983\n",
      "Epoch 14/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5572 - <lambda>: 2.5572 - val_loss: 3.0088 - val_<lambda>: 3.0088\n",
      "Epoch 15/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5552 - <lambda>: 2.5552 - val_loss: 3.1178 - val_<lambda>: 3.1178\n",
      "Epoch 16/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5405 - <lambda>: 2.5405 - val_loss: 3.0426 - val_<lambda>: 3.0426\n",
      "Epoch 17/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5322 - <lambda>: 2.5322 - val_loss: 3.1265 - val_<lambda>: 3.1265\n",
      "Epoch 18/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5133 - <lambda>: 2.5133 - val_loss: 3.2047 - val_<lambda>: 3.2047\n",
      "Epoch 19/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5150 - <lambda>: 2.5150 - val_loss: 3.0431 - val_<lambda>: 3.0431\n",
      "Epoch 20/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4984 - <lambda>: 2.4984 - val_loss: 3.0447 - val_<lambda>: 3.0447\n",
      "Epoch 21/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4711 - <lambda>: 2.4711 - val_loss: 3.1828 - val_<lambda>: 3.1828\n",
      "Epoch 22/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.4669 - <lambda>: 2.4669 - val_loss: 3.0803 - val_<lambda>: 3.0803\n",
      "Epoch 23/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.4599 - <lambda>: 2.4599 - val_loss: 3.1300 - val_<lambda>: 3.1300\n",
      "Epoch 24/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.4568 - <lambda>: 2.4568 - val_loss: 3.2557 - val_<lambda>: 3.2557\n",
      "Epoch 25/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.4324 - <lambda>: 2.4324 - val_loss: 3.1686 - val_<lambda>: 3.1686\n",
      "Epoch 26/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.4328 - <lambda>: 2.4328 - val_loss: 3.1798 - val_<lambda>: 3.1798\n",
      "Epoch 27/500\n",
      "284/287 [============================>.] - ETA: 0s - loss: 2.4024 - <lambda>: 2.4024\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.4038 - <lambda>: 2.4038 - val_loss: 3.1037 - val_<lambda>: 3.1037\n",
      "Epoch 28/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.3901 - <lambda>: 2.3901 - val_loss: 3.1356 - val_<lambda>: 3.1356\n",
      "Epoch 29/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.3709 - <lambda>: 2.3709 - val_loss: 3.1337 - val_<lambda>: 3.1337\n",
      "Epoch 30/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.3493 - <lambda>: 2.3493 - val_loss: 3.2303 - val_<lambda>: 3.2303\n",
      "Epoch 31/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.3503 - <lambda>: 2.3503 - val_loss: 3.1421 - val_<lambda>: 3.1421\n",
      "Epoch 32/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.3415 - <lambda>: 2.3415 - val_loss: 3.0704 - val_<lambda>: 3.0704\n",
      "Epoch 33/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.3014 - <lambda>: 2.3014 - val_loss: 3.2020 - val_<lambda>: 3.2020\n",
      "Epoch 34/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.3074 - <lambda>: 2.3074 - val_loss: 3.1223 - val_<lambda>: 3.1223\n",
      "Epoch 35/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.2995 - <lambda>: 2.2995 - val_loss: 3.1785 - val_<lambda>: 3.1785\n",
      "Epoch 36/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.2767 - <lambda>: 2.2767 - val_loss: 3.1675 - val_<lambda>: 3.1675\n",
      "Epoch 37/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.2692 - <lambda>: 2.2692 - val_loss: 3.2174 - val_<lambda>: 3.2174\n",
      "\n",
      "\n",
      "  0.5 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "287/287 [==============================] - 6s 22ms/step - loss: 3.1914 - <lambda>: 3.1914 - val_loss: 4.0683 - val_<lambda>: 4.0683\n",
      "Epoch 2/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.7494 - <lambda>: 2.7494 - val_loss: 3.0634 - val_<lambda>: 3.0634\n",
      "Epoch 3/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6810 - <lambda>: 2.6810 - val_loss: 2.8673 - val_<lambda>: 2.8673\n",
      "Epoch 4/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6593 - <lambda>: 2.6593 - val_loss: 2.9418 - val_<lambda>: 2.9418\n",
      "Epoch 5/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6258 - <lambda>: 2.6258 - val_loss: 2.9544 - val_<lambda>: 2.9544\n",
      "Epoch 6/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5949 - <lambda>: 2.5949 - val_loss: 3.0490 - val_<lambda>: 3.0490\n",
      "Epoch 7/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.5973 - <lambda>: 2.5973 - val_loss: 2.9875 - val_<lambda>: 2.9875\n",
      "Epoch 8/500\n",
      "287/287 [==============================] - 7s 25ms/step - loss: 2.5653 - <lambda>: 2.5653 - val_loss: 2.8437 - val_<lambda>: 2.8437\n",
      "Epoch 9/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5592 - <lambda>: 2.5592 - val_loss: 3.0333 - val_<lambda>: 3.0333\n",
      "Epoch 10/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5533 - <lambda>: 2.5533 - val_loss: 2.9192 - val_<lambda>: 2.9192\n",
      "Epoch 11/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5404 - <lambda>: 2.5404 - val_loss: 2.9117 - val_<lambda>: 2.9117\n",
      "Epoch 12/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5155 - <lambda>: 2.5155 - val_loss: 2.9478 - val_<lambda>: 2.9478\n",
      "Epoch 13/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5125 - <lambda>: 2.5125 - val_loss: 2.9219 - val_<lambda>: 2.9219\n",
      "Epoch 14/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5316 - <lambda>: 2.5316 - val_loss: 2.9095 - val_<lambda>: 2.9095\n",
      "Epoch 15/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4900 - <lambda>: 2.4900 - val_loss: 2.8803 - val_<lambda>: 2.8803\n",
      "Epoch 16/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4863 - <lambda>: 2.4863 - val_loss: 2.8949 - val_<lambda>: 2.8949\n",
      "Epoch 17/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.4968 - <lambda>: 2.4968 - val_loss: 2.8908 - val_<lambda>: 2.8908\n",
      "Epoch 18/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.4814 - <lambda>: 2.4814 - val_loss: 2.8968 - val_<lambda>: 2.8968\n",
      "Epoch 19/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4639 - <lambda>: 2.4639 - val_loss: 2.9024 - val_<lambda>: 2.9024\n",
      "Epoch 20/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4451 - <lambda>: 2.4451 - val_loss: 2.9913 - val_<lambda>: 2.9913\n",
      "Epoch 21/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.4510 - <lambda>: 2.4510 - val_loss: 3.0453 - val_<lambda>: 3.0453\n",
      "Epoch 22/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4339 - <lambda>: 2.4339 - val_loss: 2.8859 - val_<lambda>: 2.8859\n",
      "Epoch 23/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4254 - <lambda>: 2.4254 - val_loss: 2.9633 - val_<lambda>: 2.9633\n",
      "Epoch 24/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4141 - <lambda>: 2.4141 - val_loss: 2.9041 - val_<lambda>: 2.9041\n",
      "Epoch 25/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4127 - <lambda>: 2.4127 - val_loss: 2.9918 - val_<lambda>: 2.9918\n",
      "Epoch 26/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4034 - <lambda>: 2.4034 - val_loss: 2.8829 - val_<lambda>: 2.8829\n",
      "Epoch 27/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3927 - <lambda>: 2.3927 - val_loss: 2.8922 - val_<lambda>: 2.8922\n",
      "Epoch 28/500\n",
      "286/287 [============================>.] - ETA: 0s - loss: 2.3888 - <lambda>: 2.3888\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3922 - <lambda>: 2.3922 - val_loss: 3.0009 - val_<lambda>: 3.0009\n",
      "Epoch 29/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.3637 - <lambda>: 2.3637 - val_loss: 2.9373 - val_<lambda>: 2.9373\n",
      "Epoch 30/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3455 - <lambda>: 2.3455 - val_loss: 2.9172 - val_<lambda>: 2.9172\n",
      "Epoch 31/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3428 - <lambda>: 2.3428 - val_loss: 2.9815 - val_<lambda>: 2.9815\n",
      "Epoch 32/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3411 - <lambda>: 2.3411 - val_loss: 2.9094 - val_<lambda>: 2.9094\n",
      "Epoch 33/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3214 - <lambda>: 2.3214 - val_loss: 3.0160 - val_<lambda>: 3.0160\n",
      "Epoch 34/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3110 - <lambda>: 2.3110 - val_loss: 2.9326 - val_<lambda>: 2.9326\n",
      "Epoch 35/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3025 - <lambda>: 2.3025 - val_loss: 3.0696 - val_<lambda>: 3.0696\n",
      "Epoch 36/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2860 - <lambda>: 2.2860 - val_loss: 2.9491 - val_<lambda>: 2.9491\n",
      "Epoch 37/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.2754 - <lambda>: 2.2754 - val_loss: 2.9245 - val_<lambda>: 2.9245\n",
      "Epoch 38/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2685 - <lambda>: 2.2685 - val_loss: 2.9867 - val_<lambda>: 2.9867\n",
      "\n",
      "\n",
      "  0.6 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "285/287 [============================>.] - ETA: 0s - loss: 3.2099 - <lambda>: 3.2099WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0080s). Check your callbacks.\n",
      "287/287 [==============================] - 6s 22ms/step - loss: 3.2043 - <lambda>: 3.2043 - val_loss: 3.6974 - val_<lambda>: 3.6974\n",
      "Epoch 2/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.4849 - <lambda>: 2.4849 - val_loss: 2.6635 - val_<lambda>: 2.6635\n",
      "Epoch 3/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4071 - <lambda>: 2.4071 - val_loss: 2.8113 - val_<lambda>: 2.8113\n",
      "Epoch 4/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.3803 - <lambda>: 2.3803 - val_loss: 2.5937 - val_<lambda>: 2.5937\n",
      "Epoch 5/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3529 - <lambda>: 2.3529 - val_loss: 2.6826 - val_<lambda>: 2.6826\n",
      "Epoch 6/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3421 - <lambda>: 2.3421 - val_loss: 2.5970 - val_<lambda>: 2.5970\n",
      "Epoch 7/500\n",
      "287/287 [==============================] - 8s 26ms/step - loss: 2.3304 - <lambda>: 2.3304 - val_loss: 2.5540 - val_<lambda>: 2.5540\n",
      "Epoch 8/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3117 - <lambda>: 2.3117 - val_loss: 2.7057 - val_<lambda>: 2.7057\n",
      "Epoch 9/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2991 - <lambda>: 2.2991 - val_loss: 2.5877 - val_<lambda>: 2.5877\n",
      "Epoch 10/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3049 - <lambda>: 2.3049 - val_loss: 2.6609 - val_<lambda>: 2.6609\n",
      "Epoch 11/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2801 - <lambda>: 2.2801 - val_loss: 2.6308 - val_<lambda>: 2.6308\n",
      "Epoch 12/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2604 - <lambda>: 2.2604 - val_loss: 2.5820 - val_<lambda>: 2.5820\n",
      "Epoch 13/500\n",
      "287/287 [==============================] - 7s 25ms/step - loss: 2.2905 - <lambda>: 2.2905 - val_loss: 2.5245 - val_<lambda>: 2.5245\n",
      "Epoch 14/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2451 - <lambda>: 2.2451 - val_loss: 2.5671 - val_<lambda>: 2.5671\n",
      "Epoch 15/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2374 - <lambda>: 2.2374 - val_loss: 2.6006 - val_<lambda>: 2.6006\n",
      "Epoch 16/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2272 - <lambda>: 2.2272 - val_loss: 2.5897 - val_<lambda>: 2.5897\n",
      "Epoch 17/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2238 - <lambda>: 2.2238 - val_loss: 2.6183 - val_<lambda>: 2.6183\n",
      "Epoch 18/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.2107 - <lambda>: 2.2107 - val_loss: 2.6419 - val_<lambda>: 2.6419\n",
      "Epoch 19/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.1984 - <lambda>: 2.1984 - val_loss: 2.6660 - val_<lambda>: 2.6660\n",
      "Epoch 20/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.1954 - <lambda>: 2.1954 - val_loss: 2.6440 - val_<lambda>: 2.6440\n",
      "Epoch 21/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.1800 - <lambda>: 2.1800 - val_loss: 2.5744 - val_<lambda>: 2.5744\n",
      "Epoch 22/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287/287 [==============================] - 5s 18ms/step - loss: 2.1731 - <lambda>: 2.1731 - val_loss: 2.6388 - val_<lambda>: 2.6388\n",
      "Epoch 23/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.1684 - <lambda>: 2.1684 - val_loss: 2.5605 - val_<lambda>: 2.5605\n",
      "Epoch 24/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.1556 - <lambda>: 2.1556 - val_loss: 2.6335 - val_<lambda>: 2.6335\n",
      "Epoch 25/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.1450 - <lambda>: 2.1450 - val_loss: 2.6345 - val_<lambda>: 2.6345\n",
      "Epoch 26/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.1334 - <lambda>: 2.1334 - val_loss: 2.6451 - val_<lambda>: 2.6451\n",
      "Epoch 27/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.1382 - <lambda>: 2.1382 - val_loss: 2.6810 - val_<lambda>: 2.6810\n",
      "Epoch 28/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.1335 - <lambda>: 2.1335 - val_loss: 2.6154 - val_<lambda>: 2.6154\n",
      "Epoch 29/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.1250 - <lambda>: 2.1250 - val_loss: 2.7123 - val_<lambda>: 2.7123\n",
      "Epoch 30/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.1258 - <lambda>: 2.1258 - val_loss: 2.6269 - val_<lambda>: 2.6269\n",
      "Epoch 31/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.1072 - <lambda>: 2.1072 - val_loss: 2.7361 - val_<lambda>: 2.7361\n",
      "Epoch 32/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0955 - <lambda>: 2.0955 - val_loss: 2.7061 - val_<lambda>: 2.7061\n",
      "Epoch 33/500\n",
      "287/287 [==============================] - ETA: 0s - loss: 2.0921 - <lambda>: 2.0921 ETA: 0s - los\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0921 - <lambda>: 2.0921 - val_loss: 2.6498 - val_<lambda>: 2.6498\n",
      "Epoch 34/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0841 - <lambda>: 2.0841 - val_loss: 2.6134 - val_<lambda>: 2.6134\n",
      "Epoch 35/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0621 - <lambda>: 2.0621 - val_loss: 2.5929 - val_<lambda>: 2.5929\n",
      "Epoch 36/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0650 - <lambda>: 2.0650 - val_loss: 2.7637 - val_<lambda>: 2.7637\n",
      "Epoch 37/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0537 - <lambda>: 2.0537 - val_loss: 2.7565 - val_<lambda>: 2.7565\n",
      "Epoch 38/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0513 - <lambda>: 2.0513 - val_loss: 2.6332 - val_<lambda>: 2.6332\n",
      "Epoch 39/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0407 - <lambda>: 2.0407 - val_loss: 2.6791 - val_<lambda>: 2.6791\n",
      "Epoch 40/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0392 - <lambda>: 2.0392 - val_loss: 2.7105 - val_<lambda>: 2.7105\n",
      "Epoch 41/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0258 - <lambda>: 2.0258 - val_loss: 2.7052 - val_<lambda>: 2.7052\n",
      "Epoch 42/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0153 - <lambda>: 2.0153 - val_loss: 2.7088 - val_<lambda>: 2.7088\n",
      "Epoch 43/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0037 - <lambda>: 2.0037 - val_loss: 2.6731 - val_<lambda>: 2.6731\n",
      "\n",
      "\n",
      "  0.7 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "287/287 [==============================] - 6s 21ms/step - loss: 2.8749 - <lambda>: 2.8749 - val_loss: 3.5961 - val_<lambda>: 3.5961\n",
      "Epoch 2/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.1737 - <lambda>: 2.1737 - val_loss: 2.2369 - val_<lambda>: 2.2369\n",
      "Epoch 3/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0613 - <lambda>: 2.0613 - val_loss: 2.1873 - val_<lambda>: 2.1873\n",
      "Epoch 4/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0160 - <lambda>: 2.0160 - val_loss: 2.4129 - val_<lambda>: 2.4129\n",
      "Epoch 5/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9938 - <lambda>: 1.9938 - val_loss: 2.1770 - val_<lambda>: 2.1770\n",
      "Epoch 6/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9663 - <lambda>: 1.9663 - val_loss: 2.3470 - val_<lambda>: 2.3470\n",
      "Epoch 7/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9660 - <lambda>: 1.9660 - val_loss: 2.2445 - val_<lambda>: 2.2445\n",
      "Epoch 8/500\n",
      "287/287 [==============================] - 7s 25ms/step - loss: 1.9283 - <lambda>: 1.9283 - val_loss: 2.1155 - val_<lambda>: 2.1155\n",
      "Epoch 9/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9076 - <lambda>: 1.9076 - val_loss: 2.1503 - val_<lambda>: 2.1503\n",
      "Epoch 10/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8893 - <lambda>: 1.8893 - val_loss: 2.0615 - val_<lambda>: 2.0615\n",
      "Epoch 11/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8994 - <lambda>: 1.8994 - val_loss: 2.0870 - val_<lambda>: 2.0870\n",
      "Epoch 12/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8834 - <lambda>: 1.8834 - val_loss: 2.1331 - val_<lambda>: 2.1331\n",
      "Epoch 13/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.8796 - <lambda>: 1.8796 - val_loss: 2.1643 - val_<lambda>: 2.1643\n",
      "Epoch 14/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8520 - <lambda>: 1.8520 - val_loss: 2.1792 - val_<lambda>: 2.1792\n",
      "Epoch 15/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8663 - <lambda>: 1.8663 - val_loss: 2.0886 - val_<lambda>: 2.0886\n",
      "Epoch 16/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.8469 - <lambda>: 1.8469 - val_loss: 2.1461 - val_<lambda>: 2.1461\n",
      "Epoch 17/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.8355 - <lambda>: 1.8355 - val_loss: 2.1107 - val_<lambda>: 2.1107\n",
      "Epoch 18/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8360 - <lambda>: 1.8360 - val_loss: 2.1510 - val_<lambda>: 2.1510\n",
      "Epoch 19/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.8268 - <lambda>: 1.8268 - val_loss: 2.1357 - val_<lambda>: 2.1357\n",
      "Epoch 20/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.8100 - <lambda>: 1.8100 - val_loss: 2.1349 - val_<lambda>: 2.1349\n",
      "Epoch 21/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8176 - <lambda>: 1.8176 - val_loss: 2.1157 - val_<lambda>: 2.1157\n",
      "Epoch 22/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8092 - <lambda>: 1.8092 - val_loss: 2.2851 - val_<lambda>: 2.2851\n",
      "Epoch 23/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.8002 - <lambda>: 1.8002 - val_loss: 2.1588 - val_<lambda>: 2.1588\n",
      "Epoch 24/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.7919 - <lambda>: 1.7919 - val_loss: 2.1206 - val_<lambda>: 2.1206\n",
      "Epoch 25/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.7955 - <lambda>: 1.7955 - val_loss: 2.1919 - val_<lambda>: 2.1919\n",
      "Epoch 26/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.7909 - <lambda>: 1.7909 - val_loss: 2.2091 - val_<lambda>: 2.2091\n",
      "Epoch 27/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.7860 - <lambda>: 1.7860 - val_loss: 2.0890 - val_<lambda>: 2.0890\n",
      "Epoch 28/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.7698 - <lambda>: 1.7698 - val_loss: 2.1824 - val_<lambda>: 2.1824\n",
      "Epoch 29/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.7710 - <lambda>: 1.7710 - val_loss: 2.1513 - val_<lambda>: 2.1513\n",
      "Epoch 30/500\n",
      "287/287 [==============================] - ETA: 0s - loss: 1.7717 - <lambda>: 1.7717\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.7717 - <lambda>: 1.7717 - val_loss: 2.2435 - val_<lambda>: 2.2435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.7442 - <lambda>: 1.7442 - val_loss: 2.1610 - val_<lambda>: 2.1610\n",
      "Epoch 32/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.7506 - <lambda>: 1.7506 - val_loss: 2.2004 - val_<lambda>: 2.2004\n",
      "Epoch 33/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.7301 - <lambda>: 1.7301 - val_loss: 2.1473 - val_<lambda>: 2.1473\n",
      "Epoch 34/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.7184 - <lambda>: 1.7184 - val_loss: 2.1630 - val_<lambda>: 2.1630\n",
      "Epoch 35/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.7297 - <lambda>: 1.7297 - val_loss: 2.1687 - val_<lambda>: 2.1687\n",
      "Epoch 36/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.7059 - <lambda>: 1.7059 - val_loss: 2.1837 - val_<lambda>: 2.1837\n",
      "Epoch 37/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.7182 - <lambda>: 1.7182 - val_loss: 2.1497 - val_<lambda>: 2.1497\n",
      "Epoch 38/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.6933 - <lambda>: 1.6933 - val_loss: 2.2007 - val_<lambda>: 2.2007\n",
      "Epoch 39/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.6974 - <lambda>: 1.6974 - val_loss: 2.1839 - val_<lambda>: 2.1839\n",
      "Epoch 40/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.6998 - <lambda>: 1.6998 - val_loss: 2.1682 - val_<lambda>: 2.1682\n",
      "\n",
      "\n",
      "  0.8 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "287/287 [==============================] - 6s 21ms/step - loss: 2.4847 - <lambda>: 2.4847 - val_loss: 3.3632 - val_<lambda>: 3.3632\n",
      "Epoch 2/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.6346 - <lambda>: 1.6346 - val_loss: 1.6109 - val_<lambda>: 1.6109\n",
      "Epoch 3/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.5517 - <lambda>: 1.5517 - val_loss: 1.5642 - val_<lambda>: 1.5642\n",
      "Epoch 4/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.5165 - <lambda>: 1.5165 - val_loss: 1.6743 - val_<lambda>: 1.6743\n",
      "Epoch 5/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.5081 - <lambda>: 1.5081 - val_loss: 1.7028 - val_<lambda>: 1.7028\n",
      "Epoch 6/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.4599 - <lambda>: 1.4599 - val_loss: 1.6018 - val_<lambda>: 1.6018\n",
      "Epoch 7/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.4434 - <lambda>: 1.4434 - val_loss: 1.5680 - val_<lambda>: 1.5680\n",
      "Epoch 8/500\n",
      "287/287 [==============================] - 7s 25ms/step - loss: 1.4439 - <lambda>: 1.4439 - val_loss: 1.5272 - val_<lambda>: 1.5272\n",
      "Epoch 9/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.4083 - <lambda>: 1.4083 - val_loss: 1.5717 - val_<lambda>: 1.5717\n",
      "Epoch 10/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.4069 - <lambda>: 1.4069 - val_loss: 1.5120 - val_<lambda>: 1.5120\n",
      "Epoch 11/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3951 - <lambda>: 1.3951 - val_loss: 1.6188 - val_<lambda>: 1.6188\n",
      "Epoch 12/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.3756 - <lambda>: 1.3756 - val_loss: 1.6132 - val_<lambda>: 1.6132\n",
      "Epoch 13/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3827 - <lambda>: 1.3827 - val_loss: 1.5253 - val_<lambda>: 1.5253\n",
      "Epoch 14/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3796 - <lambda>: 1.3796 - val_loss: 1.5210 - val_<lambda>: 1.5210\n",
      "Epoch 15/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3660 - <lambda>: 1.3660 - val_loss: 1.5549 - val_<lambda>: 1.5549\n",
      "Epoch 16/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3540 - <lambda>: 1.3540 - val_loss: 1.5297 - val_<lambda>: 1.5297\n",
      "Epoch 17/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3568 - <lambda>: 1.3568 - val_loss: 1.5561 - val_<lambda>: 1.5561\n",
      "Epoch 18/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3526 - <lambda>: 1.3526 - val_loss: 1.5495 - val_<lambda>: 1.5495\n",
      "Epoch 19/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3529 - <lambda>: 1.3529 - val_loss: 1.5738 - val_<lambda>: 1.5738\n",
      "Epoch 20/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.3437 - <lambda>: 1.3437 - val_loss: 1.5805 - val_<lambda>: 1.5805\n",
      "Epoch 21/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3370 - <lambda>: 1.3370 - val_loss: 1.5346 - val_<lambda>: 1.5346\n",
      "Epoch 22/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3297 - <lambda>: 1.3297 - val_loss: 1.5664 - val_<lambda>: 1.5664\n",
      "Epoch 23/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3319 - <lambda>: 1.3319 - val_loss: 1.5603 - val_<lambda>: 1.5603\n",
      "Epoch 24/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.3164 - <lambda>: 1.3164 - val_loss: 1.5729 - val_<lambda>: 1.5729\n",
      "Epoch 25/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.3303 - <lambda>: 1.3303 - val_loss: 1.5611 - val_<lambda>: 1.5611\n",
      "Epoch 26/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3247 - <lambda>: 1.3247 - val_loss: 1.5515 - val_<lambda>: 1.5515\n",
      "Epoch 27/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3218 - <lambda>: 1.3218 - val_loss: 1.5391 - val_<lambda>: 1.5391\n",
      "Epoch 28/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3148 - <lambda>: 1.3148 - val_loss: 1.5529 - val_<lambda>: 1.5529\n",
      "Epoch 29/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.2996 - <lambda>: 1.2996 - val_loss: 1.6356 - val_<lambda>: 1.6356\n",
      "Epoch 30/500\n",
      "286/287 [============================>.] - ETA: 0s - loss: 1.3008 - <lambda>: 1.3008\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.3003 - <lambda>: 1.3003 - val_loss: 1.5804 - val_<lambda>: 1.5804\n",
      "Epoch 31/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.2950 - <lambda>: 1.2950 - val_loss: 1.5782 - val_<lambda>: 1.5782\n",
      "Epoch 32/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.2857 - <lambda>: 1.2857 - val_loss: 1.5465 - val_<lambda>: 1.5465\n",
      "Epoch 33/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.2759 - <lambda>: 1.2759 - val_loss: 1.5737 - val_<lambda>: 1.5737\n",
      "Epoch 34/500\n",
      "287/287 [==============================] - 8s 26ms/step - loss: 1.2726 - <lambda>: 1.2726 - val_loss: 1.5078 - val_<lambda>: 1.5078\n",
      "Epoch 35/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.2650 - <lambda>: 1.2650 - val_loss: 1.5369 - val_<lambda>: 1.5369\n",
      "Epoch 36/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.2644 - <lambda>: 1.2644 - val_loss: 1.5430 - val_<lambda>: 1.5430\n",
      "Epoch 37/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.2686 - <lambda>: 1.2686 - val_loss: 1.5434 - val_<lambda>: 1.5434\n",
      "Epoch 38/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.2605 - <lambda>: 1.2605 - val_loss: 1.5325 - val_<lambda>: 1.5325\n",
      "Epoch 39/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.2636 - <lambda>: 1.2636 - val_loss: 1.5674 - val_<lambda>: 1.5674\n",
      "Epoch 40/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.2513 - <lambda>: 1.2513 - val_loss: 1.6453 - val_<lambda>: 1.6453\n",
      "Epoch 41/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.2501 - <lambda>: 1.2501 - val_loss: 1.5715 - val_<lambda>: 1.5715\n",
      "Epoch 42/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.2530 - <lambda>: 1.2530 - val_loss: 1.5451 - val_<lambda>: 1.5451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.2481 - <lambda>: 1.2481 - val_loss: 1.6026 - val_<lambda>: 1.6026\n",
      "Epoch 44/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.2519 - <lambda>: 1.2519 - val_loss: 1.5562 - val_<lambda>: 1.5562\n",
      "Epoch 45/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.2442 - <lambda>: 1.2442 - val_loss: 1.5467 - val_<lambda>: 1.5467\n",
      "Epoch 46/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.2375 - <lambda>: 1.2375 - val_loss: 1.6036 - val_<lambda>: 1.6036\n",
      "Epoch 47/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.2362 - <lambda>: 1.2362 - val_loss: 1.5666 - val_<lambda>: 1.5666\n",
      "Epoch 48/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.2292 - <lambda>: 1.2292 - val_loss: 1.5769 - val_<lambda>: 1.5769\n",
      "Epoch 49/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.2296 - <lambda>: 1.2296 - val_loss: 1.5977 - val_<lambda>: 1.5977\n",
      "Epoch 50/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.2318 - <lambda>: 1.2318 - val_loss: 1.6683 - val_<lambda>: 1.6683\n",
      "Epoch 51/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.2279 - <lambda>: 1.2279 - val_loss: 1.5928 - val_<lambda>: 1.5928\n",
      "Epoch 52/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.2215 - <lambda>: 1.2215 - val_loss: 1.5745 - val_<lambda>: 1.5745\n",
      "Epoch 53/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.2167 - <lambda>: 1.2167 - val_loss: 1.6035 - val_<lambda>: 1.6035\n",
      "Epoch 54/500\n",
      "284/287 [============================>.] - ETA: 0s - loss: 1.2228 - <lambda>: 1.2228\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.2220 - <lambda>: 1.2220 - val_loss: 1.5825 - val_<lambda>: 1.5825\n",
      "Epoch 55/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.2152 - <lambda>: 1.2152 - val_loss: 1.6240 - val_<lambda>: 1.6240\n",
      "Epoch 56/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.1994 - <lambda>: 1.1994 - val_loss: 1.6671 - val_<lambda>: 1.6671\n",
      "Epoch 57/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.1929 - <lambda>: 1.1929 - val_loss: 1.5888 - val_<lambda>: 1.5888\n",
      "Epoch 58/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.1895 - <lambda>: 1.1895 - val_loss: 1.6396 - val_<lambda>: 1.6396\n",
      "Epoch 59/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.1833 - <lambda>: 1.1833 - val_loss: 1.5864 - val_<lambda>: 1.5864\n",
      "Epoch 60/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.1857 - <lambda>: 1.1857 - val_loss: 1.5984 - val_<lambda>: 1.5984\n",
      "Epoch 61/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.1903 - <lambda>: 1.1903 - val_loss: 1.6359 - val_<lambda>: 1.6359\n",
      "Epoch 62/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 1.1869 - <lambda>: 1.1869 - val_loss: 1.6013 - val_<lambda>: 1.6013\n",
      "Epoch 63/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.1745 - <lambda>: 1.1745 - val_loss: 1.6375 - val_<lambda>: 1.6375\n",
      "Epoch 64/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.1791 - <lambda>: 1.1791 - val_loss: 1.6020 - val_<lambda>: 1.6020\n",
      "\n",
      "\n",
      "  0.9 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "287/287 [==============================] - 6s 21ms/step - loss: 1.8913 - <lambda>: 1.8913 - val_loss: 1.7759 - val_<lambda>: 1.7759\n",
      "Epoch 2/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.0145 - <lambda>: 1.0145 - val_loss: 0.9733 - val_<lambda>: 0.9733\n",
      "Epoch 3/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 0.9506 - <lambda>: 0.9506 - val_loss: 1.0376 - val_<lambda>: 1.0376\n",
      "Epoch 4/500\n",
      "287/287 [==============================] - 7s 26ms/step - loss: 0.8933 - <lambda>: 0.8933 - val_loss: 0.8951 - val_<lambda>: 0.8951\n",
      "Epoch 5/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 0.9058 - <lambda>: 0.9058 - val_loss: 0.9558 - val_<lambda>: 0.9558\n",
      "Epoch 6/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 0.8638 - <lambda>: 0.8638 - val_loss: 0.9768 - val_<lambda>: 0.9768\n",
      "Epoch 7/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 0.8304 - <lambda>: 0.8304 - val_loss: 0.9043 - val_<lambda>: 0.9043\n",
      "Epoch 8/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 0.8270 - <lambda>: 0.8270 - val_loss: 0.8998 - val_<lambda>: 0.8998\n",
      "Epoch 9/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 0.8082 - <lambda>: 0.8082 - val_loss: 0.8993 - val_<lambda>: 0.8993\n",
      "Epoch 10/500\n",
      "287/287 [==============================] - 7s 25ms/step - loss: 0.7948 - <lambda>: 0.7948 - val_loss: 0.8583 - val_<lambda>: 0.8583\n",
      "Epoch 11/500\n",
      "287/287 [==============================] - 6s 19ms/step - loss: 0.7915 - <lambda>: 0.7915 - val_loss: 0.8428 - val_<lambda>: 0.8428\n",
      "Epoch 12/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 0.7931 - <lambda>: 0.7931 - val_loss: 0.8616 - val_<lambda>: 0.8616\n",
      "Epoch 13/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 0.7836 - <lambda>: 0.7836 - val_loss: 0.8443 - val_<lambda>: 0.8443\n",
      "Epoch 14/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 0.7888 - <lambda>: 0.7888 - val_loss: 0.8583 - val_<lambda>: 0.8583\n",
      "Epoch 15/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 0.7794 - <lambda>: 0.7794 - val_loss: 0.8691 - val_<lambda>: 0.8691\n",
      "Epoch 16/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 0.7923 - <lambda>: 0.7923 - val_loss: 0.8878 - val_<lambda>: 0.8878\n",
      "Epoch 17/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 0.7708 - <lambda>: 0.7708 - val_loss: 0.8450 - val_<lambda>: 0.8450\n",
      "Epoch 18/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 0.7669 - <lambda>: 0.7669 - val_loss: 0.8708 - val_<lambda>: 0.8708\n",
      "Epoch 19/500\n",
      "287/287 [==============================] - 7s 26ms/step - loss: 0.7677 - <lambda>: 0.7677 - val_loss: 0.8394 - val_<lambda>: 0.8394\n",
      "Epoch 20/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 0.7661 - <lambda>: 0.7661 - val_loss: 0.8940 - val_<lambda>: 0.8940\n",
      "Epoch 21/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 0.7563 - <lambda>: 0.7563 - val_loss: 0.8596 - val_<lambda>: 0.8596\n",
      "Epoch 22/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7628 - <lambda>: 0.7628 - val_loss: 0.8679 - val_<lambda>: 0.8679\n",
      "Epoch 23/500\n",
      "287/287 [==============================] - 6s 19ms/step - loss: 0.7475 - <lambda>: 0.7475 - val_loss: 0.8778 - val_<lambda>: 0.8778\n",
      "Epoch 24/500\n",
      "287/287 [==============================] - 6s 19ms/step - loss: 0.7568 - <lambda>: 0.7568 - val_loss: 0.8407 - val_<lambda>: 0.8407\n",
      "Epoch 25/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7524 - <lambda>: 0.7524 - val_loss: 0.8832 - val_<lambda>: 0.8832\n",
      "Epoch 26/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7427 - <lambda>: 0.7427 - val_loss: 0.8885 - val_<lambda>: 0.8885\n",
      "Epoch 27/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7474 - <lambda>: 0.7474 - val_loss: 0.8540 - val_<lambda>: 0.8540\n",
      "Epoch 28/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7459 - <lambda>: 0.7459 - val_loss: 0.8869 - val_<lambda>: 0.8869\n",
      "Epoch 29/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7483 - <lambda>: 0.7483 - val_loss: 0.8580 - val_<lambda>: 0.8580\n",
      "Epoch 30/500\n",
      "287/287 [==============================] - 7s 26ms/step - loss: 0.7348 - <lambda>: 0.7348 - val_loss: 0.8339 - val_<lambda>: 0.8339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/500\n",
      "287/287 [==============================] - 6s 20ms/step - loss: 0.7365 - <lambda>: 0.7365 - val_loss: 0.8230 - val_<lambda>: 0.8230\n",
      "Epoch 32/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7450 - <lambda>: 0.7450 - val_loss: 0.8451 - val_<lambda>: 0.8451\n",
      "Epoch 33/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7361 - <lambda>: 0.7361 - val_loss: 0.8953 - val_<lambda>: 0.8953\n",
      "Epoch 34/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7228 - <lambda>: 0.7228 - val_loss: 0.8501 - val_<lambda>: 0.8501\n",
      "Epoch 35/500\n",
      "287/287 [==============================] - 6s 19ms/step - loss: 0.7283 - <lambda>: 0.7283 - val_loss: 0.8568 - val_<lambda>: 0.8568\n",
      "Epoch 36/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7314 - <lambda>: 0.7314 - val_loss: 0.8457 - val_<lambda>: 0.8457\n",
      "Epoch 37/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7360 - <lambda>: 0.7360 - val_loss: 0.8616 - val_<lambda>: 0.8616\n",
      "Epoch 38/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7234 - <lambda>: 0.7234 - val_loss: 0.8480 - val_<lambda>: 0.8480\n",
      "Epoch 39/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7208 - <lambda>: 0.7208 - val_loss: 0.8649 - val_<lambda>: 0.8649\n",
      "Epoch 40/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7345 - <lambda>: 0.7345 - val_loss: 0.8642 - val_<lambda>: 0.8642\n",
      "Epoch 41/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7217 - <lambda>: 0.7217 - val_loss: 0.8406 - val_<lambda>: 0.8406\n",
      "Epoch 42/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7149 - <lambda>: 0.7149 - val_loss: 0.8230 - val_<lambda>: 0.8230\n",
      "Epoch 43/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7184 - <lambda>: 0.7184 - val_loss: 0.8703 - val_<lambda>: 0.8703\n",
      "Epoch 44/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7169 - <lambda>: 0.7169 - val_loss: 0.8879 - val_<lambda>: 0.8879\n",
      "Epoch 45/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7080 - <lambda>: 0.7080 - val_loss: 0.8354 - val_<lambda>: 0.8354\n",
      "Epoch 46/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7157 - <lambda>: 0.7157 - val_loss: 0.8302 - val_<lambda>: 0.8302\n",
      "Epoch 47/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7193 - <lambda>: 0.7193 - val_loss: 0.8579 - val_<lambda>: 0.8579\n",
      "Epoch 48/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7092 - <lambda>: 0.7092 - val_loss: 0.8602 - val_<lambda>: 0.8602\n",
      "Epoch 49/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7116 - <lambda>: 0.7116 - val_loss: 0.8538 - val_<lambda>: 0.8538\n",
      "Epoch 50/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7113 - <lambda>: 0.7113 - val_loss: 0.8310 - val_<lambda>: 0.8310\n",
      "Epoch 51/500\n",
      "287/287 [==============================] - ETA: 0s - loss: 0.7068 - <lambda>: 0.7068\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7068 - <lambda>: 0.7068 - val_loss: 0.8959 - val_<lambda>: 0.8959\n",
      "Epoch 52/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.6952 - <lambda>: 0.6952 - val_loss: 0.8579 - val_<lambda>: 0.8579\n",
      "Epoch 53/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.6981 - <lambda>: 0.6981 - val_loss: 0.9039 - val_<lambda>: 0.9039\n",
      "Epoch 54/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.6958 - <lambda>: 0.6958 - val_loss: 0.8362 - val_<lambda>: 0.8362\n",
      "Epoch 55/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.6926 - <lambda>: 0.6926 - val_loss: 0.8847 - val_<lambda>: 0.8847\n",
      "Epoch 56/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.6933 - <lambda>: 0.6933 - val_loss: 0.8926 - val_<lambda>: 0.8926\n",
      "Epoch 57/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.6970 - <lambda>: 0.6970 - val_loss: 0.8569 - val_<lambda>: 0.8569\n",
      "Epoch 58/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.6865 - <lambda>: 0.6865 - val_loss: 0.8903 - val_<lambda>: 0.8903\n",
      "Epoch 59/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.6876 - <lambda>: 0.6876 - val_loss: 0.8509 - val_<lambda>: 0.8509\n",
      "Epoch 60/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.6863 - <lambda>: 0.6863 - val_loss: 0.8517 - val_<lambda>: 0.8517\n",
      "Epoch 61/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.6882 - <lambda>: 0.6882 - val_loss: 0.8744 - val_<lambda>: 0.8744\n",
      "\n",
      "\n",
      "  0.1 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "287/287 [==============================] - 6s 22ms/step - loss: 1.4524 - <lambda>: 1.4524 - val_loss: 1.6561 - val_<lambda>: 1.6561\n",
      "Epoch 2/500\n",
      "287/287 [==============================] - 6s 19ms/step - loss: 1.4041 - <lambda>: 1.4041 - val_loss: 1.5511 - val_<lambda>: 1.5511\n",
      "Epoch 3/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3980 - <lambda>: 1.3980 - val_loss: 1.5569 - val_<lambda>: 1.5569\n",
      "Epoch 4/500\n",
      "287/287 [==============================] - 8s 27ms/step - loss: 1.3879 - <lambda>: 1.3879 - val_loss: 1.5300 - val_<lambda>: 1.5300\n",
      "Epoch 5/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3767 - <lambda>: 1.3767 - val_loss: 1.5610 - val_<lambda>: 1.5610\n",
      "Epoch 6/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3802 - <lambda>: 1.3802 - val_loss: 1.5521 - val_<lambda>: 1.5521\n",
      "Epoch 7/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3699 - <lambda>: 1.3699 - val_loss: 1.5611 - val_<lambda>: 1.5611\n",
      "Epoch 8/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3563 - <lambda>: 1.3563 - val_loss: 1.6117 - val_<lambda>: 1.6117\n",
      "Epoch 9/500\n",
      "287/287 [==============================] - 7s 26ms/step - loss: 1.3693 - <lambda>: 1.3693 - val_loss: 1.5029 - val_<lambda>: 1.5029\n",
      "Epoch 10/500\n",
      "287/287 [==============================] - 6s 19ms/step - loss: 1.3559 - <lambda>: 1.3559 - val_loss: 1.5614 - val_<lambda>: 1.5614\n",
      "Epoch 11/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3544 - <lambda>: 1.3544 - val_loss: 1.5233 - val_<lambda>: 1.5233\n",
      "Epoch 12/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3432 - <lambda>: 1.3432 - val_loss: 1.5614 - val_<lambda>: 1.5614\n",
      "Epoch 13/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3476 - <lambda>: 1.3476 - val_loss: 1.5361 - val_<lambda>: 1.5361\n",
      "Epoch 14/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3374 - <lambda>: 1.3374 - val_loss: 1.5256 - val_<lambda>: 1.5256\n",
      "Epoch 15/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3267 - <lambda>: 1.3267 - val_loss: 1.5808 - val_<lambda>: 1.5808\n",
      "Epoch 16/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3277 - <lambda>: 1.3277 - val_loss: 1.5669 - val_<lambda>: 1.5669\n",
      "Epoch 17/500\n",
      "287/287 [==============================] - 7s 26ms/step - loss: 1.3126 - <lambda>: 1.3126 - val_loss: 1.5010 - val_<lambda>: 1.5010\n",
      "Epoch 18/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3122 - <lambda>: 1.3122 - val_loss: 1.5579 - val_<lambda>: 1.5579\n",
      "Epoch 19/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2993 - <lambda>: 1.2993 - val_loss: 1.5810 - val_<lambda>: 1.5810\n",
      "Epoch 20/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3025 - <lambda>: 1.3025 - val_loss: 1.5885 - val_<lambda>: 1.5885\n",
      "Epoch 21/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2913 - <lambda>: 1.2913 - val_loss: 1.5814 - val_<lambda>: 1.5814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2932 - <lambda>: 1.2932 - val_loss: 1.5835 - val_<lambda>: 1.5835\n",
      "Epoch 23/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2783 - <lambda>: 1.2783 - val_loss: 1.5499 - val_<lambda>: 1.5499\n",
      "Epoch 24/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.2676 - <lambda>: 1.2676 - val_loss: 1.5787 - val_<lambda>: 1.5787\n",
      "Epoch 25/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2638 - <lambda>: 1.2638 - val_loss: 1.5459 - val_<lambda>: 1.5459\n",
      "Epoch 26/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2614 - <lambda>: 1.2614 - val_loss: 1.5379 - val_<lambda>: 1.5379\n",
      "Epoch 27/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2566 - <lambda>: 1.2566 - val_loss: 1.5428 - val_<lambda>: 1.5428\n",
      "Epoch 28/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2509 - <lambda>: 1.2509 - val_loss: 1.5948 - val_<lambda>: 1.5948\n",
      "Epoch 29/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.2413 - <lambda>: 1.2413 - val_loss: 1.5897 - val_<lambda>: 1.5897\n",
      "Epoch 30/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2374 - <lambda>: 1.2374 - val_loss: 1.6132 - val_<lambda>: 1.6132\n",
      "Epoch 31/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2336 - <lambda>: 1.2336 - val_loss: 1.6208 - val_<lambda>: 1.6208\n",
      "Epoch 32/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2223 - <lambda>: 1.2223 - val_loss: 1.5792 - val_<lambda>: 1.5792\n",
      "Epoch 33/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2242 - <lambda>: 1.2242 - val_loss: 1.6031 - val_<lambda>: 1.6031\n",
      "Epoch 34/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2166 - <lambda>: 1.2166 - val_loss: 1.5955 - val_<lambda>: 1.5955\n",
      "Epoch 35/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2051 - <lambda>: 1.2051 - val_loss: 1.6081 - val_<lambda>: 1.6081\n",
      "Epoch 36/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.2009 - <lambda>: 1.2009 - val_loss: 1.6574 - val_<lambda>: 1.6574\n",
      "Epoch 37/500\n",
      "287/287 [==============================] - ETA: 0s - loss: 1.1997 - <lambda>: 1.1997\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.1997 - <lambda>: 1.1997 - val_loss: 1.6373 - val_<lambda>: 1.6373\n",
      "Epoch 38/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.1901 - <lambda>: 1.1901 - val_loss: 1.6916 - val_<lambda>: 1.6916\n",
      "Epoch 39/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.1805 - <lambda>: 1.1805 - val_loss: 1.6818 - val_<lambda>: 1.6818\n",
      "Epoch 40/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.1837 - <lambda>: 1.1837 - val_loss: 1.6159 - val_<lambda>: 1.6159\n",
      "Epoch 41/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.1753 - <lambda>: 1.1753 - val_loss: 1.6836 - val_<lambda>: 1.6836\n",
      "Epoch 42/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.1582 - <lambda>: 1.1582 - val_loss: 1.6726 - val_<lambda>: 1.6726\n",
      "Epoch 43/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.1654 - <lambda>: 1.1654 - val_loss: 1.6379 - val_<lambda>: 1.6379\n",
      "Epoch 44/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.1569 - <lambda>: 1.1569 - val_loss: 1.6275 - val_<lambda>: 1.6275\n",
      "Epoch 45/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.1559 - <lambda>: 1.1559 - val_loss: 1.6546 - val_<lambda>: 1.6546\n",
      "Epoch 46/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.1406 - <lambda>: 1.1406 - val_loss: 1.6845 - val_<lambda>: 1.6845\n",
      "Epoch 47/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.1396 - <lambda>: 1.1396 - val_loss: 1.6692 - val_<lambda>: 1.6692\n",
      "\n",
      "\n",
      "  0.2 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "286/287 [============================>.] - ETA: 0s - loss: 2.4130 - <lambda>: 2.4130WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0080s). Check your callbacks.\n",
      "287/287 [==============================] - 6s 23ms/step - loss: 2.4132 - <lambda>: 2.4132 - val_loss: 2.7184 - val_<lambda>: 2.7184\n",
      "Epoch 2/500\n",
      "287/287 [==============================] - 6s 20ms/step - loss: 2.3038 - <lambda>: 2.3038 - val_loss: 2.5310 - val_<lambda>: 2.5310\n",
      "Epoch 3/500\n",
      "287/287 [==============================] - 6s 20ms/step - loss: 2.2935 - <lambda>: 2.2935 - val_loss: 2.5275 - val_<lambda>: 2.5275\n",
      "Epoch 4/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.2579 - <lambda>: 2.2579 - val_loss: 2.8081 - val_<lambda>: 2.8081\n",
      "Epoch 5/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.2388 - <lambda>: 2.2388 - val_loss: 2.6345 - val_<lambda>: 2.6345\n",
      "Epoch 6/500\n",
      "287/287 [==============================] - 7s 26ms/step - loss: 2.2262 - <lambda>: 2.2262 - val_loss: 2.4770 - val_<lambda>: 2.4770\n",
      "Epoch 7/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.2180 - <lambda>: 2.2180 - val_loss: 2.5001 - val_<lambda>: 2.5001\n",
      "Epoch 8/500\n",
      "287/287 [==============================] - 6s 20ms/step - loss: 2.2056 - <lambda>: 2.2056 - val_loss: 2.4700 - val_<lambda>: 2.4700\n",
      "Epoch 9/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.1889 - <lambda>: 2.1889 - val_loss: 2.5207 - val_<lambda>: 2.5207\n",
      "Epoch 10/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.1781 - <lambda>: 2.1781 - val_loss: 2.6661 - val_<lambda>: 2.6661\n",
      "Epoch 11/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.1638 - <lambda>: 2.1638 - val_loss: 2.4805 - val_<lambda>: 2.4805\n",
      "Epoch 12/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.1576 - <lambda>: 2.1576 - val_loss: 2.5306 - val_<lambda>: 2.5306\n",
      "Epoch 13/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.1437 - <lambda>: 2.1437 - val_loss: 2.5342 - val_<lambda>: 2.5342\n",
      "Epoch 14/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.1292 - <lambda>: 2.1292 - val_loss: 2.5722 - val_<lambda>: 2.5722\n",
      "Epoch 15/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.1217 - <lambda>: 2.1217 - val_loss: 2.6538 - val_<lambda>: 2.6538\n",
      "Epoch 16/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.1044 - <lambda>: 2.1044 - val_loss: 2.5448 - val_<lambda>: 2.5448\n",
      "Epoch 17/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.0887 - <lambda>: 2.0887 - val_loss: 2.5340 - val_<lambda>: 2.5340\n",
      "Epoch 18/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.0863 - <lambda>: 2.0863 - val_loss: 2.5450 - val_<lambda>: 2.5450\n",
      "Epoch 19/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.0624 - <lambda>: 2.0624 - val_loss: 2.6148 - val_<lambda>: 2.6148\n",
      "Epoch 20/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.0705 - <lambda>: 2.0705 - val_loss: 2.5835 - val_<lambda>: 2.5835\n",
      "Epoch 21/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.0445 - <lambda>: 2.0445 - val_loss: 2.5873 - val_<lambda>: 2.5873\n",
      "Epoch 22/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.0494 - <lambda>: 2.0494 - val_loss: 2.5962 - val_<lambda>: 2.5962\n",
      "Epoch 23/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.0455 - <lambda>: 2.0455 - val_loss: 2.5149 - val_<lambda>: 2.5149\n",
      "Epoch 24/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.0170 - <lambda>: 2.0170 - val_loss: 2.5731 - val_<lambda>: 2.5731\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287/287 [==============================] - 5s 19ms/step - loss: 2.0141 - <lambda>: 2.0141 - val_loss: 2.5966 - val_<lambda>: 2.5966\n",
      "Epoch 26/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.9903 - <lambda>: 1.9903 - val_loss: 2.6595 - val_<lambda>: 2.6595\n",
      "Epoch 27/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.9849 - <lambda>: 1.9849 - val_loss: 2.5898 - val_<lambda>: 2.5898\n",
      "Epoch 28/500\n",
      "285/287 [============================>.] - ETA: 0s - loss: 1.9676 - <lambda>: 1.9676\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.9707 - <lambda>: 1.9707 - val_loss: 2.6474 - val_<lambda>: 2.6474\n",
      "Epoch 29/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.9590 - <lambda>: 1.9590 - val_loss: 2.7105 - val_<lambda>: 2.7105\n",
      "Epoch 30/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.9320 - <lambda>: 1.9320 - val_loss: 2.5935 - val_<lambda>: 2.5935\n",
      "Epoch 31/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.9141 - <lambda>: 1.9141 - val_loss: 2.7294 - val_<lambda>: 2.7294\n",
      "Epoch 32/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9060 - <lambda>: 1.9060 - val_loss: 2.6931 - val_<lambda>: 2.6931\n",
      "Epoch 33/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.9029 - <lambda>: 1.9029 - val_loss: 2.6779 - val_<lambda>: 2.6779\n",
      "Epoch 34/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.8779 - <lambda>: 1.8779 - val_loss: 2.6159 - val_<lambda>: 2.6159\n",
      "Epoch 35/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.8816 - <lambda>: 1.8816 - val_loss: 2.5996 - val_<lambda>: 2.5996\n",
      "Epoch 36/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.8721 - <lambda>: 1.8721 - val_loss: 2.7223 - val_<lambda>: 2.7223\n",
      "Epoch 37/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.8533 - <lambda>: 1.8533 - val_loss: 2.6867 - val_<lambda>: 2.6867\n",
      "Epoch 38/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.8487 - <lambda>: 1.8487 - val_loss: 2.6796 - val_<lambda>: 2.6796\n",
      "\n",
      "\n",
      "  0.3 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_37 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "287/287 [==============================] - 6s 22ms/step - loss: 2.9655 - <lambda>: 2.9655 - val_loss: 3.5235 - val_<lambda>: 3.5235\n",
      "Epoch 2/500\n",
      "287/287 [==============================] - 6s 19ms/step - loss: 2.8052 - <lambda>: 2.8052 - val_loss: 3.0693 - val_<lambda>: 3.0693\n",
      "Epoch 3/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.7398 - <lambda>: 2.7398 - val_loss: 3.1226 - val_<lambda>: 3.1226\n",
      "Epoch 4/500\n",
      "287/287 [==============================] - 6s 19ms/step - loss: 2.7301 - <lambda>: 2.7301 - val_loss: 3.0217 - val_<lambda>: 3.0217\n",
      "Epoch 5/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.7054 - <lambda>: 2.7054 - val_loss: 3.0931 - val_<lambda>: 3.0931\n",
      "Epoch 6/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.7041 - <lambda>: 2.7041 - val_loss: 3.0286 - val_<lambda>: 3.0286\n",
      "Epoch 7/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6731 - <lambda>: 2.6731 - val_loss: 3.1770 - val_<lambda>: 3.1770\n",
      "Epoch 8/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6659 - <lambda>: 2.6659 - val_loss: 3.0708 - val_<lambda>: 3.0708\n",
      "Epoch 9/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.6458 - <lambda>: 2.6458 - val_loss: 3.2647 - val_<lambda>: 3.2647\n",
      "Epoch 10/500\n",
      "287/287 [==============================] - 8s 28ms/step - loss: 2.6375 - <lambda>: 2.6375 - val_loss: 2.9907 - val_<lambda>: 2.9907\n",
      "Epoch 11/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.6230 - <lambda>: 2.6230 - val_loss: 3.0044 - val_<lambda>: 3.0044\n",
      "Epoch 12/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.6091 - <lambda>: 2.6091 - val_loss: 3.0905 - val_<lambda>: 3.0905\n",
      "Epoch 13/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5911 - <lambda>: 2.5911 - val_loss: 3.0293 - val_<lambda>: 3.0293\n",
      "Epoch 14/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5803 - <lambda>: 2.5803 - val_loss: 3.1047 - val_<lambda>: 3.1047\n",
      "Epoch 15/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5643 - <lambda>: 2.5643 - val_loss: 3.0570 - val_<lambda>: 3.0570\n",
      "Epoch 16/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5490 - <lambda>: 2.5490 - val_loss: 3.0028 - val_<lambda>: 3.0028\n",
      "Epoch 17/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5420 - <lambda>: 2.5420 - val_loss: 3.0585 - val_<lambda>: 3.0585\n",
      "Epoch 18/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5348 - <lambda>: 2.5348 - val_loss: 3.0643 - val_<lambda>: 3.0643\n",
      "Epoch 19/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5168 - <lambda>: 2.5168 - val_loss: 3.0398 - val_<lambda>: 3.0398\n",
      "Epoch 20/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5012 - <lambda>: 2.5012 - val_loss: 3.1011 - val_<lambda>: 3.1011\n",
      "Epoch 21/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4777 - <lambda>: 2.4777 - val_loss: 3.0665 - val_<lambda>: 3.0665\n",
      "Epoch 22/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4939 - <lambda>: 2.4939 - val_loss: 3.1030 - val_<lambda>: 3.1030\n",
      "Epoch 23/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4522 - <lambda>: 2.4522 - val_loss: 3.0464 - val_<lambda>: 3.0464\n",
      "Epoch 24/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4410 - <lambda>: 2.4410 - val_loss: 3.0428 - val_<lambda>: 3.0428\n",
      "Epoch 25/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4247 - <lambda>: 2.4247 - val_loss: 3.0980 - val_<lambda>: 3.0980\n",
      "Epoch 26/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4182 - <lambda>: 2.4182 - val_loss: 3.1704 - val_<lambda>: 3.1704\n",
      "Epoch 27/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4173 - <lambda>: 2.4173 - val_loss: 3.1021 - val_<lambda>: 3.1021\n",
      "Epoch 28/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4028 - <lambda>: 2.4028 - val_loss: 3.1176 - val_<lambda>: 3.1176\n",
      "Epoch 29/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3720 - <lambda>: 2.3720 - val_loss: 3.0930 - val_<lambda>: 3.0930\n",
      "Epoch 30/500\n",
      "284/287 [============================>.] - ETA: 0s - loss: 2.3591 - <lambda>: 2.3591\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3584 - <lambda>: 2.3584 - val_loss: 3.1456 - val_<lambda>: 3.1456\n",
      "Epoch 31/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3247 - <lambda>: 2.3247 - val_loss: 3.1826 - val_<lambda>: 3.1826\n",
      "Epoch 32/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3311 - <lambda>: 2.3311 - val_loss: 3.1235 - val_<lambda>: 3.1235\n",
      "Epoch 33/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3091 - <lambda>: 2.3091 - val_loss: 3.2134 - val_<lambda>: 3.2134\n",
      "Epoch 34/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2998 - <lambda>: 2.2998 - val_loss: 3.1503 - val_<lambda>: 3.1503\n",
      "Epoch 35/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2843 - <lambda>: 2.2843 - val_loss: 3.1770 - val_<lambda>: 3.1770\n",
      "Epoch 36/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2848 - <lambda>: 2.2848 - val_loss: 3.2280 - val_<lambda>: 3.2280\n",
      "Epoch 37/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2488 - <lambda>: 2.2488 - val_loss: 3.2627 - val_<lambda>: 3.2627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/500\n",
      "287/287 [==============================] - 5s 17ms/step - loss: 2.2457 - <lambda>: 2.2457 - val_loss: 3.2490 - val_<lambda>: 3.2490\n",
      "Epoch 39/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2349 - <lambda>: 2.2349 - val_loss: 3.1215 - val_<lambda>: 3.1215\n",
      "Epoch 40/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2404 - <lambda>: 2.2404 - val_loss: 3.3606 - val_<lambda>: 3.3606\n",
      "\n",
      "\n",
      "  0.4 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_39 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_40 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_41 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "287/287 [==============================] - 6s 21ms/step - loss: 3.2473 - <lambda>: 3.2473 - val_loss: 3.6497 - val_<lambda>: 3.6497\n",
      "Epoch 2/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.9582 - <lambda>: 2.9582 - val_loss: 3.1886 - val_<lambda>: 3.1886\n",
      "Epoch 3/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.9130 - <lambda>: 2.9130 - val_loss: 3.1979 - val_<lambda>: 3.1979\n",
      "Epoch 4/500\n",
      "287/287 [==============================] - 8s 26ms/step - loss: 2.8834 - <lambda>: 2.8834 - val_loss: 3.1696 - val_<lambda>: 3.1696\n",
      "Epoch 5/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.8587 - <lambda>: 2.8587 - val_loss: 3.3916 - val_<lambda>: 3.3916\n",
      "Epoch 6/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.8490 - <lambda>: 2.8490 - val_loss: 3.1164 - val_<lambda>: 3.1164\n",
      "Epoch 7/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.8226 - <lambda>: 2.8226 - val_loss: 3.1626 - val_<lambda>: 3.1626\n",
      "Epoch 8/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.8128 - <lambda>: 2.8128 - val_loss: 3.1338 - val_<lambda>: 3.1338\n",
      "Epoch 9/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.7890 - <lambda>: 2.7890 - val_loss: 3.2279 - val_<lambda>: 3.2279\n",
      "Epoch 10/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.7920 - <lambda>: 2.7920 - val_loss: 3.1884 - val_<lambda>: 3.1884\n",
      "Epoch 11/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.7805 - <lambda>: 2.7805 - val_loss: 3.1865 - val_<lambda>: 3.1865\n",
      "Epoch 12/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.7778 - <lambda>: 2.7778 - val_loss: 3.2277 - val_<lambda>: 3.2277\n",
      "Epoch 13/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.7477 - <lambda>: 2.7477 - val_loss: 3.2428 - val_<lambda>: 3.2428\n",
      "Epoch 14/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.7375 - <lambda>: 2.7375 - val_loss: 3.3453 - val_<lambda>: 3.3453\n",
      "Epoch 15/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.7220 - <lambda>: 2.7220 - val_loss: 3.1651 - val_<lambda>: 3.1651\n",
      "Epoch 16/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.7016 - <lambda>: 2.7016 - val_loss: 3.3488 - val_<lambda>: 3.3488\n",
      "Epoch 17/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.7024 - <lambda>: 2.7024 - val_loss: 3.2499 - val_<lambda>: 3.2499\n",
      "Epoch 18/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6688 - <lambda>: 2.6688 - val_loss: 3.2038 - val_<lambda>: 3.2038\n",
      "Epoch 19/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6644 - <lambda>: 2.6644 - val_loss: 3.2294 - val_<lambda>: 3.2294\n",
      "Epoch 20/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6528 - <lambda>: 2.6528 - val_loss: 3.3286 - val_<lambda>: 3.3286\n",
      "Epoch 21/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6348 - <lambda>: 2.6348 - val_loss: 3.2411 - val_<lambda>: 3.2411\n",
      "Epoch 22/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6236 - <lambda>: 2.6236 - val_loss: 3.2231 - val_<lambda>: 3.2231\n",
      "Epoch 23/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5974 - <lambda>: 2.5974 - val_loss: 3.2491 - val_<lambda>: 3.2491\n",
      "Epoch 24/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5964 - <lambda>: 2.5964 - val_loss: 3.2512 - val_<lambda>: 3.2512\n",
      "Epoch 25/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5759 - <lambda>: 2.5759 - val_loss: 3.3564 - val_<lambda>: 3.3564\n",
      "Epoch 26/500\n",
      "287/287 [==============================] - ETA: 0s - loss: 2.5707 - <lambda>: 2.5707\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5707 - <lambda>: 2.5707 - val_loss: 3.1841 - val_<lambda>: 3.1841\n",
      "Epoch 27/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5352 - <lambda>: 2.5352 - val_loss: 3.2458 - val_<lambda>: 3.2458\n",
      "Epoch 28/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5180 - <lambda>: 2.5180 - val_loss: 3.3078 - val_<lambda>: 3.3078\n",
      "Epoch 29/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5087 - <lambda>: 2.5087 - val_loss: 3.2674 - val_<lambda>: 3.2674\n",
      "Epoch 30/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4924 - <lambda>: 2.4924 - val_loss: 3.3038 - val_<lambda>: 3.3038\n",
      "Epoch 31/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4811 - <lambda>: 2.4811 - val_loss: 3.2746 - val_<lambda>: 3.2746\n",
      "Epoch 32/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4770 - <lambda>: 2.4770 - val_loss: 3.3415 - val_<lambda>: 3.3415\n",
      "Epoch 33/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4642 - <lambda>: 2.4642 - val_loss: 3.3313 - val_<lambda>: 3.3313\n",
      "Epoch 34/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4436 - <lambda>: 2.4436 - val_loss: 3.2977 - val_<lambda>: 3.2977\n",
      "Epoch 35/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4247 - <lambda>: 2.4247 - val_loss: 3.3740 - val_<lambda>: 3.3740\n",
      "Epoch 36/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4251 - <lambda>: 2.4251 - val_loss: 3.3256 - val_<lambda>: 3.3256\n",
      "\n",
      "\n",
      "  0.5 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_42 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_43 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_44 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "287/287 [==============================] - 6s 22ms/step - loss: 3.4198 - <lambda>: 3.4198 - val_loss: 3.8077 - val_<lambda>: 3.8077\n",
      "Epoch 2/500\n",
      "287/287 [==============================] - 6s 20ms/step - loss: 2.8646 - <lambda>: 2.8646 - val_loss: 3.0454 - val_<lambda>: 3.0454\n",
      "Epoch 3/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.8156 - <lambda>: 2.8156 - val_loss: 3.0696 - val_<lambda>: 3.0696\n",
      "Epoch 4/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.7622 - <lambda>: 2.7622 - val_loss: 3.1379 - val_<lambda>: 3.1379\n",
      "Epoch 5/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.7566 - <lambda>: 2.7566 - val_loss: 3.0631 - val_<lambda>: 3.0631\n",
      "Epoch 6/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.7371 - <lambda>: 2.7371 - val_loss: 3.1162 - val_<lambda>: 3.1162\n",
      "Epoch 7/500\n",
      "287/287 [==============================] - 7s 26ms/step - loss: 2.7315 - <lambda>: 2.7315 - val_loss: 3.0394 - val_<lambda>: 3.0394\n",
      "Epoch 8/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.7082 - <lambda>: 2.7082 - val_loss: 3.0444 - val_<lambda>: 3.0444\n",
      "Epoch 9/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6870 - <lambda>: 2.6870 - val_loss: 3.2377 - val_<lambda>: 3.2377\n",
      "Epoch 10/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287/287 [==============================] - 7s 25ms/step - loss: 2.7014 - <lambda>: 2.7014 - val_loss: 3.0250 - val_<lambda>: 3.0250\n",
      "Epoch 11/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6786 - <lambda>: 2.6786 - val_loss: 3.3678 - val_<lambda>: 3.3678\n",
      "Epoch 12/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6738 - <lambda>: 2.6738 - val_loss: 3.0370 - val_<lambda>: 3.0370\n",
      "Epoch 13/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6788 - <lambda>: 2.6788 - val_loss: 3.0769 - val_<lambda>: 3.0769\n",
      "Epoch 14/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6560 - <lambda>: 2.6560 - val_loss: 3.1073 - val_<lambda>: 3.1073\n",
      "Epoch 15/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6397 - <lambda>: 2.6397 - val_loss: 3.0398 - val_<lambda>: 3.0398\n",
      "Epoch 16/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6226 - <lambda>: 2.6226 - val_loss: 3.0411 - val_<lambda>: 3.0411\n",
      "Epoch 17/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.6170 - <lambda>: 2.6170 - val_loss: 3.0372 - val_<lambda>: 3.0372\n",
      "Epoch 18/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5854 - <lambda>: 2.5854 - val_loss: 3.0772 - val_<lambda>: 3.0772\n",
      "Epoch 19/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5749 - <lambda>: 2.5749 - val_loss: 3.2310 - val_<lambda>: 3.2310\n",
      "Epoch 20/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5696 - <lambda>: 2.5696 - val_loss: 3.1190 - val_<lambda>: 3.1190\n",
      "Epoch 21/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5484 - <lambda>: 2.5484 - val_loss: 3.1538 - val_<lambda>: 3.1538\n",
      "Epoch 22/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5550 - <lambda>: 2.5550 - val_loss: 3.1041 - val_<lambda>: 3.1041\n",
      "Epoch 23/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5392 - <lambda>: 2.5392 - val_loss: 3.3027 - val_<lambda>: 3.3027\n",
      "Epoch 24/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5300 - <lambda>: 2.5300 - val_loss: 3.1994 - val_<lambda>: 3.1994\n",
      "Epoch 25/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5074 - <lambda>: 2.5074 - val_loss: 3.0822 - val_<lambda>: 3.0822\n",
      "Epoch 26/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5054 - <lambda>: 2.5054 - val_loss: 3.1705 - val_<lambda>: 3.1705\n",
      "Epoch 27/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4786 - <lambda>: 2.4786 - val_loss: 3.1879 - val_<lambda>: 3.1879\n",
      "Epoch 28/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4699 - <lambda>: 2.4699 - val_loss: 3.2024 - val_<lambda>: 3.2024\n",
      "Epoch 29/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4759 - <lambda>: 2.4759 - val_loss: 3.1384 - val_<lambda>: 3.1384\n",
      "Epoch 30/500\n",
      "287/287 [==============================] - ETA: 0s - loss: 2.4527 - <lambda>: 2.4527\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4527 - <lambda>: 2.4527 - val_loss: 3.1284 - val_<lambda>: 3.1284\n",
      "Epoch 31/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4314 - <lambda>: 2.4314 - val_loss: 3.1983 - val_<lambda>: 3.1983\n",
      "Epoch 32/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4084 - <lambda>: 2.4084 - val_loss: 3.1988 - val_<lambda>: 3.1988\n",
      "Epoch 33/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4060 - <lambda>: 2.4060 - val_loss: 3.1780 - val_<lambda>: 3.1780\n",
      "Epoch 34/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3812 - <lambda>: 2.3812 - val_loss: 3.1961 - val_<lambda>: 3.1961\n",
      "Epoch 35/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3966 - <lambda>: 2.3966 - val_loss: 3.2774 - val_<lambda>: 3.2774\n",
      "Epoch 36/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3640 - <lambda>: 2.3640 - val_loss: 3.1951 - val_<lambda>: 3.1951\n",
      "Epoch 37/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3479 - <lambda>: 2.3479 - val_loss: 3.2076 - val_<lambda>: 3.2076\n",
      "Epoch 38/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3519 - <lambda>: 2.3519 - val_loss: 3.1507 - val_<lambda>: 3.1507\n",
      "Epoch 39/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3373 - <lambda>: 2.3373 - val_loss: 3.3406 - val_<lambda>: 3.3406\n",
      "Epoch 40/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3257 - <lambda>: 2.3257 - val_loss: 3.2684 - val_<lambda>: 3.2684\n",
      "\n",
      "\n",
      "  0.6 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_45 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_46 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_47 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "287/287 [==============================] - 6s 21ms/step - loss: 3.5983 - <lambda>: 3.5983 - val_loss: 4.4635 - val_<lambda>: 4.4635\n",
      "Epoch 2/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.5991 - <lambda>: 2.5991 - val_loss: 2.9529 - val_<lambda>: 2.9529\n",
      "Epoch 3/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.5649 - <lambda>: 2.5649 - val_loss: 2.6399 - val_<lambda>: 2.6399\n",
      "Epoch 4/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.5201 - <lambda>: 2.5201 - val_loss: 2.9483 - val_<lambda>: 2.9483\n",
      "Epoch 5/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4682 - <lambda>: 2.4682 - val_loss: 2.7508 - val_<lambda>: 2.7508\n",
      "Epoch 6/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4701 - <lambda>: 2.4701 - val_loss: 2.7656 - val_<lambda>: 2.7656\n",
      "Epoch 7/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4472 - <lambda>: 2.4472 - val_loss: 2.7320 - val_<lambda>: 2.7320\n",
      "Epoch 8/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4289 - <lambda>: 2.4289 - val_loss: 2.8172 - val_<lambda>: 2.8172\n",
      "Epoch 9/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4318 - <lambda>: 2.4318 - val_loss: 2.6839 - val_<lambda>: 2.6839\n",
      "Epoch 10/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4079 - <lambda>: 2.4079 - val_loss: 2.6918 - val_<lambda>: 2.6918\n",
      "Epoch 11/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.4005 - <lambda>: 2.4005 - val_loss: 2.7278 - val_<lambda>: 2.7278\n",
      "Epoch 12/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3834 - <lambda>: 2.3834 - val_loss: 2.7124 - val_<lambda>: 2.7124\n",
      "Epoch 13/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3924 - <lambda>: 2.3924 - val_loss: 2.7127 - val_<lambda>: 2.7127\n",
      "Epoch 14/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3709 - <lambda>: 2.3709 - val_loss: 2.7111 - val_<lambda>: 2.7111\n",
      "Epoch 15/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3548 - <lambda>: 2.3548 - val_loss: 2.6804 - val_<lambda>: 2.6804\n",
      "Epoch 16/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3552 - <lambda>: 2.3552 - val_loss: 2.6646 - val_<lambda>: 2.6646\n",
      "Epoch 17/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3369 - <lambda>: 2.3369 - val_loss: 2.6772 - val_<lambda>: 2.6772\n",
      "Epoch 18/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3498 - <lambda>: 2.3498 - val_loss: 2.7145 - val_<lambda>: 2.7145\n",
      "Epoch 19/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3335 - <lambda>: 2.3335 - val_loss: 2.7172 - val_<lambda>: 2.7172\n",
      "Epoch 20/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3303 - <lambda>: 2.3303 - val_loss: 2.7571 - val_<lambda>: 2.7571\n",
      "Epoch 21/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3172 - <lambda>: 2.3172 - val_loss: 2.7830 - val_<lambda>: 2.7830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3155 - <lambda>: 2.3155 - val_loss: 2.7651 - val_<lambda>: 2.7651\n",
      "Epoch 23/500\n",
      "286/287 [============================>.] - ETA: 0s - loss: 2.3087 - <lambda>: 2.3087\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.3068 - <lambda>: 2.3068 - val_loss: 2.7056 - val_<lambda>: 2.7056\n",
      "Epoch 24/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2816 - <lambda>: 2.2816 - val_loss: 2.7644 - val_<lambda>: 2.7644\n",
      "Epoch 25/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2874 - <lambda>: 2.2874 - val_loss: 2.8045 - val_<lambda>: 2.8045\n",
      "Epoch 26/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2691 - <lambda>: 2.2691 - val_loss: 2.8149 - val_<lambda>: 2.8149\n",
      "Epoch 27/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2704 - <lambda>: 2.2704 - val_loss: 2.7513 - val_<lambda>: 2.7513\n",
      "Epoch 28/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2575 - <lambda>: 2.2575 - val_loss: 2.7343 - val_<lambda>: 2.7343\n",
      "Epoch 29/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2429 - <lambda>: 2.2429 - val_loss: 2.7677 - val_<lambda>: 2.7677\n",
      "Epoch 30/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2364 - <lambda>: 2.2364 - val_loss: 2.7539 - val_<lambda>: 2.7539\n",
      "Epoch 31/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2439 - <lambda>: 2.2439 - val_loss: 2.8417 - val_<lambda>: 2.8417\n",
      "Epoch 32/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2217 - <lambda>: 2.2217 - val_loss: 2.7811 - val_<lambda>: 2.7811\n",
      "Epoch 33/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.2292 - <lambda>: 2.2292 - val_loss: 2.8119 - val_<lambda>: 2.8119\n",
      "\n",
      "\n",
      "  0.7 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_50 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "287/287 [==============================] - 6s 22ms/step - loss: 2.9492 - <lambda>: 2.9492 - val_loss: 3.6464 - val_<lambda>: 3.6464\n",
      "Epoch 2/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.2108 - <lambda>: 2.2108 - val_loss: 2.2411 - val_<lambda>: 2.2411\n",
      "Epoch 3/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.1331 - <lambda>: 2.1331 - val_loss: 2.4359 - val_<lambda>: 2.4359\n",
      "Epoch 4/500\n",
      "287/287 [==============================] - 7s 26ms/step - loss: 2.0700 - <lambda>: 2.0700 - val_loss: 2.2357 - val_<lambda>: 2.2357\n",
      "Epoch 5/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 2.0682 - <lambda>: 2.0682 - val_loss: 2.1669 - val_<lambda>: 2.1669\n",
      "Epoch 6/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0213 - <lambda>: 2.0213 - val_loss: 2.1950 - val_<lambda>: 2.1950\n",
      "Epoch 7/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0271 - <lambda>: 2.0271 - val_loss: 2.2276 - val_<lambda>: 2.2276\n",
      "Epoch 8/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0071 - <lambda>: 2.0071 - val_loss: 2.2196 - val_<lambda>: 2.2196\n",
      "Epoch 9/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9910 - <lambda>: 1.9910 - val_loss: 2.1911 - val_<lambda>: 2.1911\n",
      "Epoch 10/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9917 - <lambda>: 1.9917 - val_loss: 2.2849 - val_<lambda>: 2.2849\n",
      "Epoch 11/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 2.0041 - <lambda>: 2.0041 - val_loss: 2.2148 - val_<lambda>: 2.2148\n",
      "Epoch 12/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9745 - <lambda>: 1.9745 - val_loss: 2.2363 - val_<lambda>: 2.2363\n",
      "Epoch 13/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9509 - <lambda>: 1.9509 - val_loss: 2.2814 - val_<lambda>: 2.2814\n",
      "Epoch 14/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9737 - <lambda>: 1.9737 - val_loss: 2.2370 - val_<lambda>: 2.2370\n",
      "Epoch 15/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9565 - <lambda>: 1.9565 - val_loss: 2.2120 - val_<lambda>: 2.2120\n",
      "Epoch 16/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9323 - <lambda>: 1.9323 - val_loss: 2.1749 - val_<lambda>: 2.1749\n",
      "Epoch 17/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9354 - <lambda>: 1.9354 - val_loss: 2.2044 - val_<lambda>: 2.2044\n",
      "Epoch 18/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9269 - <lambda>: 1.9269 - val_loss: 2.2152 - val_<lambda>: 2.2152\n",
      "Epoch 19/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9087 - <lambda>: 1.9087 - val_loss: 2.1717 - val_<lambda>: 2.1717\n",
      "Epoch 20/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9132 - <lambda>: 1.9132 - val_loss: 2.1824 - val_<lambda>: 2.1824\n",
      "Epoch 21/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9064 - <lambda>: 1.9064 - val_loss: 2.2218 - val_<lambda>: 2.2218\n",
      "Epoch 22/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.9017 - <lambda>: 1.9017 - val_loss: 2.2267 - val_<lambda>: 2.2267\n",
      "Epoch 23/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8901 - <lambda>: 1.8901 - val_loss: 2.2080 - val_<lambda>: 2.2080\n",
      "Epoch 24/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8892 - <lambda>: 1.8892 - val_loss: 2.2196 - val_<lambda>: 2.2196\n",
      "Epoch 25/500\n",
      "286/287 [============================>.] - ETA: 0s - loss: 1.8857 - <lambda>: 1.8857\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8856 - <lambda>: 1.8856 - val_loss: 2.2084 - val_<lambda>: 2.2084\n",
      "Epoch 26/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8774 - <lambda>: 1.8774 - val_loss: 2.1841 - val_<lambda>: 2.1841\n",
      "Epoch 27/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8513 - <lambda>: 1.8513 - val_loss: 2.2001 - val_<lambda>: 2.2001\n",
      "Epoch 28/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8536 - <lambda>: 1.8536 - val_loss: 2.1991 - val_<lambda>: 2.1991\n",
      "Epoch 29/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8489 - <lambda>: 1.8489 - val_loss: 2.2543 - val_<lambda>: 2.2543\n",
      "Epoch 30/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8329 - <lambda>: 1.8329 - val_loss: 2.2184 - val_<lambda>: 2.2184\n",
      "Epoch 31/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8360 - <lambda>: 1.8360 - val_loss: 2.2896 - val_<lambda>: 2.2896\n",
      "Epoch 32/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8371 - <lambda>: 1.8371 - val_loss: 2.2248 - val_<lambda>: 2.2248\n",
      "Epoch 33/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8279 - <lambda>: 1.8279 - val_loss: 2.2198 - val_<lambda>: 2.2198\n",
      "Epoch 34/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8279 - <lambda>: 1.8279 - val_loss: 2.2462 - val_<lambda>: 2.2462\n",
      "Epoch 35/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.8161 - <lambda>: 1.8161 - val_loss: 2.3063 - val_<lambda>: 2.3063\n",
      "\n",
      "\n",
      "  0.8 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_51 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_52 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_53 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287/287 [==============================] - 6s 22ms/step - loss: 2.4691 - <lambda>: 2.4691 - val_loss: 3.5015 - val_<lambda>: 3.5015\n",
      "Epoch 2/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.6628 - <lambda>: 1.6628 - val_loss: 1.7522 - val_<lambda>: 1.7522\n",
      "Epoch 3/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.6202 - <lambda>: 1.6202 - val_loss: 1.6506 - val_<lambda>: 1.6506\n",
      "Epoch 4/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.5639 - <lambda>: 1.5639 - val_loss: 1.7068 - val_<lambda>: 1.7068\n",
      "Epoch 5/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.5394 - <lambda>: 1.5394 - val_loss: 1.6305 - val_<lambda>: 1.6305\n",
      "Epoch 6/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.5095 - <lambda>: 1.5095 - val_loss: 1.6342 - val_<lambda>: 1.6342\n",
      "Epoch 7/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.5078 - <lambda>: 1.5078 - val_loss: 1.5987 - val_<lambda>: 1.5987\n",
      "Epoch 8/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.4847 - <lambda>: 1.4847 - val_loss: 1.5971 - val_<lambda>: 1.5971\n",
      "Epoch 9/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.4694 - <lambda>: 1.4694 - val_loss: 1.6283 - val_<lambda>: 1.6283\n",
      "Epoch 10/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.4445 - <lambda>: 1.4445 - val_loss: 1.6139 - val_<lambda>: 1.6139\n",
      "Epoch 11/500\n",
      "287/287 [==============================] - 8s 27ms/step - loss: 1.4494 - <lambda>: 1.4494 - val_loss: 1.5807 - val_<lambda>: 1.5807\n",
      "Epoch 12/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.4367 - <lambda>: 1.4367 - val_loss: 1.6723 - val_<lambda>: 1.6723\n",
      "Epoch 13/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.4424 - <lambda>: 1.4424 - val_loss: 1.6648 - val_<lambda>: 1.6648\n",
      "Epoch 14/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.4300 - <lambda>: 1.4300 - val_loss: 1.5816 - val_<lambda>: 1.5816\n",
      "Epoch 15/500\n",
      "287/287 [==============================] - 8s 28ms/step - loss: 1.4228 - <lambda>: 1.4228 - val_loss: 1.5711 - val_<lambda>: 1.5711\n",
      "Epoch 16/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.4173 - <lambda>: 1.4173 - val_loss: 1.6003 - val_<lambda>: 1.6003\n",
      "Epoch 17/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.4179 - <lambda>: 1.4179 - val_loss: 1.6098 - val_<lambda>: 1.6098\n",
      "Epoch 18/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.4114 - <lambda>: 1.4114 - val_loss: 1.5825 - val_<lambda>: 1.5825\n",
      "Epoch 19/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.4024 - <lambda>: 1.4024 - val_loss: 1.5840 - val_<lambda>: 1.5840\n",
      "Epoch 20/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3977 - <lambda>: 1.3977 - val_loss: 1.6345 - val_<lambda>: 1.6345\n",
      "Epoch 21/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3957 - <lambda>: 1.3957 - val_loss: 1.6004 - val_<lambda>: 1.6004\n",
      "Epoch 22/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3875 - <lambda>: 1.3875 - val_loss: 1.6057 - val_<lambda>: 1.6057\n",
      "Epoch 23/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3903 - <lambda>: 1.3903 - val_loss: 1.5935 - val_<lambda>: 1.5935\n",
      "Epoch 24/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3879 - <lambda>: 1.3879 - val_loss: 1.6777 - val_<lambda>: 1.6777\n",
      "Epoch 25/500\n",
      "287/287 [==============================] - 7s 26ms/step - loss: 1.3923 - <lambda>: 1.3923 - val_loss: 1.5682 - val_<lambda>: 1.5682\n",
      "Epoch 26/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3830 - <lambda>: 1.3830 - val_loss: 1.5763 - val_<lambda>: 1.5763\n",
      "Epoch 27/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3693 - <lambda>: 1.3693 - val_loss: 1.5738 - val_<lambda>: 1.5738\n",
      "Epoch 28/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3669 - <lambda>: 1.3669 - val_loss: 1.5869 - val_<lambda>: 1.5869\n",
      "Epoch 29/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3590 - <lambda>: 1.3590 - val_loss: 1.5812 - val_<lambda>: 1.5812\n",
      "Epoch 30/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3606 - <lambda>: 1.3606 - val_loss: 1.5787 - val_<lambda>: 1.5787\n",
      "Epoch 31/500\n",
      "287/287 [==============================] - 5s 18ms/step - loss: 1.3679 - <lambda>: 1.3679 - val_loss: 1.6189 - val_<lambda>: 1.6189\n",
      "Epoch 32/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3508 - <lambda>: 1.3508 - val_loss: 1.5913 - val_<lambda>: 1.5913\n",
      "Epoch 33/500\n",
      "287/287 [==============================] - 6s 19ms/step - loss: 1.3561 - <lambda>: 1.3561 - val_loss: 1.6139 - val_<lambda>: 1.6139\n",
      "Epoch 34/500\n",
      "287/287 [==============================] - 6s 19ms/step - loss: 1.3457 - <lambda>: 1.3457 - val_loss: 1.6557 - val_<lambda>: 1.6557\n",
      "Epoch 35/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3476 - <lambda>: 1.3476 - val_loss: 1.6414 - val_<lambda>: 1.6414\n",
      "Epoch 36/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3556 - <lambda>: 1.3556 - val_loss: 1.5833 - val_<lambda>: 1.5833\n",
      "Epoch 37/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3442 - <lambda>: 1.3442 - val_loss: 1.6161 - val_<lambda>: 1.6161\n",
      "Epoch 38/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3379 - <lambda>: 1.3379 - val_loss: 1.6260 - val_<lambda>: 1.6260\n",
      "Epoch 39/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3373 - <lambda>: 1.3373 - val_loss: 1.6102 - val_<lambda>: 1.6102\n",
      "Epoch 40/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3301 - <lambda>: 1.3301 - val_loss: 1.6852 - val_<lambda>: 1.6852\n",
      "Epoch 41/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3217 - <lambda>: 1.3217 - val_loss: 1.5968 - val_<lambda>: 1.5968\n",
      "Epoch 42/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3387 - <lambda>: 1.3387 - val_loss: 1.6301 - val_<lambda>: 1.6301\n",
      "Epoch 43/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3242 - <lambda>: 1.3242 - val_loss: 1.6393 - val_<lambda>: 1.6393\n",
      "Epoch 44/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3201 - <lambda>: 1.3201 - val_loss: 1.6497 - val_<lambda>: 1.6497\n",
      "Epoch 45/500\n",
      "285/287 [============================>.] - ETA: 0s - loss: 1.3093 - <lambda>: 1.3093\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.3092 - <lambda>: 1.3092 - val_loss: 1.6310 - val_<lambda>: 1.6310\n",
      "Epoch 46/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2989 - <lambda>: 1.2989 - val_loss: 1.6210 - val_<lambda>: 1.6210\n",
      "Epoch 47/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2951 - <lambda>: 1.2951 - val_loss: 1.6217 - val_<lambda>: 1.6217\n",
      "Epoch 48/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2990 - <lambda>: 1.2990 - val_loss: 1.6757 - val_<lambda>: 1.6757\n",
      "Epoch 49/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2981 - <lambda>: 1.2981 - val_loss: 1.6039 - val_<lambda>: 1.6039\n",
      "Epoch 50/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2904 - <lambda>: 1.2904 - val_loss: 1.6068 - val_<lambda>: 1.6068\n",
      "Epoch 51/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2908 - <lambda>: 1.2908 - val_loss: 1.6628 - val_<lambda>: 1.6628\n",
      "Epoch 52/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2915 - <lambda>: 1.2915 - val_loss: 1.7134 - val_<lambda>: 1.7134\n",
      "Epoch 53/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2784 - <lambda>: 1.2784 - val_loss: 1.7054 - val_<lambda>: 1.7054\n",
      "Epoch 54/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2789 - <lambda>: 1.2789 - val_loss: 1.6447 - val_<lambda>: 1.6447\n",
      "Epoch 55/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 1.2915 - <lambda>: 1.2915 - val_loss: 1.6272 - val_<lambda>: 1.6272\n",
      "\n",
      "\n",
      "  0.9 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_54 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_55 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_56 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "287/287 [==============================] - 7s 25ms/step - loss: 1.9410 - <lambda>: 1.9410 - val_loss: 2.4931 - val_<lambda>: 2.4931\n",
      "Epoch 2/500\n",
      "287/287 [==============================] - 6s 20ms/step - loss: 1.0254 - <lambda>: 1.0254 - val_loss: 0.9861 - val_<lambda>: 0.9861\n",
      "Epoch 3/500\n",
      "287/287 [==============================] - 6s 20ms/step - loss: 0.9428 - <lambda>: 0.9428 - val_loss: 0.9619 - val_<lambda>: 0.9619\n",
      "Epoch 4/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.9374 - <lambda>: 0.9374 - val_loss: 0.9983 - val_<lambda>: 0.9983\n",
      "Epoch 5/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.9094 - <lambda>: 0.9094 - val_loss: 1.0223 - val_<lambda>: 1.0223\n",
      "Epoch 6/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.8815 - <lambda>: 0.8815 - val_loss: 0.9968 - val_<lambda>: 0.9968\n",
      "Epoch 7/500\n",
      "287/287 [==============================] - 8s 27ms/step - loss: 0.8851 - <lambda>: 0.8851 - val_loss: 0.8917 - val_<lambda>: 0.8917\n",
      "Epoch 8/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.8584 - <lambda>: 0.8584 - val_loss: 0.9955 - val_<lambda>: 0.9955\n",
      "Epoch 9/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.8416 - <lambda>: 0.8416 - val_loss: 0.9027 - val_<lambda>: 0.9027\n",
      "Epoch 10/500\n",
      "287/287 [==============================] - 7s 26ms/step - loss: 0.8473 - <lambda>: 0.8473 - val_loss: 0.8742 - val_<lambda>: 0.8742\n",
      "Epoch 11/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.8270 - <lambda>: 0.8270 - val_loss: 1.1424 - val_<lambda>: 1.1424\n",
      "Epoch 12/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.8134 - <lambda>: 0.8134 - val_loss: 0.8926 - val_<lambda>: 0.8926\n",
      "Epoch 13/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.8179 - <lambda>: 0.8179 - val_loss: 0.9244 - val_<lambda>: 0.9244\n",
      "Epoch 14/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.8079 - <lambda>: 0.8079 - val_loss: 0.8909 - val_<lambda>: 0.8909\n",
      "Epoch 15/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.8119 - <lambda>: 0.8119 - val_loss: 0.9774 - val_<lambda>: 0.9774\n",
      "Epoch 16/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.8029 - <lambda>: 0.8029 - val_loss: 0.8946 - val_<lambda>: 0.8946\n",
      "Epoch 17/500\n",
      "287/287 [==============================] - 7s 25ms/step - loss: 0.7971 - <lambda>: 0.7971 - val_loss: 0.8634 - val_<lambda>: 0.8634\n",
      "Epoch 18/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7933 - <lambda>: 0.7933 - val_loss: 0.8871 - val_<lambda>: 0.8871\n",
      "Epoch 19/500\n",
      "287/287 [==============================] - 6s 20ms/step - loss: 0.7913 - <lambda>: 0.7913 - val_loss: 0.8583 - val_<lambda>: 0.8583\n",
      "Epoch 20/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7865 - <lambda>: 0.7865 - val_loss: 0.8806 - val_<lambda>: 0.8806\n",
      "Epoch 21/500\n",
      "287/287 [==============================] - 6s 20ms/step - loss: 0.7927 - <lambda>: 0.7927 - val_loss: 0.9431 - val_<lambda>: 0.9431\n",
      "Epoch 22/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7822 - <lambda>: 0.7822 - val_loss: 0.8681 - val_<lambda>: 0.8681\n",
      "Epoch 23/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7814 - <lambda>: 0.7814 - val_loss: 0.8812 - val_<lambda>: 0.8812\n",
      "Epoch 24/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7823 - <lambda>: 0.7823 - val_loss: 0.9007 - val_<lambda>: 0.9007\n",
      "Epoch 25/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7742 - <lambda>: 0.7742 - val_loss: 0.9361 - val_<lambda>: 0.9361\n",
      "Epoch 26/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7775 - <lambda>: 0.7775 - val_loss: 0.9095 - val_<lambda>: 0.9095\n",
      "Epoch 27/500\n",
      "287/287 [==============================] - 6s 19ms/step - loss: 0.7858 - <lambda>: 0.7858 - val_loss: 0.8834 - val_<lambda>: 0.8834\n",
      "Epoch 28/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7768 - <lambda>: 0.7768 - val_loss: 0.8797 - val_<lambda>: 0.8797\n",
      "Epoch 29/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7666 - <lambda>: 0.7666 - val_loss: 0.8742 - val_<lambda>: 0.8742\n",
      "Epoch 30/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7627 - <lambda>: 0.7627 - val_loss: 0.9107 - val_<lambda>: 0.9107\n",
      "Epoch 31/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7660 - <lambda>: 0.7660 - val_loss: 0.8844 - val_<lambda>: 0.8844\n",
      "Epoch 32/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7641 - <lambda>: 0.7641 - val_loss: 0.8987 - val_<lambda>: 0.8987\n",
      "Epoch 33/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7584 - <lambda>: 0.7584 - val_loss: 0.8701 - val_<lambda>: 0.8701\n",
      "Epoch 34/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7648 - <lambda>: 0.7648 - val_loss: 0.8785 - val_<lambda>: 0.8785\n",
      "Epoch 35/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7595 - <lambda>: 0.7595 - val_loss: 0.8999 - val_<lambda>: 0.8999\n",
      "Epoch 36/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7654 - <lambda>: 0.7654 - val_loss: 0.8723 - val_<lambda>: 0.8723\n",
      "Epoch 37/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7467 - <lambda>: 0.7467 - val_loss: 0.8885 - val_<lambda>: 0.8885\n",
      "Epoch 38/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7491 - <lambda>: 0.7491 - val_loss: 0.8731 - val_<lambda>: 0.8731\n",
      "Epoch 39/500\n",
      "285/287 [============================>.] - ETA: 0s - loss: 0.7552 - <lambda>: 0.7552\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7547 - <lambda>: 0.7547 - val_loss: 0.8828 - val_<lambda>: 0.8828\n",
      "Epoch 40/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7433 - <lambda>: 0.7433 - val_loss: 0.8723 - val_<lambda>: 0.8723\n",
      "Epoch 41/500\n",
      "287/287 [==============================] - 6s 19ms/step - loss: 0.7394 - <lambda>: 0.7394 - val_loss: 0.8777 - val_<lambda>: 0.8777\n",
      "Epoch 42/500\n",
      "287/287 [==============================] - 6s 19ms/step - loss: 0.7335 - <lambda>: 0.7335 - val_loss: 0.8879 - val_<lambda>: 0.8879\n",
      "Epoch 43/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7439 - <lambda>: 0.7439 - val_loss: 0.8835 - val_<lambda>: 0.8835\n",
      "Epoch 44/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7414 - <lambda>: 0.7414 - val_loss: 0.9057 - val_<lambda>: 0.9057\n",
      "Epoch 45/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7423 - <lambda>: 0.7423 - val_loss: 0.8789 - val_<lambda>: 0.8789\n",
      "Epoch 46/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7443 - <lambda>: 0.7443 - val_loss: 0.8632 - val_<lambda>: 0.8632\n",
      "Epoch 47/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7437 - <lambda>: 0.7437 - val_loss: 0.8687 - val_<lambda>: 0.8687\n",
      "Epoch 48/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7218 - <lambda>: 0.7218 - val_loss: 0.8812 - val_<lambda>: 0.8812\n",
      "Epoch 49/500\n",
      "287/287 [==============================] - 5s 19ms/step - loss: 0.7392 - <lambda>: 0.7392 - val_loss: 0.8843 - val_<lambda>: 0.8843\n"
     ]
    }
   ],
   "source": [
    "##### ~~~!!!!! >o< ^o^\n",
    "###T-Td  (GHI )\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "import tensorflow.keras.backend as K\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model,Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPool1D,Flatten, Reshape, LSTM,Input,concatenate,BatchNormalization,LayerNormalization,GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#from lightgbm import LGBMRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#  \n",
    "train = pd.read_csv('./train/train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "#1. DATA\n",
    "\n",
    "# GHI column  \n",
    "def Add_features(data):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "# train data column\n",
    "def preprocess_data(data, is_train=True):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "    temp = data.copy()\n",
    "    temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "    if is_train==True:          \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')   #  Target\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill') #  Target\n",
    "        temp = temp.dropna()    #  \n",
    "        return temp.iloc[:-96]  #   . (   )\n",
    "\n",
    "    elif is_train == False:\n",
    "        temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "        return temp.iloc[-48:, :]\n",
    "\n",
    "df_train = preprocess_data(train)\n",
    "# print(df_train.shape)   # (52464, 10)\n",
    "# print(df_train[:48])\n",
    "\n",
    "#-------------test \n",
    "df_test = []\n",
    "for i in range(81):\n",
    "    file_path = 'test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    df_test.append(temp)\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "test = np.array(df_test)\n",
    "print(df_test.shape) # (3888, 8)\n",
    "x_test = df_test.to_numpy()\n",
    "\n",
    "###=====================================================================\n",
    "#  () --> y1, y2    \n",
    "def split_xy(dataset, time_steps) :\n",
    "    x, y1, y2 = [],[],[]\n",
    "    for i in range(len(dataset)) :\n",
    "        x_end = i + time_steps\n",
    "        if x_end > len(dataset) :\n",
    "            break\n",
    "        tmp_x = dataset[i:x_end, :-2] # ['Hour' ~ 'T']\n",
    "        tmp_y1 = dataset[x_end-1:x_end,-2] # Target1\n",
    "        tmp_y2 = dataset[x_end-1:x_end,-1]   # Target2\n",
    "        x.append(tmp_x)\n",
    "        y1.append(tmp_y1)\n",
    "        y2.append(tmp_y2)\n",
    "    return np.array(x), np.array(y1), np.array(y2)\n",
    "\n",
    "X = df_train.to_numpy()\n",
    "# print(X.shape)      # (52464, 10)\n",
    "x,y1,y2 = split_xy(X,1)\n",
    "print(x.shape, y1.shape, y2.shape) #(52464, 1, 8) (52464, 1) (52464, 1)\n",
    "\n",
    "#####===========\n",
    "x_train, x_val, y1_train, y1_val, y2_train, y2_val = train_test_split(x,y1,y2,\n",
    "                           train_size = 0.7,shuffle = False, random_state = 0)\n",
    "\n",
    "# print(x_train.shape,x_val.shape) #(36724, 1, 8) (15740, 1, 8)\n",
    "print(y1_train.shape, y1_val.shape, y2_train.shape, y2_val.shape) #(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
    "x_train= x_train.reshape(36724*1, 10)\n",
    "x_val= x_val.reshape(15740*1, 10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "x_train= x_train.reshape(36724, 1, 10)\n",
    "x_val= x_val.reshape(15740, 1, 10)\n",
    "x_test= x_test.reshape(3888, 1, 10)\n",
    "# print(x_train.shape,x_val.shape,x_test.shape) #(36724, 1, 8) (15740, 1, 8) (3888, 1, 8)\n",
    "\n",
    "#  : Quantile loss definition\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "    err = (y_true - y_pred)\n",
    "    return K.mean(K.maximum(q*err, (q-1)*err), axis=-1)\n",
    "\n",
    "q = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "#2. \n",
    "\n",
    "def modeling() :\n",
    "    inputs = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    adam = Adam(learning_rate=0.003)\n",
    "    conv = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Flatten()(conv)\n",
    "\n",
    "    conv1 = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Flatten()(conv1)\n",
    "\n",
    "    conv2 = Conv1D(filters = 256,kernel_size=3,activation='relu',padding='same')(inputs)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 64,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 32,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Flatten()(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters = 256,kernel_size=1,activation='relu',padding='same')(inputs)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 64,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 32,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Flatten()(conv3)\n",
    "    \n",
    "    lstm = LSTM(256,activation='relu',return_sequences=True)(inputs)\n",
    "    lstm = LSTM(64,activation='relu',return_sequences=True)(lstm)\n",
    "    lstm = LSTM(16,activation='relu')(lstm)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    concat = concatenate([conv,conv1,conv2,conv3,lstm])\n",
    "    dense = Dense(128,activation='relu')(concat)\n",
    "    dense = Dense(64,activation='relu')(dense)\n",
    "    dense = Dense(32,activation='relu')(dense)\n",
    "    dense = Dense(8,activation='relu')(dense)\n",
    "    outputs = Dense(1,activation='relu')(dense)\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model\n",
    "# 3. , \n",
    "#####========,  , predict\n",
    "#y1, y2  \n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 30)\n",
    "lr = ReduceLROnPlateau(monitor = 'val_loss', patience = 20, factor = 0.9, verbose = 1)\n",
    "epochs = 500\n",
    "bs = 128\n",
    "\n",
    "# !!\n",
    "x = []\n",
    "for i in q:\n",
    "    if q!=0.5:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models3/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = lambda y_true,y_pred: quantile_loss(i,y_true,y_pred), optimizer = 'adam', metrics = [lambda y,y_pred: quantile_loss(i,y,y_pred)])\n",
    "        model.fit(x_train,y1_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y1_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "    else:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models3/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = 'mae', optimizer = 'adam')\n",
    "        model.fit(x_train,y1_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y1_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "df_temp1 = pd.concat(x, axis = 1)\n",
    "df_temp1[df_temp1<0] = 0\n",
    "num_temp1 = df_temp1.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = num_temp1\n",
    "\n",
    "x = []\n",
    "# !!\n",
    "for i in q:\n",
    "    if q!=0.5:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models3/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = lambda y_true,y_pred: quantile_loss(i,y_true,y_pred), optimizer = 'adam', metrics = [lambda y,y_pred: quantile_loss(i,y,y_pred)])\n",
    "        model.fit(x_train,y2_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y2_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "    else:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models3/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = 'mae', optimizer = 'adam')\n",
    "        model.fit(x_train,y2_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y2_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "df_temp2 = pd.concat(x, axis = 1)\n",
    "df_temp2[df_temp2<0] = 0\n",
    "num_temp2 = df_temp2.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = num_temp2\n",
    "\n",
    "##??????????????     ?????????   \n",
    "submission.to_csv('models3/upgrade_sub_0125__2.csv', index = False)\n",
    "##  135  -- 1.94653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3888, 10)\n",
      "(52464, 1, 10) (52464, 1) (52464, 1)\n",
      "(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
      "\n",
      "\n",
      "  0.1 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.2 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.3 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.4 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.5 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.6 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.7 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.8 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.9 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.1 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.2 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.3 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_37 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.4 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_39 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_40 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_41 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.5 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_42 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_43 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_44 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.6 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_45 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_46 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_47 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.7 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_50 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.8 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_51 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_52 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_53 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.9 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_54 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_55 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_56 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "##### ~~~!!!!! >o< ^o^\n",
    "###T-Td  (GHI )\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "import tensorflow.keras.backend as K\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model,Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPool1D,Flatten, Reshape, LSTM,Input,concatenate,BatchNormalization,LayerNormalization,GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#from lightgbm import LGBMRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#  \n",
    "train = pd.read_csv('./train/train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "#1. DATA\n",
    "\n",
    "# GHI column  \n",
    "def Add_features(data):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "# train data column\n",
    "def preprocess_data(data, is_train=True):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "    temp = data.copy()\n",
    "    temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "    if is_train==True:          \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')   #  Target\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill') #  Target\n",
    "        temp = temp.dropna()    #  \n",
    "        return temp.iloc[:-96]  #   . (   )\n",
    "\n",
    "    elif is_train == False:\n",
    "        temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "        return temp.iloc[-48:, :]\n",
    "\n",
    "df_train = preprocess_data(train)\n",
    "# print(df_train.shape)   # (52464, 10)\n",
    "# print(df_train[:48])\n",
    "\n",
    "#-------------test \n",
    "df_test = []\n",
    "for i in range(81):\n",
    "    file_path = 'test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    df_test.append(temp)\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "test = np.array(df_test)\n",
    "print(df_test.shape) # (3888, 8)\n",
    "x_test = df_test.to_numpy()\n",
    "\n",
    "###=====================================================================\n",
    "#  () --> y1, y2    \n",
    "def split_xy(dataset, time_steps) :\n",
    "    x, y1, y2 = [],[],[]\n",
    "    for i in range(len(dataset)) :\n",
    "        x_end = i + time_steps\n",
    "        if x_end > len(dataset) :\n",
    "            break\n",
    "        tmp_x = dataset[i:x_end, :-2] # ['Hour' ~ 'T']\n",
    "        tmp_y1 = dataset[x_end-1:x_end,-2] # Target1\n",
    "        tmp_y2 = dataset[x_end-1:x_end,-1]   # Target2\n",
    "        x.append(tmp_x)\n",
    "        y1.append(tmp_y1)\n",
    "        y2.append(tmp_y2)\n",
    "    return np.array(x), np.array(y1), np.array(y2)\n",
    "\n",
    "X = df_train.to_numpy()\n",
    "# print(X.shape)      # (52464, 10)\n",
    "x,y1,y2 = split_xy(X,1)\n",
    "print(x.shape, y1.shape, y2.shape) #(52464, 1, 8) (52464, 1) (52464, 1)\n",
    "\n",
    "#####===========\n",
    "x_train, x_val, y1_train, y1_val, y2_train, y2_val = train_test_split(x,y1,y2,\n",
    "                           train_size = 0.7,shuffle = False, random_state = 0)\n",
    "\n",
    "# print(x_train.shape,x_val.shape) #(36724, 1, 8) (15740, 1, 8)\n",
    "print(y1_train.shape, y1_val.shape, y2_train.shape, y2_val.shape) #(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
    "x_train= x_train.reshape(36724*1, 10)\n",
    "x_val= x_val.reshape(15740*1, 10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "x_train= x_train.reshape(36724, 1, 10)\n",
    "x_val= x_val.reshape(15740, 1, 10)\n",
    "x_test= x_test.reshape(3888, 1, 10)\n",
    "# print(x_train.shape,x_val.shape,x_test.shape) #(36724, 1, 8) (15740, 1, 8) (3888, 1, 8)\n",
    "\n",
    "#  : Quantile loss definition\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "    err = (y_true - y_pred)\n",
    "    return K.mean(K.maximum(q*err, (q-1)*err), axis=-1)\n",
    "\n",
    "q = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "#2. \n",
    "\n",
    "def modeling() :\n",
    "    inputs = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    adam = Adam(learning_rate=0.003)\n",
    "    conv = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Flatten()(conv)\n",
    "\n",
    "    conv1 = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Flatten()(conv1)\n",
    "\n",
    "    conv2 = Conv1D(filters = 256,kernel_size=3,activation='relu',padding='same')(inputs)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 64,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 32,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Flatten()(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters = 256,kernel_size=1,activation='relu',padding='same')(inputs)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 64,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 32,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Flatten()(conv3)\n",
    "    \n",
    "    lstm = GRU(256,activation='relu')(inputs)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    concat = concatenate([conv,conv1,conv2,conv3,lstm])\n",
    "    dense = Dense(128,activation='relu')(concat)\n",
    "    dense = Dense(64,activation='relu')(dense)\n",
    "    dense = Dense(32,activation='relu')(dense)\n",
    "    dense = Dense(8,activation='relu')(dense)\n",
    "    outputs = Dense(1,activation='relu')(dense)\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# 3. , \n",
    "#####========,  , predict\n",
    "#y1, y2  \n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "lr = ReduceLROnPlateau(monitor = 'val_loss', patience = 5, factor = 0.9, verbose = 1)\n",
    "epochs = 200\n",
    "bs = 512\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = []\n",
    "for i in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "    filepath_cp = f'models3/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "    model = load_model(filepath_cp,compile=False)\n",
    "    pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "    x.append(pred)\n",
    "df_temp1 = pd.concat(x, axis = 1)\n",
    "df_temp1[df_temp1<0] = 0\n",
    "num_temp1 = df_temp1.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = num_temp1\n",
    "\n",
    "x = []\n",
    "# !!\n",
    "for i in q:\n",
    "    print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "    filepath_cp = f'models3/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "    model = load_model(filepath_cp,compile=False)\n",
    "    pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "    x.append(pred)\n",
    "df_temp2 = pd.concat(x, axis = 1)\n",
    "df_temp2[df_temp2<0] = 0\n",
    "num_temp2 = df_temp2.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = num_temp2\n",
    "submission.to_csv('models3/upgrade_sub_0125_.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ~~~!!!!! >o< ^o^\n",
    "###T-Td  (GHI )\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "import tensorflow.keras.backend as K\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model,Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPool1D,Flatten, Reshape, LSTM,Input,concatenate,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from lightgbm import LGBMRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#  \n",
    "train = pd.read_csv('./train/train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "#1. DATA\n",
    "\n",
    "# GHI column  \n",
    "def Add_features(data):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "# train data column\n",
    "def preprocess_data(data, is_train=True):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "    temp = data.copy()\n",
    "    temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "    if is_train==True:          \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')   #  Target\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill') #  Target\n",
    "        temp = temp.dropna()    #  \n",
    "        return temp.iloc[:-96]  #   . (   )\n",
    "\n",
    "    elif is_train == False:\n",
    "        temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "        return temp.iloc[-48:, :]\n",
    "\n",
    "df_train = preprocess_data(train)\n",
    "# print(df_train.shape)   # (52464, 10)\n",
    "# print(df_train[:48])\n",
    "\n",
    "#-------------test \n",
    "df_test = []\n",
    "for i in range(81):\n",
    "    file_path = 'test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    df_test.append(temp)\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "test = np.array(df_test)\n",
    "print(df_test.shape) # (3888, 8)\n",
    "x_test = df_test.to_numpy()\n",
    "\n",
    "###=====================================================================\n",
    "#  () --> y1, y2    \n",
    "def split_xy(dataset, time_steps) :\n",
    "    x, y1, y2 = [],[],[]\n",
    "    for i in range(len(dataset)) :\n",
    "        x_end = i + time_steps\n",
    "        if x_end > len(dataset) :\n",
    "            break\n",
    "        tmp_x = dataset[i:x_end, :-2] # ['Hour' ~ 'T']\n",
    "        tmp_y1 = dataset[x_end-1:x_end,-2] # Target1\n",
    "        tmp_y2 = dataset[x_end-1:x_end,-1]   # Target2\n",
    "        x.append(tmp_x)\n",
    "        y1.append(tmp_y1)\n",
    "        y2.append(tmp_y2)\n",
    "    return np.array(x), np.array(y1), np.array(y2)\n",
    "\n",
    "X = df_train.to_numpy()\n",
    "# print(X.shape)      # (52464, 10)\n",
    "x,y1,y2 = split_xy(X,1)\n",
    "print(x.shape, y1.shape, y2.shape) #(52464, 1, 8) (52464, 1) (52464, 1)\n",
    "\n",
    "#####===========\n",
    "x_train, x_val, y1_train, y1_val, y2_train, y2_val = train_test_split(x,y1,y2,\n",
    "                           train_size = 0.7,shuffle = False, random_state = 0)\n",
    "\n",
    "# print(x_train.shape,x_val.shape) #(36724, 1, 8) (15740, 1, 8)\n",
    "print(y1_train.shape, y1_val.shape, y2_train.shape, y2_val.shape) #(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
    "x_train= x_train.reshape(36724*1, 10)\n",
    "x_val= x_val.reshape(15740*1, 10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "x_train= x_train.reshape(36724, 1, 10)\n",
    "x_val= x_val.reshape(15740, 1, 10)\n",
    "x_test= x_test.reshape(3888, 1, 10)\n",
    "# print(x_train.shape,x_val.shape,x_test.shape) #(36724, 1, 8) (15740, 1, 8) (3888, 1, 8)\n",
    "\n",
    "#  : Quantile loss definition\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "    err = (y_true - y_pred)\n",
    "    return K.mean(K.maximum(q*err, (q-1)*err), axis=-1)\n",
    "\n",
    "q = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "#2. \n",
    "\n",
    "def modeling() :\n",
    "    inputs = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    adam = Adam(learning_rate=0.003)\n",
    "    conv = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Flatten()(conv)\n",
    "\n",
    "    conv1 = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Flatten()(conv1)\n",
    "\n",
    "    conv2 = Conv1D(filters = 256,kernel_size=3,activation='relu',padding='same')(inputs)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 64,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 32,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Flatten()(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters = 256,kernel_size=1,activation='relu',padding='same')(inputs)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 64,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 32,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Flatten()(conv3)\n",
    "    \n",
    "    lstm = LSTM(256,activation='relu',return_sequences=True)(inputs)\n",
    "    lstm = LSTM(64,activation='relu',return_sequences=True)(lstm)\n",
    "    lstm = LSTM(16,activation='relu')(lstm)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    \n",
    "    lstm1 = LSTM(256,activation='relu',return_sequences=True)(inputs)\n",
    "    lstm1 = LSTM(16,activation='relu')(lstm1)\n",
    "    lstm1 = LayerNormalization()(lstm1)\n",
    "    \n",
    "    lstm2 = LSTM(16,activation='relu')(inputs)\n",
    "    lstm2 = LayerNormalization()(lstm2)\n",
    "    \n",
    "    \n",
    "    concat = concatenate([conv,conv1,conv2,conv3,lstm,lstm1,lstm2])\n",
    "    dense = Dense(128,activation='relu')(concat)\n",
    "    dense = Dense(64,activation='relu')(dense)\n",
    "    dense = Dense(32,activation='relu')(dense)\n",
    "    dense = Dense(8,activation='relu')(dense)\n",
    "    outputs = Dense(1,activation='relu')(dense)\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model\n",
    "# 3. , \n",
    "#####========,  , predict\n",
    "#y1, y2  \n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 30)\n",
    "lr = ReduceLROnPlateau(monitor = 'val_loss', patience = 20, factor = 0.9, verbose = 1)\n",
    "epochs = 500\n",
    "bs = 16\n",
    "\n",
    "# !!\n",
    "x = []\n",
    "for i in q:\n",
    "    if q!=0.5:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models4/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = lambda y_true,y_pred: quantile_loss(i,y_true,y_pred), optimizer = 'adam', metrics = [lambda y,y_pred: quantile_loss(i,y,y_pred)])\n",
    "        model.fit(x_train,y1_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y1_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "    else:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models4/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = 'mae', optimizer = 'adam')\n",
    "        model.fit(x_train,y1_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y1_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "df_temp1 = pd.concat(x, axis = 1)\n",
    "df_temp1[df_temp1<0] = 0\n",
    "num_temp1 = df_temp1.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = num_temp1\n",
    "\n",
    "x = []\n",
    "# !!\n",
    "for i in q:\n",
    "    if q!=0.5:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models4/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = lambda y_true,y_pred: quantile_loss(i,y_true,y_pred), optimizer = 'adam', metrics = [lambda y,y_pred: quantile_loss(i,y,y_pred)])\n",
    "        model.fit(x_train,y2_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y2_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "    else:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models4/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = 'mae', optimizer = 'adam')\n",
    "        model.fit(x_train,y2_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y2_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "df_temp2 = pd.concat(x, axis = 1)\n",
    "df_temp2[df_temp2<0] = 0\n",
    "num_temp2 = df_temp2.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = num_temp2\n",
    "\n",
    "##??????????????     ?????????   \n",
    "submission.to_csv('models4/upgrade_sub_0125__2.csv', index = False)\n",
    "##  135  -- 1.94653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ~~~!!!!! >o< ^o^\n",
    "###T-Td  (GHI )\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "import tensorflow.keras.backend as K\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model,Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPool1D,Flatten, Reshape, LSTM,Input,concatenate,BatchNormalization,LayerNormalization,GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from lightgbm import LGBMRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#  \n",
    "train = pd.read_csv('./train/train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "#1. DATA\n",
    "\n",
    "# GHI column  \n",
    "def Add_features(data):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "# train data column\n",
    "def preprocess_data(data, is_train=True):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "    temp = data.copy()\n",
    "    temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "    if is_train==True:          \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')   #  Target\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill') #  Target\n",
    "        temp = temp.dropna()    #  \n",
    "        return temp.iloc[:-96]  #   . (   )\n",
    "\n",
    "    elif is_train == False:\n",
    "        temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "        return temp.iloc[-48:, :]\n",
    "\n",
    "df_train = preprocess_data(train)\n",
    "# print(df_train.shape)   # (52464, 10)\n",
    "# print(df_train[:48])\n",
    "\n",
    "#-------------test \n",
    "df_test = []\n",
    "for i in range(81):\n",
    "    file_path = 'test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    df_test.append(temp)\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "test = np.array(df_test)\n",
    "print(df_test.shape) # (3888, 8)\n",
    "x_test = df_test.to_numpy()\n",
    "\n",
    "###=====================================================================\n",
    "#  () --> y1, y2    \n",
    "def split_xy(dataset, time_steps) :\n",
    "    x, y1, y2 = [],[],[]\n",
    "    for i in range(len(dataset)) :\n",
    "        x_end = i + time_steps\n",
    "        if x_end > len(dataset) :\n",
    "            break\n",
    "        tmp_x = dataset[i:x_end, :-2] # ['Hour' ~ 'T']\n",
    "        tmp_y1 = dataset[x_end-1:x_end,-2] # Target1\n",
    "        tmp_y2 = dataset[x_end-1:x_end,-1]   # Target2\n",
    "        x.append(tmp_x)\n",
    "        y1.append(tmp_y1)\n",
    "        y2.append(tmp_y2)\n",
    "    return np.array(x), np.array(y1), np.array(y2)\n",
    "\n",
    "X = df_train.to_numpy()\n",
    "# print(X.shape)      # (52464, 10)\n",
    "x,y1,y2 = split_xy(X,1)\n",
    "print(x.shape, y1.shape, y2.shape) #(52464, 1, 8) (52464, 1) (52464, 1)\n",
    "\n",
    "#####===========\n",
    "x_train, x_val, y1_train, y1_val, y2_train, y2_val = train_test_split(x,y1,y2,\n",
    "                           train_size = 0.7,shuffle = False, random_state = 0)\n",
    "\n",
    "# print(x_train.shape,x_val.shape) #(36724, 1, 8) (15740, 1, 8)\n",
    "print(y1_train.shape, y1_val.shape, y2_train.shape, y2_val.shape) #(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
    "x_train= x_train.reshape(36724*1, 10)\n",
    "x_val= x_val.reshape(15740*1, 10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "x_train= x_train.reshape(36724, 1, 10)\n",
    "x_val= x_val.reshape(15740, 1, 10)\n",
    "x_test= x_test.reshape(3888, 1, 10)\n",
    "# print(x_train.shape,x_val.shape,x_test.shape) #(36724, 1, 8) (15740, 1, 8) (3888, 1, 8)\n",
    "\n",
    "#  : Quantile loss definition\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "    err = (y_true - y_pred)\n",
    "    return K.mean(K.maximum(q*err, (q-1)*err), axis=-1)\n",
    "\n",
    "q = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "#2. \n",
    "\n",
    "def modeling() :\n",
    "    inputs = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    adam = Adam(learning_rate=0.003)\n",
    "    conv = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Flatten()(conv)\n",
    "\n",
    "    conv1 = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Flatten()(conv1)\n",
    "\n",
    "    conv2 = Conv1D(filters = 256,kernel_size=3,activation='relu',padding='same')(inputs)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 64,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 32,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Flatten()(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters = 256,kernel_size=1,activation='relu',padding='same')(inputs)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 64,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 32,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Flatten()(conv3)\n",
    "    \n",
    "    lstm = GRU(256,activation='relu')(inputs)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    concat = concatenate([conv,conv1,conv2,conv3,lstm])\n",
    "    dense = Dense(128,activation='relu')(concat)\n",
    "    dense = Dense(64,activation='relu')(dense)\n",
    "    dense = Dense(32,activation='relu')(dense)\n",
    "    dense = Dense(8,activation='relu')(dense)\n",
    "    outputs = Dense(1,activation='relu')(dense)\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# 3. , \n",
    "#####========,  , predict\n",
    "#y1, y2  \n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "lr = ReduceLROnPlateau(monitor = 'val_loss', patience = 5, factor = 0.9, verbose = 1)\n",
    "epochs = 200\n",
    "bs = 512\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = []\n",
    "for i in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "    filepath_cp = f'models4/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "    model = load_model(filepath_cp,compile=False)\n",
    "    pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "    x.append(pred)\n",
    "df_temp1 = pd.concat(x, axis = 1)\n",
    "df_temp1[df_temp1<0] = 0\n",
    "num_temp1 = df_temp1.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = num_temp1\n",
    "\n",
    "x = []\n",
    "# !!\n",
    "for i in q:\n",
    "    print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "    filepath_cp = f'models4/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "    model = load_model(filepath_cp,compile=False)\n",
    "    pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "    x.append(pred)\n",
    "df_temp2 = pd.concat(x, axis = 1)\n",
    "df_temp2[df_temp2<0] = 0\n",
    "num_temp2 = df_temp2.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = num_temp2\n",
    "submission.to_csv('models4/upgrade_sub_0125_.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3888, 10)\n",
      "(52464, 1, 10) (52464, 1) (52464, 1)\n",
      "(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
      "\n",
      "\n",
      "  0.1 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node functional_1/conv1d_10/conv1d (defined at <ipython-input-1-f533db158565>:251) ]] [Op:__inference_train_function_23342]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f533db158565>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[0mcp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_cp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmonitor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mquantile_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mquantile_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3888\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node functional_1/conv1d_10/conv1d (defined at <ipython-input-1-f533db158565>:251) ]] [Op:__inference_train_function_23342]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "##### ~~~!!!!! >o< ^o^\n",
    "###T-Td  (GHI )\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "import tensorflow.keras.backend as K\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model,Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPool1D,Flatten, Reshape, LSTM,Input,concatenate,BatchNormalization,LayerNormalization,GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from lightgbm import LGBMRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#  \n",
    "train = pd.read_csv('./train/train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "#1. DATA\n",
    "\n",
    "# GHI column  \n",
    "def Add_features(data):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "# train data column\n",
    "def preprocess_data(data, is_train=True):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "    temp = data.copy()\n",
    "    temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "    if is_train==True:          \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')   #  Target\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill') #  Target\n",
    "        temp = temp.dropna()    #  \n",
    "        return temp.iloc[:-96]  #   . (   )\n",
    "\n",
    "    elif is_train == False:\n",
    "        temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "        return temp.iloc[-48:, :]\n",
    "\n",
    "df_train = preprocess_data(train)\n",
    "# print(df_train.shape)   # (52464, 10)\n",
    "# print(df_train[:48])\n",
    "\n",
    "#-------------test \n",
    "df_test = []\n",
    "for i in range(81):\n",
    "    file_path = 'test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    df_test.append(temp)\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "test = np.array(df_test)\n",
    "print(df_test.shape) # (3888, 8)\n",
    "x_test = df_test.to_numpy()\n",
    "\n",
    "###=====================================================================\n",
    "#  () --> y1, y2    \n",
    "def split_xy(dataset, time_steps) :\n",
    "    x, y1, y2 = [],[],[]\n",
    "    for i in range(len(dataset)) :\n",
    "        x_end = i + time_steps\n",
    "        if x_end > len(dataset) :\n",
    "            break\n",
    "        tmp_x = dataset[i:x_end, :-2] # ['Hour' ~ 'T']\n",
    "        tmp_y1 = dataset[x_end-1:x_end,-2] # Target1\n",
    "        tmp_y2 = dataset[x_end-1:x_end,-1]   # Target2\n",
    "        x.append(tmp_x)\n",
    "        y1.append(tmp_y1)\n",
    "        y2.append(tmp_y2)\n",
    "    return np.array(x), np.array(y1), np.array(y2)\n",
    "\n",
    "X = df_train.to_numpy()\n",
    "# print(X.shape)      # (52464, 10)\n",
    "x,y1,y2 = split_xy(X,1)\n",
    "print(x.shape, y1.shape, y2.shape) #(52464, 1, 8) (52464, 1) (52464, 1)\n",
    "\n",
    "#####===========\n",
    "x_train, x_val, y1_train, y1_val, y2_train, y2_val = train_test_split(x,y1,y2,\n",
    "                           train_size = 0.7,shuffle = False, random_state = 0)\n",
    "\n",
    "# print(x_train.shape,x_val.shape) #(36724, 1, 8) (15740, 1, 8)\n",
    "print(y1_train.shape, y1_val.shape, y2_train.shape, y2_val.shape) #(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
    "x_train= x_train.reshape(36724*1, 10)\n",
    "x_val= x_val.reshape(15740*1, 10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "x_train= x_train.reshape(36724, 1, 10)\n",
    "x_val= x_val.reshape(15740, 1, 10)\n",
    "x_test= x_test.reshape(3888, 1, 10)\n",
    "# print(x_train.shape,x_val.shape,x_test.shape) #(36724, 1, 8) (15740, 1, 8) (3888, 1, 8)\n",
    "\n",
    "#  : Quantile loss definition\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "    err = (y_true - y_pred)\n",
    "    return K.mean(K.maximum(q*err, (q-1)*err), axis=-1)\n",
    "\n",
    "q = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "#2. \n",
    "\n",
    "def modeling() :\n",
    "    inputs = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    adam = Adam(learning_rate=0.003)\n",
    "    conv = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Flatten()(conv)\n",
    "\n",
    "    conv1 = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = LSTM(32,activation='relu')(conv1)\n",
    "\n",
    "    conv2 = Conv1D(filters = 256,kernel_size=3,activation='relu',padding='same')(inputs)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 64,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 32,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = LSTM(32,activation='relu')(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters = 256,kernel_size=1,activation='relu',padding='same')(inputs)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 64,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 32,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = LSTM(32,activation='relu')(conv3)\n",
    "    \n",
    "    lstm = LSTM(256,activation='relu',return_sequences=True)(inputs)\n",
    "    lstm = LSTM(64,activation='relu',return_sequences=True)(lstm)\n",
    "    lstm = LSTM(16,activation='relu')(lstm)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    \n",
    "    lstm1 = LSTM(256,activation='relu',return_sequences=True)(inputs)\n",
    "    lstm1 = LSTM(16,activation='relu')(lstm1)\n",
    "    lstm1 = LayerNormalization()(lstm1)\n",
    "    \n",
    "    lstm2 = LSTM(16,activation='relu')(inputs)\n",
    "    lstm2 = LayerNormalization()(lstm2)\n",
    "    \n",
    "    concat = concatenate([conv,conv1,conv2,conv3,lstm,lstm1,lstm2])\n",
    "    dense = Dense(128,activation='relu')(concat)\n",
    "    dense = Dense(64,activation='relu')(dense)\n",
    "    dense = Dense(32,activation='relu')(dense)\n",
    "    dense = Dense(8,activation='relu')(dense)\n",
    "    outputs = Dense(1,activation='relu')(dense)\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model\n",
    "# 3. , \n",
    "#####========,  , predict\n",
    "#y1, y2  \n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "lr = ReduceLROnPlateau(monitor = 'val_loss', patience = 5, factor = 0.9, verbose = 1)\n",
    "epochs = 500\n",
    "bs = 16\n",
    "\n",
    "# !!\n",
    "x = []\n",
    "for i in q:\n",
    "    if q!=0.5:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models5/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = lambda y_true,y_pred: quantile_loss(i,y_true,y_pred), optimizer = 'adam', metrics = [lambda y,y_pred: quantile_loss(i,y,y_pred)])\n",
    "        model.fit(x_train,y1_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y1_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "    else:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models5/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = 'mae', optimizer = 'adam')\n",
    "        model.fit(x_train,y1_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y1_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "df_temp1 = pd.concat(x, axis = 1)\n",
    "df_temp1[df_temp1<0] = 0\n",
    "num_temp1 = df_temp1.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = num_temp1\n",
    "\n",
    "x = []\n",
    "# !!\n",
    "for i in q:\n",
    "    if q!=0.5:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models5/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = lambda y_true,y_pred: quantile_loss(i,y_true,y_pred), optimizer = 'adam', metrics = [lambda y,y_pred: quantile_loss(i,y,y_pred)])\n",
    "        model.fit(x_train,y2_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y2_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "    else:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models5/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = 'mae', optimizer = 'adam')\n",
    "        model.fit(x_train,y2_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y2_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "df_temp2 = pd.concat(x, axis = 1)\n",
    "df_temp2[df_temp2<0] = 0\n",
    "num_temp2 = df_temp2.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = num_temp2\n",
    "\n",
    "##??????????????     ?????????   \n",
    "submission.to_csv('models5/upgrade_sub_0125__2.csv', index = False)\n",
    "##  135  -- 1.94653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ~~~!!!!! >o< ^o^\n",
    "###T-Td  (GHI )\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "import tensorflow.keras.backend as K\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model,Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPool1D,Flatten, Reshape, LSTM,Input,concatenate,BatchNormalization,LayerNormalization,GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from lightgbm import LGBMRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#  \n",
    "train = pd.read_csv('./train/train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "#1. DATA\n",
    "\n",
    "# GHI column  \n",
    "def Add_features(data):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "# train data column\n",
    "def preprocess_data(data, is_train=True):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "    temp = data.copy()\n",
    "    temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "    if is_train==True:          \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')   #  Target\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill') #  Target\n",
    "        temp = temp.dropna()    #  \n",
    "        return temp.iloc[:-96]  #   . (   )\n",
    "\n",
    "    elif is_train == False:\n",
    "        temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "        return temp.iloc[-48:, :]\n",
    "\n",
    "df_train = preprocess_data(train)\n",
    "# print(df_train.shape)   # (52464, 10)\n",
    "# print(df_train[:48])\n",
    "\n",
    "#-------------test \n",
    "df_test = []\n",
    "for i in range(81):\n",
    "    file_path = 'test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    df_test.append(temp)\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "test = np.array(df_test)\n",
    "print(df_test.shape) # (3888, 8)\n",
    "x_test = df_test.to_numpy()\n",
    "\n",
    "###=====================================================================\n",
    "#  () --> y1, y2    \n",
    "def split_xy(dataset, time_steps) :\n",
    "    x, y1, y2 = [],[],[]\n",
    "    for i in range(len(dataset)) :\n",
    "        x_end = i + time_steps\n",
    "        if x_end > len(dataset) :\n",
    "            break\n",
    "        tmp_x = dataset[i:x_end, :-2] # ['Hour' ~ 'T']\n",
    "        tmp_y1 = dataset[x_end-1:x_end,-2] # Target1\n",
    "        tmp_y2 = dataset[x_end-1:x_end,-1]   # Target2\n",
    "        x.append(tmp_x)\n",
    "        y1.append(tmp_y1)\n",
    "        y2.append(tmp_y2)\n",
    "    return np.array(x), np.array(y1), np.array(y2)\n",
    "\n",
    "X = df_train.to_numpy()\n",
    "# print(X.shape)      # (52464, 10)\n",
    "x,y1,y2 = split_xy(X,1)\n",
    "print(x.shape, y1.shape, y2.shape) #(52464, 1, 8) (52464, 1) (52464, 1)\n",
    "\n",
    "#####===========\n",
    "x_train, x_val, y1_train, y1_val, y2_train, y2_val = train_test_split(x,y1,y2,\n",
    "                           train_size = 0.7,shuffle = False, random_state = 0)\n",
    "\n",
    "# print(x_train.shape,x_val.shape) #(36724, 1, 8) (15740, 1, 8)\n",
    "print(y1_train.shape, y1_val.shape, y2_train.shape, y2_val.shape) #(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
    "x_train= x_train.reshape(36724*1, 10)\n",
    "x_val= x_val.reshape(15740*1, 10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "x_train= x_train.reshape(36724, 1, 10)\n",
    "x_val= x_val.reshape(15740, 1, 10)\n",
    "x_test= x_test.reshape(3888, 1, 10)\n",
    "# print(x_train.shape,x_val.shape,x_test.shape) #(36724, 1, 8) (15740, 1, 8) (3888, 1, 8)\n",
    "\n",
    "#  : Quantile loss definition\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "    err = (y_true - y_pred)\n",
    "    return K.mean(K.maximum(q*err, (q-1)*err), axis=-1)\n",
    "\n",
    "q = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "#2. \n",
    "\n",
    "def modeling() :\n",
    "    inputs = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    adam = Adam(learning_rate=0.003)\n",
    "    conv = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Flatten()(conv)\n",
    "\n",
    "    conv1 = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Flatten()(conv1)\n",
    "\n",
    "    conv2 = Conv1D(filters = 256,kernel_size=3,activation='relu',padding='same')(inputs)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 64,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 32,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Flatten()(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters = 256,kernel_size=1,activation='relu',padding='same')(inputs)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 64,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 32,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Flatten()(conv3)\n",
    "    \n",
    "    lstm = GRU(256,activation='relu')(inputs)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    concat = concatenate([conv,conv1,conv2,conv3,lstm])\n",
    "    dense = Dense(128,activation='relu')(concat)\n",
    "    dense = Dense(64,activation='relu')(dense)\n",
    "    dense = Dense(32,activation='relu')(dense)\n",
    "    dense = Dense(8,activation='relu')(dense)\n",
    "    outputs = Dense(1,activation='relu')(dense)\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# 3. , \n",
    "#####========,  , predict\n",
    "#y1, y2  \n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "lr = ReduceLROnPlateau(monitor = 'val_loss', patience = 5, factor = 0.9, verbose = 1)\n",
    "epochs = 200\n",
    "bs = 512\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = []\n",
    "for i in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "    filepath_cp = f'models5/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "    model = load_model(filepath_cp,compile=False)\n",
    "    pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "    x.append(pred)\n",
    "df_temp1 = pd.concat(x, axis = 1)\n",
    "df_temp1[df_temp1<0] = 0\n",
    "num_temp1 = df_temp1.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = num_temp1\n",
    "\n",
    "x = []\n",
    "# !!\n",
    "for i in q:\n",
    "    print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "    filepath_cp = f'models5/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "    model = load_model(filepath_cp,compile=False)\n",
    "    pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "    x.append(pred)\n",
    "df_temp2 = pd.concat(x, axis = 1)\n",
    "df_temp2[df_temp2<0] = 0\n",
    "num_temp2 = df_temp2.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = num_temp2\n",
    "submission.to_csv('models5/upgrade_sub_0125_.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
