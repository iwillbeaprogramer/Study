{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3888, 10)\n",
      "(52464, 1, 10) (52464, 1) (52464, 1)\n",
      "(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
      "\n",
      "\n",
      "  0.1 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_21 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "2296/2296 [==============================] - 37s 16ms/step - loss: 1.4445 - <lambda>: 1.4445 - val_loss: 1.5479 - val_<lambda>: 1.5479\n",
      "Epoch 2/500\n",
      "2296/2296 [==============================] - 36s 16ms/step - loss: 1.4126 - <lambda>: 1.4126 - val_loss: 1.5175 - val_<lambda>: 1.5175\n",
      "Epoch 3/500\n",
      "2296/2296 [==============================] - 36s 16ms/step - loss: 1.4050 - <lambda>: 1.4050 - val_loss: 1.5490 - val_<lambda>: 1.5490\n",
      "Epoch 4/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3986 - <lambda>: 1.3986 - val_loss: 1.5353 - val_<lambda>: 1.5353\n",
      "Epoch 5/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3871 - <lambda>: 1.3871 - val_loss: 1.6655 - val_<lambda>: 1.6655\n",
      "Epoch 6/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3839 - <lambda>: 1.3839 - val_loss: 1.5304 - val_<lambda>: 1.5304\n",
      "Epoch 7/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3735 - <lambda>: 1.3735 - val_loss: 1.5266 - val_<lambda>: 1.5266\n",
      "Epoch 8/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3736 - <lambda>: 1.3736 - val_loss: 1.5220 - val_<lambda>: 1.5220\n",
      "Epoch 9/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3652 - <lambda>: 1.3652 - val_loss: 1.5161 - val_<lambda>: 1.5161\n",
      "Epoch 10/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3620 - <lambda>: 1.3620 - val_loss: 1.5123 - val_<lambda>: 1.5123\n",
      "Epoch 11/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3603 - <lambda>: 1.3603 - val_loss: 1.4871 - val_<lambda>: 1.4871\n",
      "Epoch 12/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3545 - <lambda>: 1.3545 - val_loss: 1.5214 - val_<lambda>: 1.5214\n",
      "Epoch 13/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3553 - <lambda>: 1.3553 - val_loss: 1.5531 - val_<lambda>: 1.5531\n",
      "Epoch 14/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3479 - <lambda>: 1.3479 - val_loss: 1.5052 - val_<lambda>: 1.5052\n",
      "Epoch 15/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3436 - <lambda>: 1.3436 - val_loss: 1.5044 - val_<lambda>: 1.5044\n",
      "Epoch 16/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3367 - <lambda>: 1.3367 - val_loss: 1.5343 - val_<lambda>: 1.5343\n",
      "Epoch 17/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3333 - <lambda>: 1.3333 - val_loss: 1.5491 - val_<lambda>: 1.5491\n",
      "Epoch 18/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3262 - <lambda>: 1.3262 - val_loss: 1.6312 - val_<lambda>: 1.6312\n",
      "Epoch 19/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3258 - <lambda>: 1.3258 - val_loss: 1.5801 - val_<lambda>: 1.5801\n",
      "Epoch 20/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3217 - <lambda>: 1.3217 - val_loss: 1.5461 - val_<lambda>: 1.5461\n",
      "Epoch 21/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3155 - <lambda>: 1.3155 - val_loss: 1.5287 - val_<lambda>: 1.5287\n",
      "Epoch 22/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3175 - <lambda>: 1.3175 - val_loss: 1.5086 - val_<lambda>: 1.5086\n",
      "Epoch 23/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3112 - <lambda>: 1.3112 - val_loss: 1.5255 - val_<lambda>: 1.5255\n",
      "Epoch 24/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3075 - <lambda>: 1.3075 - val_loss: 1.5264 - val_<lambda>: 1.5264\n",
      "Epoch 25/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.3049 - <lambda>: 1.3049 - val_loss: 1.5301 - val_<lambda>: 1.5301\n",
      "Epoch 26/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2973 - <lambda>: 1.2973 - val_loss: 1.5968 - val_<lambda>: 1.5968\n",
      "Epoch 27/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2938 - <lambda>: 1.2938 - val_loss: 1.5637 - val_<lambda>: 1.5637\n",
      "Epoch 28/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2910 - <lambda>: 1.2910 - val_loss: 1.5548 - val_<lambda>: 1.5548\n",
      "Epoch 29/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2855 - <lambda>: 1.2855 - val_loss: 1.6219 - val_<lambda>: 1.6219\n",
      "Epoch 30/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2853 - <lambda>: 1.2853 - val_loss: 1.5517 - val_<lambda>: 1.5517\n",
      "Epoch 31/500\n",
      "2294/2296 [============================>.] - ETA: 0s - loss: 1.2764 - <lambda>: 1.2764\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2762 - <lambda>: 1.2762 - val_loss: 1.5530 - val_<lambda>: 1.5530\n",
      "Epoch 32/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2719 - <lambda>: 1.2719 - val_loss: 1.5757 - val_<lambda>: 1.5757\n",
      "Epoch 33/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2690 - <lambda>: 1.2690 - val_loss: 1.9584 - val_<lambda>: 1.9584\n",
      "Epoch 34/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2590 - <lambda>: 1.2590 - val_loss: 1.6495 - val_<lambda>: 1.6495\n",
      "Epoch 35/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2559 - <lambda>: 1.2559 - val_loss: 1.5577 - val_<lambda>: 1.5577\n",
      "Epoch 36/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2533 - <lambda>: 1.2533 - val_loss: 1.5500 - val_<lambda>: 1.5500\n",
      "Epoch 37/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2490 - <lambda>: 1.2490 - val_loss: 1.5808 - val_<lambda>: 1.5808\n",
      "Epoch 38/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2465 - <lambda>: 1.2465 - val_loss: 1.6122 - val_<lambda>: 1.6122\n",
      "Epoch 39/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2387 - <lambda>: 1.2387 - val_loss: 1.5933 - val_<lambda>: 1.5933\n",
      "Epoch 40/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2403 - <lambda>: 1.2403 - val_loss: 1.6752 - val_<lambda>: 1.6752\n",
      "Epoch 41/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 1.2345 - <lambda>: 1.2345 - val_loss: 1.5858 - val_<lambda>: 1.5858\n",
      "\n",
      "\n",
      "  0.2 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_22 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "2296/2296 [==============================] - 36s 16ms/step - loss: 2.4091 - <lambda>: 2.4091 - val_loss: 2.5343 - val_<lambda>: 2.5343\n",
      "Epoch 2/500\n",
      "2296/2296 [==============================] - 36s 15ms/step - loss: 2.3293 - <lambda>: 2.3293 - val_loss: 2.4607 - val_<lambda>: 2.4607\n",
      "Epoch 3/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3001 - <lambda>: 2.3001 - val_loss: 2.5738 - val_<lambda>: 2.5738\n",
      "Epoch 4/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2847 - <lambda>: 2.2847 - val_loss: 2.4927 - val_<lambda>: 2.4927\n",
      "Epoch 5/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2744 - <lambda>: 2.2744 - val_loss: 2.4995 - val_<lambda>: 2.4995\n",
      "Epoch 6/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2552 - <lambda>: 2.2552 - val_loss: 2.6663 - val_<lambda>: 2.6663\n",
      "Epoch 7/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2421 - <lambda>: 2.2421 - val_loss: 2.5280 - val_<lambda>: 2.5280\n",
      "Epoch 8/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2266 - <lambda>: 2.2266 - val_loss: 2.6640 - val_<lambda>: 2.6640\n",
      "Epoch 9/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2266 - <lambda>: 2.2266 - val_loss: 2.4716 - val_<lambda>: 2.4716\n",
      "Epoch 10/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2117 - <lambda>: 2.2117 - val_loss: 2.5402 - val_<lambda>: 2.5402\n",
      "Epoch 11/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2077 - <lambda>: 2.2077 - val_loss: 2.5348 - val_<lambda>: 2.5348\n",
      "Epoch 12/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2003 - <lambda>: 2.2003 - val_loss: 2.5000 - val_<lambda>: 2.5000\n",
      "Epoch 13/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1910 - <lambda>: 2.1910 - val_loss: 2.5142 - val_<lambda>: 2.5142\n",
      "Epoch 14/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1797 - <lambda>: 2.1797 - val_loss: 2.5137 - val_<lambda>: 2.5137\n",
      "Epoch 15/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1754 - <lambda>: 2.1754 - val_loss: 2.5550 - val_<lambda>: 2.5550\n",
      "Epoch 16/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1786 - <lambda>: 2.1786 - val_loss: 2.4702 - val_<lambda>: 2.4702\n",
      "Epoch 17/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1605 - <lambda>: 2.1605 - val_loss: 2.5277 - val_<lambda>: 2.5277\n",
      "Epoch 18/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1524 - <lambda>: 2.1524 - val_loss: 2.5078 - val_<lambda>: 2.5078\n",
      "Epoch 19/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1425 - <lambda>: 2.1425 - val_loss: 2.5466 - val_<lambda>: 2.5466\n",
      "Epoch 20/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1301 - <lambda>: 2.1301 - val_loss: 2.5085 - val_<lambda>: 2.5085\n",
      "Epoch 21/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1244 - <lambda>: 2.1244 - val_loss: 2.5095 - val_<lambda>: 2.5095\n",
      "Epoch 22/500\n",
      "2293/2296 [============================>.] - ETA: 0s - loss: 2.1178 - <lambda>: 2.1178\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.1176 - <lambda>: 2.1176 - val_loss: 2.5148 - val_<lambda>: 2.5148\n",
      "Epoch 23/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0951 - <lambda>: 2.0951 - val_loss: 2.6296 - val_<lambda>: 2.6296\n",
      "Epoch 24/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0948 - <lambda>: 2.0948 - val_loss: 2.4926 - val_<lambda>: 2.4926\n",
      "Epoch 25/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0823 - <lambda>: 2.0823 - val_loss: 2.5585 - val_<lambda>: 2.5585\n",
      "Epoch 26/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0726 - <lambda>: 2.0726 - val_loss: 2.6398 - val_<lambda>: 2.6398\n",
      "Epoch 27/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0659 - <lambda>: 2.0659 - val_loss: 2.5582 - val_<lambda>: 2.5582\n",
      "Epoch 28/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0518 - <lambda>: 2.0518 - val_loss: 2.5350 - val_<lambda>: 2.5350\n",
      "Epoch 29/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0559 - <lambda>: 2.0559 - val_loss: 2.5558 - val_<lambda>: 2.5558\n",
      "Epoch 30/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0479 - <lambda>: 2.0479 - val_loss: 2.7350 - val_<lambda>: 2.7350\n",
      "Epoch 31/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0344 - <lambda>: 2.0344 - val_loss: 2.6216 - val_<lambda>: 2.6216\n",
      "Epoch 32/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.0186 - <lambda>: 2.0186 - val_loss: 2.5835 - val_<lambda>: 2.5835\n",
      "\n",
      "\n",
      "  0.3 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_23 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.9462 - <lambda>: 2.9462 - val_loss: 3.2321 - val_<lambda>: 3.2321\n",
      "Epoch 2/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8025 - <lambda>: 2.8025 - val_loss: 3.0434 - val_<lambda>: 3.0434\n",
      "Epoch 3/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7685 - <lambda>: 2.7685 - val_loss: 3.1366 - val_<lambda>: 3.1366\n",
      "Epoch 4/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7487 - <lambda>: 2.7487 - val_loss: 2.9573 - val_<lambda>: 2.9573\n",
      "Epoch 5/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7338 - <lambda>: 2.7338 - val_loss: 3.0424 - val_<lambda>: 3.0424\n",
      "Epoch 6/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7069 - <lambda>: 2.7069 - val_loss: 3.0490 - val_<lambda>: 3.0490\n",
      "Epoch 7/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7012 - <lambda>: 2.7012 - val_loss: 3.0227 - val_<lambda>: 3.0227\n",
      "Epoch 8/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6856 - <lambda>: 2.6856 - val_loss: 3.0095 - val_<lambda>: 3.0095\n",
      "Epoch 9/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6735 - <lambda>: 2.6735 - val_loss: 3.1017 - val_<lambda>: 3.1017\n",
      "Epoch 10/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6697 - <lambda>: 2.6697 - val_loss: 3.0536 - val_<lambda>: 3.0536\n",
      "Epoch 11/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6504 - <lambda>: 2.6504 - val_loss: 3.0038 - val_<lambda>: 3.0038\n",
      "Epoch 12/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6469 - <lambda>: 2.6469 - val_loss: 3.0380 - val_<lambda>: 3.0380\n",
      "Epoch 13/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6400 - <lambda>: 2.6400 - val_loss: 3.0588 - val_<lambda>: 3.0588\n",
      "Epoch 14/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6304 - <lambda>: 2.6304 - val_loss: 3.1036 - val_<lambda>: 3.1036\n",
      "Epoch 15/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6097 - <lambda>: 2.6097 - val_loss: 3.0411 - val_<lambda>: 3.0411\n",
      "Epoch 16/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6013 - <lambda>: 2.6013 - val_loss: 3.0737 - val_<lambda>: 3.0737\n",
      "Epoch 17/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5974 - <lambda>: 2.5974 - val_loss: 3.0584 - val_<lambda>: 3.0584\n",
      "Epoch 18/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5845 - <lambda>: 2.5845 - val_loss: 3.0559 - val_<lambda>: 3.0559\n",
      "Epoch 19/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5700 - <lambda>: 2.5700 - val_loss: 3.0485 - val_<lambda>: 3.0485\n",
      "Epoch 20/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5655 - <lambda>: 2.5655 - val_loss: 3.0239 - val_<lambda>: 3.0239\n",
      "Epoch 21/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5564 - <lambda>: 2.5564 - val_loss: 3.0841 - val_<lambda>: 3.0841\n",
      "Epoch 22/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5418 - <lambda>: 2.5418 - val_loss: 3.1762 - val_<lambda>: 3.1762\n",
      "Epoch 23/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5407 - <lambda>: 2.5407 - val_loss: 3.0929 - val_<lambda>: 3.0929\n",
      "Epoch 24/500\n",
      "2296/2296 [==============================] - ETA: 0s - loss: 2.5271 - <lambda>: 2.5271\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5271 - <lambda>: 2.5271 - val_loss: 3.1200 - val_<lambda>: 3.1200\n",
      "Epoch 25/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5201 - <lambda>: 2.5201 - val_loss: 3.1190 - val_<lambda>: 3.1190\n",
      "Epoch 26/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4986 - <lambda>: 2.4986 - val_loss: 3.0840 - val_<lambda>: 3.0840\n",
      "Epoch 27/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4826 - <lambda>: 2.4826 - val_loss: 3.0645 - val_<lambda>: 3.0645\n",
      "Epoch 28/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4921 - <lambda>: 2.4921 - val_loss: 3.2018 - val_<lambda>: 3.2018\n",
      "Epoch 29/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4739 - <lambda>: 2.4739 - val_loss: 3.1594 - val_<lambda>: 3.1594\n",
      "Epoch 30/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4664 - <lambda>: 2.4664 - val_loss: 3.1168 - val_<lambda>: 3.1168\n",
      "Epoch 31/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4561 - <lambda>: 2.4561 - val_loss: 3.0756 - val_<lambda>: 3.0756\n",
      "Epoch 32/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4487 - <lambda>: 2.4487 - val_loss: 3.1318 - val_<lambda>: 3.1318\n",
      "Epoch 33/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4350 - <lambda>: 2.4350 - val_loss: 3.0932 - val_<lambda>: 3.0932\n",
      "Epoch 34/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4263 - <lambda>: 2.4263 - val_loss: 3.0818 - val_<lambda>: 3.0818\n",
      "\n",
      "\n",
      "  0.4 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_24 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 3.1310 - <lambda>: 3.1310 - val_loss: 3.3535 - val_<lambda>: 3.3535\n",
      "Epoch 2/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.9611 - <lambda>: 2.9611 - val_loss: 3.2803 - val_<lambda>: 3.2803\n",
      "Epoch 3/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.9055 - <lambda>: 2.9055 - val_loss: 3.1176 - val_<lambda>: 3.1176\n",
      "Epoch 4/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8712 - <lambda>: 2.8712 - val_loss: 3.1736 - val_<lambda>: 3.1736\n",
      "Epoch 5/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8567 - <lambda>: 2.8567 - val_loss: 3.2231 - val_<lambda>: 3.2231\n",
      "Epoch 6/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8620 - <lambda>: 2.8620 - val_loss: 3.0791 - val_<lambda>: 3.0791\n",
      "Epoch 7/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8369 - <lambda>: 2.8369 - val_loss: 3.1237 - val_<lambda>: 3.1237\n",
      "Epoch 8/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8235 - <lambda>: 2.8235 - val_loss: 3.1324 - val_<lambda>: 3.1324\n",
      "Epoch 9/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8124 - <lambda>: 2.8124 - val_loss: 3.1840 - val_<lambda>: 3.1840\n",
      "Epoch 10/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8021 - <lambda>: 2.8021 - val_loss: 3.1257 - val_<lambda>: 3.1257\n",
      "Epoch 11/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7930 - <lambda>: 2.7930 - val_loss: 3.1089 - val_<lambda>: 3.1089\n",
      "Epoch 12/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7895 - <lambda>: 2.7895 - val_loss: 3.1129 - val_<lambda>: 3.1129\n",
      "Epoch 13/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7722 - <lambda>: 2.7722 - val_loss: 3.1475 - val_<lambda>: 3.1475\n",
      "Epoch 14/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7699 - <lambda>: 2.7699 - val_loss: 3.1056 - val_<lambda>: 3.1056\n",
      "Epoch 15/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7589 - <lambda>: 2.7589 - val_loss: 3.1670 - val_<lambda>: 3.1670\n",
      "Epoch 16/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7496 - <lambda>: 2.7496 - val_loss: 3.1737 - val_<lambda>: 3.1737\n",
      "Epoch 17/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7350 - <lambda>: 2.7350 - val_loss: 3.2577 - val_<lambda>: 3.2577\n",
      "Epoch 18/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7297 - <lambda>: 2.7297 - val_loss: 3.1732 - val_<lambda>: 3.1732\n",
      "Epoch 19/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7151 - <lambda>: 2.7151 - val_loss: 3.1864 - val_<lambda>: 3.1864\n",
      "Epoch 20/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7099 - <lambda>: 2.7099 - val_loss: 3.1826 - val_<lambda>: 3.1826\n",
      "Epoch 21/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6955 - <lambda>: 2.6955 - val_loss: 3.1859 - val_<lambda>: 3.1859\n",
      "Epoch 22/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6907 - <lambda>: 2.6907 - val_loss: 3.2123 - val_<lambda>: 3.2123\n",
      "Epoch 23/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6676 - <lambda>: 2.6676 - val_loss: 3.2775 - val_<lambda>: 3.2775\n",
      "Epoch 24/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6683 - <lambda>: 2.6683 - val_loss: 3.1666 - val_<lambda>: 3.1666\n",
      "Epoch 25/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6587 - <lambda>: 2.6587 - val_loss: 3.2574 - val_<lambda>: 3.2574\n",
      "Epoch 26/500\n",
      "2294/2296 [============================>.] - ETA: 0s - loss: 2.6588 - <lambda>: 2.6588\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6592 - <lambda>: 2.6592 - val_loss: 3.1820 - val_<lambda>: 3.1820\n",
      "Epoch 27/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6345 - <lambda>: 2.6345 - val_loss: 3.2016 - val_<lambda>: 3.2016\n",
      "Epoch 28/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6166 - <lambda>: 2.6166 - val_loss: 3.3053 - val_<lambda>: 3.3053\n",
      "Epoch 29/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6159 - <lambda>: 2.6159 - val_loss: 3.2274 - val_<lambda>: 3.2274\n",
      "Epoch 30/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6062 - <lambda>: 2.6062 - val_loss: 3.2723 - val_<lambda>: 3.2723\n",
      "Epoch 31/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6017 - <lambda>: 2.6017 - val_loss: 3.3072 - val_<lambda>: 3.3072\n",
      "Epoch 32/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5906 - <lambda>: 2.5906 - val_loss: 3.1930 - val_<lambda>: 3.1930\n",
      "Epoch 33/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5861 - <lambda>: 2.5861 - val_loss: 3.2234 - val_<lambda>: 3.2234\n",
      "Epoch 34/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5726 - <lambda>: 2.5726 - val_loss: 3.3119 - val_<lambda>: 3.3119\n",
      "Epoch 35/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5570 - <lambda>: 2.5570 - val_loss: 3.2261 - val_<lambda>: 3.2261\n",
      "Epoch 36/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5584 - <lambda>: 2.5584 - val_loss: 3.2360 - val_<lambda>: 3.2360\n",
      "\n",
      "\n",
      "  0.5 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_25 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "2296/2296 [==============================] - 36s 16ms/step - loss: 3.0746 - <lambda>: 3.0746 - val_loss: 3.1376 - val_<lambda>: 3.1376\n",
      "Epoch 2/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8591 - <lambda>: 2.8591 - val_loss: 3.0729 - val_<lambda>: 3.0729\n",
      "Epoch 3/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.8040 - <lambda>: 2.8040 - val_loss: 3.9153 - val_<lambda>: 3.9153\n",
      "Epoch 4/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7923 - <lambda>: 2.7923 - val_loss: 3.1443 - val_<lambda>: 3.1443\n",
      "Epoch 5/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7616 - <lambda>: 2.7616 - val_loss: 3.1232 - val_<lambda>: 3.1232\n",
      "Epoch 6/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7558 - <lambda>: 2.7558 - val_loss: 3.3166 - val_<lambda>: 3.3166\n",
      "Epoch 7/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7294 - <lambda>: 2.7294 - val_loss: 3.0932 - val_<lambda>: 3.0932\n",
      "Epoch 8/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7265 - <lambda>: 2.7265 - val_loss: 2.9959 - val_<lambda>: 2.9959\n",
      "Epoch 9/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7261 - <lambda>: 2.7261 - val_loss: 2.9974 - val_<lambda>: 2.9974\n",
      "Epoch 10/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.7096 - <lambda>: 2.7096 - val_loss: 3.1021 - val_<lambda>: 3.1021\n",
      "Epoch 11/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6975 - <lambda>: 2.6975 - val_loss: 2.9827 - val_<lambda>: 2.9827\n",
      "Epoch 12/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6871 - <lambda>: 2.6871 - val_loss: 3.1050 - val_<lambda>: 3.1050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6801 - <lambda>: 2.6801 - val_loss: 3.0087 - val_<lambda>: 3.0087\n",
      "Epoch 14/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6719 - <lambda>: 2.6719 - val_loss: 2.9955 - val_<lambda>: 2.9955\n",
      "Epoch 15/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6643 - <lambda>: 2.6643 - val_loss: 3.0585 - val_<lambda>: 3.0585\n",
      "Epoch 16/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6531 - <lambda>: 2.6531 - val_loss: 3.0735 - val_<lambda>: 3.0735\n",
      "Epoch 17/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6525 - <lambda>: 2.6525 - val_loss: 2.9867 - val_<lambda>: 2.9867\n",
      "Epoch 18/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6378 - <lambda>: 2.6378 - val_loss: 2.9931 - val_<lambda>: 2.9931\n",
      "Epoch 19/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6333 - <lambda>: 2.6333 - val_loss: 2.9879 - val_<lambda>: 2.9879\n",
      "Epoch 20/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6298 - <lambda>: 2.6298 - val_loss: 3.0782 - val_<lambda>: 3.0782\n",
      "Epoch 21/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6200 - <lambda>: 2.6200 - val_loss: 3.0095 - val_<lambda>: 3.0095\n",
      "Epoch 22/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6085 - <lambda>: 2.6085 - val_loss: 3.1110 - val_<lambda>: 3.1110\n",
      "Epoch 23/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.6017 - <lambda>: 2.6017 - val_loss: 3.0792 - val_<lambda>: 3.0792\n",
      "Epoch 24/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5925 - <lambda>: 2.5925 - val_loss: 2.9975 - val_<lambda>: 2.9975\n",
      "Epoch 25/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5860 - <lambda>: 2.5860 - val_loss: 3.1187 - val_<lambda>: 3.1187\n",
      "Epoch 26/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5752 - <lambda>: 2.5752 - val_loss: 3.0727 - val_<lambda>: 3.0727\n",
      "Epoch 27/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5717 - <lambda>: 2.5717 - val_loss: 3.0839 - val_<lambda>: 3.0839\n",
      "Epoch 28/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5659 - <lambda>: 2.5659 - val_loss: 3.0525 - val_<lambda>: 3.0525\n",
      "Epoch 29/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5519 - <lambda>: 2.5519 - val_loss: 3.1826 - val_<lambda>: 3.1826\n",
      "Epoch 30/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5580 - <lambda>: 2.5580 - val_loss: 3.1019 - val_<lambda>: 3.1019\n",
      "Epoch 31/500\n",
      "2294/2296 [============================>.] - ETA: 0s - loss: 2.5495 - <lambda>: 2.5495\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5494 - <lambda>: 2.5494 - val_loss: 3.1133 - val_<lambda>: 3.1133\n",
      "Epoch 32/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5290 - <lambda>: 2.5290 - val_loss: 3.0733 - val_<lambda>: 3.0733\n",
      "Epoch 33/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5171 - <lambda>: 2.5171 - val_loss: 3.1215 - val_<lambda>: 3.1215\n",
      "Epoch 34/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5094 - <lambda>: 2.5094 - val_loss: 3.0746 - val_<lambda>: 3.0746\n",
      "Epoch 35/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5022 - <lambda>: 2.5022 - val_loss: 3.0995 - val_<lambda>: 3.0995\n",
      "Epoch 36/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4874 - <lambda>: 2.4874 - val_loss: 3.0682 - val_<lambda>: 3.0682\n",
      "Epoch 37/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4884 - <lambda>: 2.4884 - val_loss: 3.0690 - val_<lambda>: 3.0690\n",
      "Epoch 38/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4778 - <lambda>: 2.4778 - val_loss: 3.1229 - val_<lambda>: 3.1229\n",
      "Epoch 39/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4761 - <lambda>: 2.4761 - val_loss: 3.1299 - val_<lambda>: 3.1299\n",
      "Epoch 40/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4741 - <lambda>: 2.4741 - val_loss: 3.0983 - val_<lambda>: 3.0983\n",
      "Epoch 41/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4638 - <lambda>: 2.4638 - val_loss: 3.2018 - val_<lambda>: 3.2018\n",
      "\n",
      "\n",
      "  0.6 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_26 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "2296/2296 [==============================] - 36s 16ms/step - loss: 2.7967 - <lambda>: 2.7967 - val_loss: 2.9073 - val_<lambda>: 2.9073\n",
      "Epoch 2/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5651 - <lambda>: 2.5651 - val_loss: 2.9497 - val_<lambda>: 2.9497\n",
      "Epoch 3/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.5036 - <lambda>: 2.5036 - val_loss: 2.7281 - val_<lambda>: 2.7281\n",
      "Epoch 4/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4843 - <lambda>: 2.4843 - val_loss: 2.7279 - val_<lambda>: 2.7279\n",
      "Epoch 5/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4559 - <lambda>: 2.4559 - val_loss: 2.6678 - val_<lambda>: 2.6678\n",
      "Epoch 6/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4527 - <lambda>: 2.4527 - val_loss: 2.6841 - val_<lambda>: 2.6841\n",
      "Epoch 7/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4396 - <lambda>: 2.4396 - val_loss: 2.6629 - val_<lambda>: 2.6629\n",
      "Epoch 8/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4283 - <lambda>: 2.4283 - val_loss: 2.7576 - val_<lambda>: 2.7576\n",
      "Epoch 9/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4125 - <lambda>: 2.4125 - val_loss: 2.7591 - val_<lambda>: 2.7591\n",
      "Epoch 10/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4143 - <lambda>: 2.4143 - val_loss: 2.6490 - val_<lambda>: 2.6490\n",
      "Epoch 11/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.4054 - <lambda>: 2.4054 - val_loss: 2.6763 - val_<lambda>: 2.6763\n",
      "Epoch 12/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3951 - <lambda>: 2.3951 - val_loss: 2.7150 - val_<lambda>: 2.7150\n",
      "Epoch 13/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3869 - <lambda>: 2.3869 - val_loss: 2.7208 - val_<lambda>: 2.7208\n",
      "Epoch 14/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3745 - <lambda>: 2.3745 - val_loss: 2.7292 - val_<lambda>: 2.7292\n",
      "Epoch 15/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3642 - <lambda>: 2.3642 - val_loss: 2.6764 - val_<lambda>: 2.6764\n",
      "Epoch 16/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3652 - <lambda>: 2.3652 - val_loss: 2.6803 - val_<lambda>: 2.6803\n",
      "Epoch 17/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3671 - <lambda>: 2.3671 - val_loss: 2.6770 - val_<lambda>: 2.6770\n",
      "Epoch 18/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3549 - <lambda>: 2.3549 - val_loss: 2.7239 - val_<lambda>: 2.7239\n",
      "Epoch 19/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3435 - <lambda>: 2.3435 - val_loss: 2.6669 - val_<lambda>: 2.6669\n",
      "Epoch 20/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3332 - <lambda>: 2.3332 - val_loss: 2.7245 - val_<lambda>: 2.7245\n",
      "Epoch 21/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3355 - <lambda>: 2.3355 - val_loss: 2.6901 - val_<lambda>: 2.6901\n",
      "Epoch 22/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3305 - <lambda>: 2.3305 - val_loss: 2.7383 - val_<lambda>: 2.7383\n",
      "Epoch 23/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3180 - <lambda>: 2.3180 - val_loss: 2.6928 - val_<lambda>: 2.6928\n",
      "Epoch 24/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3037 - <lambda>: 2.3037 - val_loss: 2.6717 - val_<lambda>: 2.6717\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.3069 - <lambda>: 2.3069 - val_loss: 2.7001 - val_<lambda>: 2.7001\n",
      "Epoch 26/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2979 - <lambda>: 2.2979 - val_loss: 2.7456 - val_<lambda>: 2.7456\n",
      "Epoch 27/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2919 - <lambda>: 2.2919 - val_loss: 2.7322 - val_<lambda>: 2.7322\n",
      "Epoch 28/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2943 - <lambda>: 2.2943 - val_loss: 2.7319 - val_<lambda>: 2.7319\n",
      "Epoch 29/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2829 - <lambda>: 2.2829 - val_loss: 2.7252 - val_<lambda>: 2.7252\n",
      "Epoch 30/500\n",
      "2295/2296 [============================>.] - ETA: 0s - loss: 2.2793 - <lambda>: 2.2793\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2792 - <lambda>: 2.2792 - val_loss: 2.7072 - val_<lambda>: 2.7072\n",
      "Epoch 31/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2579 - <lambda>: 2.2579 - val_loss: 2.7552 - val_<lambda>: 2.7552\n",
      "Epoch 32/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2559 - <lambda>: 2.2559 - val_loss: 2.7322 - val_<lambda>: 2.7322\n",
      "Epoch 33/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2499 - <lambda>: 2.2499 - val_loss: 2.7137 - val_<lambda>: 2.7137\n",
      "Epoch 34/500\n",
      "2296/2296 [==============================] - 39s 17ms/step - loss: 2.2479 - <lambda>: 2.2479 - val_loss: 2.6869 - val_<lambda>: 2.6869\n",
      "Epoch 35/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2413 - <lambda>: 2.2413 - val_loss: 2.7267 - val_<lambda>: 2.7267\n",
      "Epoch 36/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2340 - <lambda>: 2.2340 - val_loss: 2.7711 - val_<lambda>: 2.7711\n",
      "Epoch 37/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2324 - <lambda>: 2.2324 - val_loss: 2.7317 - val_<lambda>: 2.7317\n",
      "Epoch 38/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2204 - <lambda>: 2.2204 - val_loss: 2.7994 - val_<lambda>: 2.7994\n",
      "Epoch 39/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2330 - <lambda>: 2.2330 - val_loss: 2.7115 - val_<lambda>: 2.7115\n",
      "Epoch 40/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 2.2153 - <lambda>: 2.2153 - val_loss: 2.7604 - val_<lambda>: 2.7604\n",
      "\n",
      "\n",
      "  0.7 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_27 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "2296/2296 [==============================] - 36s 16ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 2/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 3/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 4/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 5/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 6/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 7/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 8/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 9/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 10/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 11/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 12/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 13/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 14/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 15/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 16/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 17/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 18/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 19/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 20/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 21/500\n",
      "2296/2296 [==============================] - ETA: 0s - loss: 12.2223 - <lambda>: 12.2223\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 22/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 23/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 24/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 25/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 26/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 27/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 28/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 29/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2223 - <lambda>: 12.2223 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 30/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "Epoch 31/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 12.2224 - <lambda>: 12.2224 - val_loss: 13.0311 - val_<lambda>: 13.0311\n",
      "\n",
      "\n",
      "  0.9 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_28 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "2296/2296 [==============================] - 36s 15ms/step - loss: 1.2355 - <lambda>: 1.2355 - val_loss: 1.0072 - val_<lambda>: 1.0072\n",
      "Epoch 2/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.9423 - <lambda>: 0.9423 - val_loss: 0.9538 - val_<lambda>: 0.9538\n",
      "Epoch 3/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.9090 - <lambda>: 0.9090 - val_loss: 0.9145 - val_<lambda>: 0.9145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8861 - <lambda>: 0.8861 - val_loss: 0.9712 - val_<lambda>: 0.9712\n",
      "Epoch 5/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8677 - <lambda>: 0.8677 - val_loss: 0.9468 - val_<lambda>: 0.9468\n",
      "Epoch 6/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8521 - <lambda>: 0.8521 - val_loss: 0.9640 - val_<lambda>: 0.9640\n",
      "Epoch 7/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8585 - <lambda>: 0.8585 - val_loss: 0.9001 - val_<lambda>: 0.9001\n",
      "Epoch 8/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8379 - <lambda>: 0.8379 - val_loss: 0.9089 - val_<lambda>: 0.9089\n",
      "Epoch 9/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8309 - <lambda>: 0.8309 - val_loss: 0.9308 - val_<lambda>: 0.9308\n",
      "Epoch 10/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8328 - <lambda>: 0.8328 - val_loss: 1.0083 - val_<lambda>: 1.0083\n",
      "Epoch 11/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8235 - <lambda>: 0.8235 - val_loss: 0.8654 - val_<lambda>: 0.8654\n",
      "Epoch 12/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8254 - <lambda>: 0.8254 - val_loss: 0.8719 - val_<lambda>: 0.8719\n",
      "Epoch 13/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8176 - <lambda>: 0.8176 - val_loss: 0.8733 - val_<lambda>: 0.8733\n",
      "Epoch 14/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8122 - <lambda>: 0.8122 - val_loss: 0.9301 - val_<lambda>: 0.9301\n",
      "Epoch 15/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8053 - <lambda>: 0.8053 - val_loss: 0.9027 - val_<lambda>: 0.9027\n",
      "Epoch 16/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8061 - <lambda>: 0.8061 - val_loss: 0.8806 - val_<lambda>: 0.8806\n",
      "Epoch 17/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8003 - <lambda>: 0.8003 - val_loss: 0.9193 - val_<lambda>: 0.9193\n",
      "Epoch 18/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.8028 - <lambda>: 0.8028 - val_loss: 0.8861 - val_<lambda>: 0.8861\n",
      "Epoch 19/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7907 - <lambda>: 0.7907 - val_loss: 0.9116 - val_<lambda>: 0.9116\n",
      "Epoch 20/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7941 - <lambda>: 0.7941 - val_loss: 0.9154 - val_<lambda>: 0.9154\n",
      "Epoch 21/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7894 - <lambda>: 0.7894 - val_loss: 0.8777 - val_<lambda>: 0.8777\n",
      "Epoch 22/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7901 - <lambda>: 0.7901 - val_loss: 0.9332 - val_<lambda>: 0.9332\n",
      "Epoch 23/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7840 - <lambda>: 0.7840 - val_loss: 0.9032 - val_<lambda>: 0.9032\n",
      "Epoch 24/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7836 - <lambda>: 0.7836 - val_loss: 0.8695 - val_<lambda>: 0.8695\n",
      "Epoch 25/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7807 - <lambda>: 0.7807 - val_loss: 0.8623 - val_<lambda>: 0.8623\n",
      "Epoch 26/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7792 - <lambda>: 0.7792 - val_loss: 0.9225 - val_<lambda>: 0.9225\n",
      "Epoch 27/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7750 - <lambda>: 0.7750 - val_loss: 0.8670 - val_<lambda>: 0.8670\n",
      "Epoch 28/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7769 - <lambda>: 0.7769 - val_loss: 0.8729 - val_<lambda>: 0.8729\n",
      "Epoch 29/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7760 - <lambda>: 0.7760 - val_loss: 0.9536 - val_<lambda>: 0.9536\n",
      "Epoch 30/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7716 - <lambda>: 0.7716 - val_loss: 0.8866 - val_<lambda>: 0.8866\n",
      "Epoch 31/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7730 - <lambda>: 0.7730 - val_loss: 0.8957 - val_<lambda>: 0.8957\n",
      "Epoch 32/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7733 - <lambda>: 0.7733 - val_loss: 0.9244 - val_<lambda>: 0.9244\n",
      "Epoch 33/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7691 - <lambda>: 0.7691 - val_loss: 0.8761 - val_<lambda>: 0.8761\n",
      "Epoch 34/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7730 - <lambda>: 0.7730 - val_loss: 0.8604 - val_<lambda>: 0.8604\n",
      "Epoch 35/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7663 - <lambda>: 0.7663 - val_loss: 0.9249 - val_<lambda>: 0.9249\n",
      "Epoch 36/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7657 - <lambda>: 0.7657 - val_loss: 0.8713 - val_<lambda>: 0.8713\n",
      "Epoch 37/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7653 - <lambda>: 0.7653 - val_loss: 0.8678 - val_<lambda>: 0.8678\n",
      "Epoch 38/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7589 - <lambda>: 0.7589 - val_loss: 0.8924 - val_<lambda>: 0.8924\n",
      "Epoch 39/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7635 - <lambda>: 0.7635 - val_loss: 0.8685 - val_<lambda>: 0.8685\n",
      "Epoch 40/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7568 - <lambda>: 0.7568 - val_loss: 0.9575 - val_<lambda>: 0.9575\n",
      "Epoch 41/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7650 - <lambda>: 0.7650 - val_loss: 0.8779 - val_<lambda>: 0.8779\n",
      "Epoch 42/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7586 - <lambda>: 0.7586 - val_loss: 0.8636 - val_<lambda>: 0.8636\n",
      "Epoch 43/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7514 - <lambda>: 0.7514 - val_loss: 0.9216 - val_<lambda>: 0.9216\n",
      "Epoch 44/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7529 - <lambda>: 0.7529 - val_loss: 0.8878 - val_<lambda>: 0.8878\n",
      "Epoch 45/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7586 - <lambda>: 0.7586 - val_loss: 0.8838 - val_<lambda>: 0.8838\n",
      "Epoch 46/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7504 - <lambda>: 0.7504 - val_loss: 0.9811 - val_<lambda>: 0.9811\n",
      "Epoch 47/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7529 - <lambda>: 0.7529 - val_loss: 0.8614 - val_<lambda>: 0.8614\n",
      "Epoch 48/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7513 - <lambda>: 0.7513 - val_loss: 0.8673 - val_<lambda>: 0.8673\n",
      "Epoch 49/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7502 - <lambda>: 0.7502 - val_loss: 0.9134 - val_<lambda>: 0.9134\n",
      "Epoch 50/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7476 - <lambda>: 0.7476 - val_loss: 0.9237 - val_<lambda>: 0.9237\n",
      "Epoch 51/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7519 - <lambda>: 0.7519 - val_loss: 0.8794 - val_<lambda>: 0.8794\n",
      "Epoch 52/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7454 - <lambda>: 0.7454 - val_loss: 0.8822 - val_<lambda>: 0.8822\n",
      "Epoch 53/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7476 - <lambda>: 0.7476 - val_loss: 0.8810 - val_<lambda>: 0.8810\n",
      "Epoch 54/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7453 - <lambda>: 0.7453 - val_loss: 0.8586 - val_<lambda>: 0.8586\n",
      "Epoch 55/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7431 - <lambda>: 0.7431 - val_loss: 0.8648 - val_<lambda>: 0.8648\n",
      "Epoch 56/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7427 - <lambda>: 0.7427 - val_loss: 0.8648 - val_<lambda>: 0.8648\n",
      "Epoch 57/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7466 - <lambda>: 0.7466 - val_loss: 0.8982 - val_<lambda>: 0.8982\n",
      "Epoch 58/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7467 - <lambda>: 0.7467 - val_loss: 0.8734 - val_<lambda>: 0.8734\n",
      "Epoch 59/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7438 - <lambda>: 0.7438 - val_loss: 0.9229 - val_<lambda>: 0.9229\n",
      "Epoch 60/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7442 - <lambda>: 0.7442 - val_loss: 0.9274 - val_<lambda>: 0.9274\n",
      "Epoch 61/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7410 - <lambda>: 0.7410 - val_loss: 0.9350 - val_<lambda>: 0.9350\n",
      "Epoch 62/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7383 - <lambda>: 0.7383 - val_loss: 0.8773 - val_<lambda>: 0.8773\n",
      "Epoch 63/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7376 - <lambda>: 0.7376 - val_loss: 0.8781 - val_<lambda>: 0.8781\n",
      "Epoch 64/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7377 - <lambda>: 0.7377 - val_loss: 0.9101 - val_<lambda>: 0.9101\n",
      "Epoch 65/500\n",
      "2296/2296 [==============================] - 38s 17ms/step - loss: 0.7375 - <lambda>: 0.7375 - val_loss: 0.9113 - val_<lambda>: 0.9113\n",
      "Epoch 66/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7338 - <lambda>: 0.7338 - val_loss: 0.8820 - val_<lambda>: 0.8820\n",
      "Epoch 67/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7341 - <lambda>: 0.7341 - val_loss: 0.9009 - val_<lambda>: 0.9009\n",
      "Epoch 68/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7352 - <lambda>: 0.7352 - val_loss: 0.8760 - val_<lambda>: 0.8760\n",
      "Epoch 69/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7342 - <lambda>: 0.7342 - val_loss: 0.8801 - val_<lambda>: 0.8801\n",
      "Epoch 70/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7335 - <lambda>: 0.7335 - val_loss: 0.8917 - val_<lambda>: 0.8917\n",
      "Epoch 71/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7327 - <lambda>: 0.7327 - val_loss: 0.9052 - val_<lambda>: 0.9052\n",
      "Epoch 72/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7297 - <lambda>: 0.7297 - val_loss: 0.8906 - val_<lambda>: 0.8906\n",
      "Epoch 73/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7310 - <lambda>: 0.7310 - val_loss: 0.9089 - val_<lambda>: 0.9089\n",
      "Epoch 74/500\n",
      "2295/2296 [============================>.] - ETA: 0s - loss: 0.7324 - <lambda>: 0.7324\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7325 - <lambda>: 0.7325 - val_loss: 0.9005 - val_<lambda>: 0.9005\n",
      "Epoch 75/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7253 - <lambda>: 0.7253 - val_loss: 0.9224 - val_<lambda>: 0.9224\n",
      "Epoch 76/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7239 - <lambda>: 0.7239 - val_loss: 0.8949 - val_<lambda>: 0.8949\n",
      "Epoch 77/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7219 - <lambda>: 0.7219 - val_loss: 0.8850 - val_<lambda>: 0.8850\n",
      "Epoch 78/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7216 - <lambda>: 0.7216 - val_loss: 0.8718 - val_<lambda>: 0.8718\n",
      "Epoch 79/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7201 - <lambda>: 0.7201 - val_loss: 0.9207 - val_<lambda>: 0.9207\n",
      "Epoch 80/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7202 - <lambda>: 0.7202 - val_loss: 0.9732 - val_<lambda>: 0.9732\n",
      "Epoch 81/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7212 - <lambda>: 0.7212 - val_loss: 0.9142 - val_<lambda>: 0.9142\n",
      "Epoch 82/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7201 - <lambda>: 0.7201 - val_loss: 0.8993 - val_<lambda>: 0.8993\n",
      "Epoch 83/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7197 - <lambda>: 0.7197 - val_loss: 0.8808 - val_<lambda>: 0.8808\n",
      "Epoch 84/500\n",
      "2296/2296 [==============================] - 35s 15ms/step - loss: 0.7180 - <lambda>: 0.7180 - val_loss: 0.8847 - val_<lambda>: 0.8847\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must have equal len keys and value when setting with an ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-dc2cf828a259>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[0mdf_temp2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_temp2\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[0mnum_temp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_temp2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m \u001b[0msubmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Day8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"q_0.1\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_temp2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;31m##??????????????     ?????????   \u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[0miloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"iloc\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m         \u001b[0miloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   1727\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1728\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0milocs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1729\u001b[1;33m                         raise ValueError(\n\u001b[0m\u001b[0;32m   1730\u001b[0m                             \u001b[1;34m\"Must have equal len keys and value \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m                             \u001b[1;34m\"when setting with an ndarray\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Must have equal len keys and value when setting with an ndarray"
     ]
    }
   ],
   "source": [
    "##### ~~~!!!!! >o< ^o^\n",
    "###T-Td  (GHI )\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "import tensorflow.keras.backend as K\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model,Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPool1D,Flatten, Reshape, LSTM,Input,concatenate,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from lightgbm import LGBMRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#  \n",
    "train = pd.read_csv('./train/train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "#1. DATA\n",
    "\n",
    "# GHI column  \n",
    "def Add_features(data):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "# train data column\n",
    "def preprocess_data(data, is_train=True):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "    temp = data.copy()\n",
    "    temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "    if is_train==True:          \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')   #  Target\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill') #  Target\n",
    "        temp = temp.dropna()    #  \n",
    "        return temp.iloc[:-96]  #   . (   )\n",
    "\n",
    "    elif is_train == False:\n",
    "        temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "        return temp.iloc[-48:, :]\n",
    "\n",
    "df_train = preprocess_data(train)\n",
    "# print(df_train.shape)   # (52464, 10)\n",
    "# print(df_train[:48])\n",
    "\n",
    "#-------------test \n",
    "df_test = []\n",
    "for i in range(81):\n",
    "    file_path = 'test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    df_test.append(temp)\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "test = np.array(df_test)\n",
    "print(df_test.shape) # (3888, 8)\n",
    "x_test = df_test.to_numpy()\n",
    "\n",
    "###=====================================================================\n",
    "#  () --> y1, y2    \n",
    "def split_xy(dataset, time_steps) :\n",
    "    x, y1, y2 = [],[],[]\n",
    "    for i in range(len(dataset)) :\n",
    "        x_end = i + time_steps\n",
    "        if x_end > len(dataset) :\n",
    "            break\n",
    "        tmp_x = dataset[i:x_end, :-2] # ['Hour' ~ 'T']\n",
    "        tmp_y1 = dataset[x_end-1:x_end,-2] # Target1\n",
    "        tmp_y2 = dataset[x_end-1:x_end,-1]   # Target2\n",
    "        x.append(tmp_x)\n",
    "        y1.append(tmp_y1)\n",
    "        y2.append(tmp_y2)\n",
    "    return np.array(x), np.array(y1), np.array(y2)\n",
    "\n",
    "X = df_train.to_numpy()\n",
    "# print(X.shape)      # (52464, 10)\n",
    "x,y1,y2 = split_xy(X,1)\n",
    "print(x.shape, y1.shape, y2.shape) #(52464, 1, 8) (52464, 1) (52464, 1)\n",
    "\n",
    "#####===========\n",
    "x_train, x_val, y1_train, y1_val, y2_train, y2_val = train_test_split(x,y1,y2,\n",
    "                           train_size = 0.7,shuffle = False, random_state = 0)\n",
    "\n",
    "# print(x_train.shape,x_val.shape) #(36724, 1, 8) (15740, 1, 8)\n",
    "print(y1_train.shape, y1_val.shape, y2_train.shape, y2_val.shape) #(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
    "x_train= x_train.reshape(36724*1, 10)\n",
    "x_val= x_val.reshape(15740*1, 10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "x_train= x_train.reshape(36724, 1, 10)\n",
    "x_val= x_val.reshape(15740, 1, 10)\n",
    "x_test= x_test.reshape(3888, 1, 10)\n",
    "# print(x_train.shape,x_val.shape,x_test.shape) #(36724, 1, 8) (15740, 1, 8) (3888, 1, 8)\n",
    "\n",
    "#  : Quantile loss definition\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "    err = (y_true - y_pred)\n",
    "    return K.mean(K.maximum(q*err, (q-1)*err), axis=-1)\n",
    "\n",
    "q = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "#2. \n",
    "\n",
    "def modeling() :\n",
    "    inputs = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    adam = Adam(learning_rate=0.003)\n",
    "    conv = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Flatten()(conv)\n",
    "\n",
    "    conv1 = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Flatten()(conv1)\n",
    "\n",
    "    conv2 = Conv1D(filters = 256,kernel_size=3,activation='relu',padding='same')(inputs)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 64,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 32,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Flatten()(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters = 256,kernel_size=1,activation='relu',padding='same')(inputs)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 64,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 32,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Flatten()(conv3)\n",
    "    \n",
    "    lstm = GRU(256,activation='relu')(inputs)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    concat = concatenate([conv,conv1,conv2,conv3,lstm])\n",
    "    dense = Dense(128,activation='relu')(concat)\n",
    "    dense = Dense(64,activation='relu')(dense)\n",
    "    dense = Dense(32,activation='relu')(dense)\n",
    "    dense = Dense(8,activation='relu')(dense)\n",
    "    outputs = Dense(1,activation='relu')(dense)\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model\n",
    "# 3. , \n",
    "#####========,  , predict\n",
    "#y1, y2  \n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 30)\n",
    "lr = ReduceLROnPlateau(monitor = 'val_loss', patience = 20, factor = 0.9, verbose = 1)\n",
    "epochs = 500\n",
    "bs = 16\n",
    "\n",
    "# !!\n",
    "# x = []\n",
    "for i in q:\n",
    "    if q!=0.5:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models2/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = lambda y_true,y_pred: quantile_loss(i,y_true,y_pred), optimizer = 'adam', metrics = [lambda y,y_pred: quantile_loss(i,y,y_pred)])\n",
    "        model.fit(x_train,y1_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y1_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "    else:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models2/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = 'mae', optimizer = 'adam')\n",
    "        model.fit(x_train,y1_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y1_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "df_temp1 = pd.concat(x, axis = 1)\n",
    "df_temp1[df_temp1<0] = 0\n",
    "num_temp1 = df_temp1.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = num_temp1\n",
    "\n",
    "x = []\n",
    "# !!\n",
    "for i in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.9]:\n",
    "    if q!=0.5:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models2/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = lambda y_true,y_pred: quantile_loss(i,y_true,y_pred), optimizer = 'adam', metrics = [lambda y,y_pred: quantile_loss(i,y,y_pred)])\n",
    "        model.fit(x_train,y2_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y2_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "    else:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models2/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = 'mae', optimizer = 'adam')\n",
    "        model.fit(x_train,y2_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y2_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "df_temp2 = pd.concat(x, axis = 1)\n",
    "df_temp2[df_temp2<0] = 0\n",
    "num_temp2 = df_temp2.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = num_temp2\n",
    "\n",
    "##??????????????     ?????????   \n",
    "submission.to_csv('models2/upgrade_sub_0125__2.csv', index = False)\n",
    "##  135  -- 1.94653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3888, 10)\n",
      "(52464, 1, 10) (52464, 1) (52464, 1)\n",
      "(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
      "\n",
      "\n",
      "  0.1 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_11 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.2 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_12 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.3 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_13 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.4 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_14 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.5 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_15 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.6 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_16 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.7 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_17 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.8 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_18 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.9 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_19 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.1 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_21 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.2 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_22 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.3 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_23 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.4 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_24 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.5 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_25 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.6 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_26 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.7 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_27 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.8 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_20 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      "  0.9 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer gru_28 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "##### ~~~!!!!! >o< ^o^\n",
    "###T-Td  (GHI )\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "import tensorflow.keras.backend as K\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model,Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPool1D,Flatten, Reshape, LSTM,Input,concatenate,BatchNormalization,LayerNormalization,GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from lightgbm import LGBMRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#  \n",
    "train = pd.read_csv('./train/train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "#1. DATA\n",
    "\n",
    "# GHI column  \n",
    "def Add_features(data):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "# train data column\n",
    "def preprocess_data(data, is_train=True):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "    temp = data.copy()\n",
    "    temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "    if is_train==True:          \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')   #  Target\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill') #  Target\n",
    "        temp = temp.dropna()    #  \n",
    "        return temp.iloc[:-96]  #   . (   )\n",
    "\n",
    "    elif is_train == False:\n",
    "        temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "        return temp.iloc[-48:, :]\n",
    "\n",
    "df_train = preprocess_data(train)\n",
    "# print(df_train.shape)   # (52464, 10)\n",
    "# print(df_train[:48])\n",
    "\n",
    "#-------------test \n",
    "df_test = []\n",
    "for i in range(81):\n",
    "    file_path = 'test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    df_test.append(temp)\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "test = np.array(df_test)\n",
    "print(df_test.shape) # (3888, 8)\n",
    "x_test = df_test.to_numpy()\n",
    "\n",
    "###=====================================================================\n",
    "#  () --> y1, y2    \n",
    "def split_xy(dataset, time_steps) :\n",
    "    x, y1, y2 = [],[],[]\n",
    "    for i in range(len(dataset)) :\n",
    "        x_end = i + time_steps\n",
    "        if x_end > len(dataset) :\n",
    "            break\n",
    "        tmp_x = dataset[i:x_end, :-2] # ['Hour' ~ 'T']\n",
    "        tmp_y1 = dataset[x_end-1:x_end,-2] # Target1\n",
    "        tmp_y2 = dataset[x_end-1:x_end,-1]   # Target2\n",
    "        x.append(tmp_x)\n",
    "        y1.append(tmp_y1)\n",
    "        y2.append(tmp_y2)\n",
    "    return np.array(x), np.array(y1), np.array(y2)\n",
    "\n",
    "X = df_train.to_numpy()\n",
    "# print(X.shape)      # (52464, 10)\n",
    "x,y1,y2 = split_xy(X,1)\n",
    "print(x.shape, y1.shape, y2.shape) #(52464, 1, 8) (52464, 1) (52464, 1)\n",
    "\n",
    "#####===========\n",
    "x_train, x_val, y1_train, y1_val, y2_train, y2_val = train_test_split(x,y1,y2,\n",
    "                           train_size = 0.7,shuffle = False, random_state = 0)\n",
    "\n",
    "# print(x_train.shape,x_val.shape) #(36724, 1, 8) (15740, 1, 8)\n",
    "print(y1_train.shape, y1_val.shape, y2_train.shape, y2_val.shape) #(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
    "x_train= x_train.reshape(36724*1, 10)\n",
    "x_val= x_val.reshape(15740*1, 10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "x_train= x_train.reshape(36724, 1, 10)\n",
    "x_val= x_val.reshape(15740, 1, 10)\n",
    "x_test= x_test.reshape(3888, 1, 10)\n",
    "# print(x_train.shape,x_val.shape,x_test.shape) #(36724, 1, 8) (15740, 1, 8) (3888, 1, 8)\n",
    "\n",
    "#  : Quantile loss definition\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "    err = (y_true - y_pred)\n",
    "    return K.mean(K.maximum(q*err, (q-1)*err), axis=-1)\n",
    "\n",
    "q = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "#2. \n",
    "\n",
    "def modeling() :\n",
    "    inputs = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    adam = Adam(learning_rate=0.003)\n",
    "    conv = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Flatten()(conv)\n",
    "\n",
    "    conv1 = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Flatten()(conv1)\n",
    "\n",
    "    conv2 = Conv1D(filters = 256,kernel_size=3,activation='relu',padding='same')(inputs)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 64,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 32,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Flatten()(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters = 256,kernel_size=1,activation='relu',padding='same')(inputs)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 64,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 32,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Flatten()(conv3)\n",
    "    \n",
    "    lstm = GRU(256,activation='relu')(inputs)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    concat = concatenate([conv,conv1,conv2,conv3,lstm])\n",
    "    dense = Dense(128,activation='relu')(concat)\n",
    "    dense = Dense(64,activation='relu')(dense)\n",
    "    dense = Dense(32,activation='relu')(dense)\n",
    "    dense = Dense(8,activation='relu')(dense)\n",
    "    outputs = Dense(1,activation='relu')(dense)\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# 3. , \n",
    "#####========,  , predict\n",
    "#y1, y2  \n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "lr = ReduceLROnPlateau(monitor = 'val_loss', patience = 5, factor = 0.9, verbose = 1)\n",
    "epochs = 200\n",
    "bs = 512\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = []\n",
    "for i in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "    filepath_cp = f'models2/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "    model = load_model(filepath_cp,compile=False)\n",
    "    pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "    x.append(pred)\n",
    "df_temp1 = pd.concat(x, axis = 1)\n",
    "df_temp1[df_temp1<0] = 0\n",
    "num_temp1 = df_temp1.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = num_temp1\n",
    "\n",
    "x = []\n",
    "# !!\n",
    "for i in q:\n",
    "    print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "    filepath_cp = f'models2/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "    model = load_model(filepath_cp,compile=False)\n",
    "    pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "    x.append(pred)\n",
    "df_temp2 = pd.concat(x, axis = 1)\n",
    "df_temp2[df_temp2<0] = 0\n",
    "num_temp2 = df_temp2.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = num_temp2\n",
    "submission.to_csv('models2/upgrade_sub_0125_.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3888, 10)\n",
      "(52464, 1, 10) (52464, 1) (52464, 1)\n",
      "(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
      "\n",
      "\n",
      "  0.1 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_43 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_44 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_45 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "2296/2296 [==============================] - 46s 20ms/step - loss: 1.4483 - <lambda>: 1.4483 - val_loss: 1.5328 - val_<lambda>: 1.5328\n",
      "Epoch 2/500\n",
      "2296/2296 [==============================] - 46s 20ms/step - loss: 1.3980 - <lambda>: 1.3980 - val_loss: 1.5295 - val_<lambda>: 1.5295\n",
      "Epoch 3/500\n",
      "2296/2296 [==============================] - 46s 20ms/step - loss: 1.3720 - <lambda>: 1.3720 - val_loss: 1.5466 - val_<lambda>: 1.5466\n",
      "Epoch 4/500\n",
      "2296/2296 [==============================] - 45s 20ms/step - loss: 1.3609 - <lambda>: 1.3609 - val_loss: 1.5218 - val_<lambda>: 1.5218\n",
      "Epoch 5/500\n",
      "2296/2296 [==============================] - 45s 19ms/step - loss: 1.3594 - <lambda>: 1.3594 - val_loss: 1.5155 - val_<lambda>: 1.5155\n",
      "Epoch 6/500\n",
      "2296/2296 [==============================] - 44s 19ms/step - loss: 1.3522 - <lambda>: 1.3522 - val_loss: 1.5503 - val_<lambda>: 1.5503\n",
      "Epoch 7/500\n",
      "2296/2296 [==============================] - 45s 20ms/step - loss: 1.3481 - <lambda>: 1.3481 - val_loss: 1.5229 - val_<lambda>: 1.5229\n",
      "Epoch 8/500\n",
      "2296/2296 [==============================] - 44s 19ms/step - loss: 1.3402 - <lambda>: 1.3402 - val_loss: 1.6063 - val_<lambda>: 1.6063\n",
      "Epoch 9/500\n",
      "2296/2296 [==============================] - 45s 19ms/step - loss: 1.3339 - <lambda>: 1.3339 - val_loss: 1.5113 - val_<lambda>: 1.5113\n",
      "Epoch 10/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.3313 - <lambda>: 1.3313 - val_loss: 1.5904 - val_<lambda>: 1.5904\n",
      "Epoch 11/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.3256 - <lambda>: 1.3256 - val_loss: 1.4974 - val_<lambda>: 1.4974\n",
      "Epoch 12/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.3227 - <lambda>: 1.3227 - val_loss: 1.5170 - val_<lambda>: 1.5170\n",
      "Epoch 13/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.3214 - <lambda>: 1.3214 - val_loss: 1.5278 - val_<lambda>: 1.5278\n",
      "Epoch 14/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.3162 - <lambda>: 1.3162 - val_loss: 1.5068 - val_<lambda>: 1.5068\n",
      "Epoch 15/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.3105 - <lambda>: 1.3105 - val_loss: 1.5318 - val_<lambda>: 1.5318\n",
      "Epoch 16/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.3083 - <lambda>: 1.3083 - val_loss: 1.5059 - val_<lambda>: 1.5059\n",
      "Epoch 17/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.3000 - <lambda>: 1.3000 - val_loss: 1.5218 - val_<lambda>: 1.5218\n",
      "Epoch 18/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.2938 - <lambda>: 1.2938 - val_loss: 1.5256 - val_<lambda>: 1.5256\n",
      "Epoch 19/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.2874 - <lambda>: 1.2874 - val_loss: 1.5051 - val_<lambda>: 1.5051\n",
      "Epoch 20/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.2868 - <lambda>: 1.2868 - val_loss: 1.5299 - val_<lambda>: 1.5299\n",
      "Epoch 21/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.2848 - <lambda>: 1.2848 - val_loss: 1.5271 - val_<lambda>: 1.5271\n",
      "Epoch 22/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.2730 - <lambda>: 1.2730 - val_loss: 1.5279 - val_<lambda>: 1.5279\n",
      "Epoch 23/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.2726 - <lambda>: 1.2726 - val_loss: 1.5381 - val_<lambda>: 1.5381\n",
      "Epoch 24/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.2601 - <lambda>: 1.2601 - val_loss: 1.5866 - val_<lambda>: 1.5866\n",
      "Epoch 25/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.2567 - <lambda>: 1.2567 - val_loss: 1.5168 - val_<lambda>: 1.5168\n",
      "Epoch 26/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.2577 - <lambda>: 1.2577 - val_loss: 1.5245 - val_<lambda>: 1.5245\n",
      "Epoch 27/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.2476 - <lambda>: 1.2476 - val_loss: 1.5930 - val_<lambda>: 1.5930\n",
      "Epoch 28/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.2442 - <lambda>: 1.2442 - val_loss: 1.5302 - val_<lambda>: 1.5302\n",
      "Epoch 29/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.2424 - <lambda>: 1.2424 - val_loss: 1.5858 - val_<lambda>: 1.5858\n",
      "Epoch 30/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.2369 - <lambda>: 1.2369 - val_loss: 1.5393 - val_<lambda>: 1.5393\n",
      "Epoch 31/500\n",
      "2294/2296 [============================>.] - ETA: 0s - loss: 1.2234 - <lambda>: 1.2234\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "2296/2296 [==============================] - 44s 19ms/step - loss: 1.2232 - <lambda>: 1.2232 - val_loss: 1.6090 - val_<lambda>: 1.6090\n",
      "Epoch 32/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.2192 - <lambda>: 1.2192 - val_loss: 1.6071 - val_<lambda>: 1.6071\n",
      "Epoch 33/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.2117 - <lambda>: 1.2117 - val_loss: 1.5655 - val_<lambda>: 1.5655\n",
      "Epoch 34/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.2050 - <lambda>: 1.2050 - val_loss: 1.5915 - val_<lambda>: 1.5915\n",
      "Epoch 35/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.2074 - <lambda>: 1.2074 - val_loss: 1.7003 - val_<lambda>: 1.7003\n",
      "Epoch 36/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.2016 - <lambda>: 1.2016 - val_loss: 1.7049 - val_<lambda>: 1.7049\n",
      "Epoch 37/500\n",
      "2296/2296 [==============================] - 44s 19ms/step - loss: 1.1944 - <lambda>: 1.1944 - val_loss: 1.6044 - val_<lambda>: 1.6044\n",
      "Epoch 38/500\n",
      "2296/2296 [==============================] - 44s 19ms/step - loss: 1.1866 - <lambda>: 1.1866 - val_loss: 1.8368 - val_<lambda>: 1.8368\n",
      "Epoch 39/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.1847 - <lambda>: 1.1847 - val_loss: 1.7531 - val_<lambda>: 1.7531\n",
      "Epoch 40/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.1795 - <lambda>: 1.1795 - val_loss: 1.9933 - val_<lambda>: 1.9933\n",
      "Epoch 41/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.1794 - <lambda>: 1.1794 - val_loss: 1.6967 - val_<lambda>: 1.6967\n",
      "\n",
      "\n",
      "  0.2 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_46 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_47 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "2296/2296 [==============================] - 44s 19ms/step - loss: 2.3576 - <lambda>: 2.3576 - val_loss: 2.4954 - val_<lambda>: 2.4954\n",
      "Epoch 2/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 2.2573 - <lambda>: 2.2573 - val_loss: 2.4799 - val_<lambda>: 2.4799\n",
      "Epoch 3/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 2.2313 - <lambda>: 2.2313 - val_loss: 2.4665 - val_<lambda>: 2.4665\n",
      "Epoch 4/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 2.2082 - <lambda>: 2.2082 - val_loss: 2.4782 - val_<lambda>: 2.4782\n",
      "Epoch 5/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 2.1925 - <lambda>: 2.1925 - val_loss: 2.5835 - val_<lambda>: 2.5835\n",
      "Epoch 6/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.1804 - <lambda>: 2.1804 - val_loss: 2.5048 - val_<lambda>: 2.5048\n",
      "Epoch 7/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.1699 - <lambda>: 2.1699 - val_loss: 2.5190 - val_<lambda>: 2.5190\n",
      "Epoch 8/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 2.1603 - <lambda>: 2.1603 - val_loss: 2.4608 - val_<lambda>: 2.4608\n",
      "Epoch 9/500\n",
      "2296/2296 [==============================] - 42s 19ms/step - loss: 2.1491 - <lambda>: 2.1491 - val_loss: 2.4432 - val_<lambda>: 2.4432\n",
      "Epoch 10/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.1364 - <lambda>: 2.1364 - val_loss: 2.4997 - val_<lambda>: 2.4997\n",
      "Epoch 11/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.1265 - <lambda>: 2.1265 - val_loss: 2.4485 - val_<lambda>: 2.4485\n",
      "Epoch 12/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.1158 - <lambda>: 2.1158 - val_loss: 2.4127 - val_<lambda>: 2.4127\n",
      "Epoch 13/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.1158 - <lambda>: 2.1158 - val_loss: 2.4672 - val_<lambda>: 2.4672\n",
      "Epoch 14/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.0964 - <lambda>: 2.0964 - val_loss: 2.4842 - val_<lambda>: 2.4842\n",
      "Epoch 15/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.0837 - <lambda>: 2.0837 - val_loss: 2.4696 - val_<lambda>: 2.4696\n",
      "Epoch 16/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 2.0732 - <lambda>: 2.0732 - val_loss: 2.4716 - val_<lambda>: 2.4716\n",
      "Epoch 17/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.0692 - <lambda>: 2.0692 - val_loss: 2.5734 - val_<lambda>: 2.5734\n",
      "Epoch 18/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.0560 - <lambda>: 2.0560 - val_loss: 2.4226 - val_<lambda>: 2.4226\n",
      "Epoch 19/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 2.0555 - <lambda>: 2.0555 - val_loss: 2.4526 - val_<lambda>: 2.4526\n",
      "Epoch 20/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.0428 - <lambda>: 2.0428 - val_loss: 2.4513 - val_<lambda>: 2.4513\n",
      "Epoch 21/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.0371 - <lambda>: 2.0371 - val_loss: 2.4574 - val_<lambda>: 2.4574\n",
      "Epoch 22/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.0217 - <lambda>: 2.0217 - val_loss: 2.4832 - val_<lambda>: 2.4832\n",
      "Epoch 23/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.0158 - <lambda>: 2.0158 - val_loss: 2.4866 - val_<lambda>: 2.4866\n",
      "Epoch 24/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.0083 - <lambda>: 2.0083 - val_loss: 2.4996 - val_<lambda>: 2.4996\n",
      "Epoch 25/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.9942 - <lambda>: 1.9942 - val_loss: 2.4987 - val_<lambda>: 2.4987\n",
      "Epoch 26/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 1.9790 - <lambda>: 1.9790 - val_loss: 2.5259 - val_<lambda>: 2.5259\n",
      "Epoch 27/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 1.9894 - <lambda>: 1.9894 - val_loss: 2.6328 - val_<lambda>: 2.6328\n",
      "Epoch 28/500\n",
      "2296/2296 [==============================] - 42s 19ms/step - loss: 1.9704 - <lambda>: 1.9704 - val_loss: 2.5489 - val_<lambda>: 2.5489\n",
      "Epoch 29/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 1.9680 - <lambda>: 1.9680 - val_loss: 2.4972 - val_<lambda>: 2.4972\n",
      "Epoch 30/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 1.9483 - <lambda>: 1.9483 - val_loss: 2.5054 - val_<lambda>: 2.5054\n",
      "Epoch 31/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 1.9445 - <lambda>: 1.9445 - val_loss: 2.5057 - val_<lambda>: 2.5057\n",
      "Epoch 32/500\n",
      "2294/2296 [============================>.] - ETA: 0s - loss: 1.9387 - <lambda>: 1.9387\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 1.9383 - <lambda>: 1.9383 - val_loss: 2.5276 - val_<lambda>: 2.5276\n",
      "Epoch 33/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 1.9204 - <lambda>: 1.9204 - val_loss: 2.5864 - val_<lambda>: 2.5864\n",
      "Epoch 34/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 1.9151 - <lambda>: 1.9151 - val_loss: 2.5788 - val_<lambda>: 2.5788\n",
      "Epoch 35/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 1.9019 - <lambda>: 1.9019 - val_loss: 2.5697 - val_<lambda>: 2.5697\n",
      "Epoch 36/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 1.8940 - <lambda>: 1.8940 - val_loss: 2.5153 - val_<lambda>: 2.5153\n",
      "Epoch 37/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 1.8873 - <lambda>: 1.8873 - val_loss: 2.5402 - val_<lambda>: 2.5402\n",
      "Epoch 38/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.8782 - <lambda>: 1.8782 - val_loss: 2.5504 - val_<lambda>: 2.5504\n",
      "Epoch 39/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 1.8769 - <lambda>: 1.8769 - val_loss: 2.5404 - val_<lambda>: 2.5404\n",
      "Epoch 40/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 1.8538 - <lambda>: 1.8538 - val_loss: 2.5475 - val_<lambda>: 2.5475\n",
      "Epoch 41/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 1.8521 - <lambda>: 1.8521 - val_loss: 2.5481 - val_<lambda>: 2.5481\n",
      "Epoch 42/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 1.8542 - <lambda>: 1.8542 - val_loss: 2.5543 - val_<lambda>: 2.5543\n",
      "\n",
      "\n",
      "  0.3 \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_50 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_51 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 2.8491 - <lambda>: 2.8491 - val_loss: 2.9227 - val_<lambda>: 2.9227\n",
      "Epoch 2/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 2.6781 - <lambda>: 2.6781 - val_loss: 2.9137 - val_<lambda>: 2.9137\n",
      "Epoch 3/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.6551 - <lambda>: 2.6551 - val_loss: 2.9057 - val_<lambda>: 2.9057\n",
      "Epoch 4/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.6124 - <lambda>: 2.6124 - val_loss: 2.9430 - val_<lambda>: 2.9430\n",
      "Epoch 5/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.6037 - <lambda>: 2.6037 - val_loss: 2.8793 - val_<lambda>: 2.8793\n",
      "Epoch 6/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.5890 - <lambda>: 2.5890 - val_loss: 2.9050 - val_<lambda>: 2.9050\n",
      "Epoch 7/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.5739 - <lambda>: 2.5739 - val_loss: 2.9657 - val_<lambda>: 2.9657\n",
      "Epoch 8/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.5632 - <lambda>: 2.5632 - val_loss: 2.9105 - val_<lambda>: 2.9105\n",
      "Epoch 9/500\n",
      "2296/2296 [==============================] - 42s 19ms/step - loss: 2.5468 - <lambda>: 2.5468 - val_loss: 3.0868 - val_<lambda>: 3.0868\n",
      "Epoch 10/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 2.5370 - <lambda>: 2.5370 - val_loss: 2.9249 - val_<lambda>: 2.9249\n",
      "Epoch 11/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.5233 - <lambda>: 2.5233 - val_loss: 2.9429 - val_<lambda>: 2.9429\n",
      "Epoch 12/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 2.5223 - <lambda>: 2.5223 - val_loss: 2.8643 - val_<lambda>: 2.8643\n",
      "Epoch 13/500\n",
      "2296/2296 [==============================] - 42s 19ms/step - loss: 2.5112 - <lambda>: 2.5112 - val_loss: 2.8873 - val_<lambda>: 2.8873\n",
      "Epoch 14/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.4974 - <lambda>: 2.4974 - val_loss: 2.9348 - val_<lambda>: 2.9348\n",
      "Epoch 15/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.4948 - <lambda>: 2.4948 - val_loss: 2.8951 - val_<lambda>: 2.8951\n",
      "Epoch 16/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.4838 - <lambda>: 2.4838 - val_loss: 2.9716 - val_<lambda>: 2.9716\n",
      "Epoch 17/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.4756 - <lambda>: 2.4756 - val_loss: 2.9750 - val_<lambda>: 2.9750\n",
      "Epoch 18/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 2.4597 - <lambda>: 2.4597 - val_loss: 2.9855 - val_<lambda>: 2.9855\n",
      "Epoch 19/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.4500 - <lambda>: 2.4500 - val_loss: 2.9066 - val_<lambda>: 2.9066\n",
      "Epoch 20/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.4403 - <lambda>: 2.4403 - val_loss: 2.8941 - val_<lambda>: 2.8941\n",
      "Epoch 21/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.4338 - <lambda>: 2.4338 - val_loss: 2.9560 - val_<lambda>: 2.9560\n",
      "Epoch 22/500\n",
      "2296/2296 [==============================] - 42s 18ms/step - loss: 2.4171 - <lambda>: 2.4171 - val_loss: 2.9073 - val_<lambda>: 2.9073\n",
      "Epoch 23/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 2.4064 - <lambda>: 2.4064 - val_loss: 2.9052 - val_<lambda>: 2.9052\n",
      "Epoch 24/500\n",
      "2296/2296 [==============================] - 45s 20ms/step - loss: 2.4012 - <lambda>: 2.4012 - val_loss: 2.9212 - val_<lambda>: 2.9212\n",
      "Epoch 25/500\n",
      "2296/2296 [==============================] - 43s 19ms/step - loss: 2.3862 - <lambda>: 2.3862 - val_loss: 2.9527 - val_<lambda>: 2.9527\n",
      "Epoch 26/500\n",
      "2296/2296 [==============================] - 44s 19ms/step - loss: 2.3791 - <lambda>: 2.3791 - val_loss: 2.9552 - val_<lambda>: 2.9552\n",
      "Epoch 27/500\n",
      "2296/2296 [==============================] - 44s 19ms/step - loss: 2.3701 - <lambda>: 2.3701 - val_loss: 3.0559 - val_<lambda>: 3.0559\n",
      "Epoch 28/500\n",
      "2296/2296 [==============================] - 44s 19ms/step - loss: 2.3578 - <lambda>: 2.3578 - val_loss: 2.8968 - val_<lambda>: 2.8968\n",
      "Epoch 29/500\n",
      "2296/2296 [==============================] - 44s 19ms/step - loss: 2.3440 - <lambda>: 2.3440 - val_loss: 3.0240 - val_<lambda>: 3.0240\n",
      "Epoch 30/500\n",
      "2296/2296 [==============================] - 44s 19ms/step - loss: 2.3378 - <lambda>: 2.3378 - val_loss: 3.0244 - val_<lambda>: 3.0244\n",
      "Epoch 31/500\n",
      "1861/2296 [=======================>......] - ETA: 7s - loss: 2.3071 - <lambda>: 2.3071"
     ]
    }
   ],
   "source": [
    "##### ~~~!!!!! >o< ^o^\n",
    "###T-Td  (GHI )\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "import tensorflow.keras.backend as K\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model,Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPool1D,Flatten, Reshape, LSTM,Input,concatenate,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from lightgbm import LGBMRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#  \n",
    "train = pd.read_csv('./train/train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "#1. DATA\n",
    "\n",
    "# GHI column  \n",
    "def Add_features(data):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "# train data column\n",
    "def preprocess_data(data, is_train=True):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "    temp = data.copy()\n",
    "    temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "    if is_train==True:          \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')   #  Target\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill') #  Target\n",
    "        temp = temp.dropna()    #  \n",
    "        return temp.iloc[:-96]  #   . (   )\n",
    "\n",
    "    elif is_train == False:\n",
    "        temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "        return temp.iloc[-48:, :]\n",
    "\n",
    "df_train = preprocess_data(train)\n",
    "# print(df_train.shape)   # (52464, 10)\n",
    "# print(df_train[:48])\n",
    "\n",
    "#-------------test \n",
    "df_test = []\n",
    "for i in range(81):\n",
    "    file_path = 'test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    df_test.append(temp)\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "test = np.array(df_test)\n",
    "print(df_test.shape) # (3888, 8)\n",
    "x_test = df_test.to_numpy()\n",
    "\n",
    "###=====================================================================\n",
    "#  () --> y1, y2    \n",
    "def split_xy(dataset, time_steps) :\n",
    "    x, y1, y2 = [],[],[]\n",
    "    for i in range(len(dataset)) :\n",
    "        x_end = i + time_steps\n",
    "        if x_end > len(dataset) :\n",
    "            break\n",
    "        tmp_x = dataset[i:x_end, :-2] # ['Hour' ~ 'T']\n",
    "        tmp_y1 = dataset[x_end-1:x_end,-2] # Target1\n",
    "        tmp_y2 = dataset[x_end-1:x_end,-1]   # Target2\n",
    "        x.append(tmp_x)\n",
    "        y1.append(tmp_y1)\n",
    "        y2.append(tmp_y2)\n",
    "    return np.array(x), np.array(y1), np.array(y2)\n",
    "\n",
    "X = df_train.to_numpy()\n",
    "# print(X.shape)      # (52464, 10)\n",
    "x,y1,y2 = split_xy(X,1)\n",
    "print(x.shape, y1.shape, y2.shape) #(52464, 1, 8) (52464, 1) (52464, 1)\n",
    "\n",
    "#####===========\n",
    "x_train, x_val, y1_train, y1_val, y2_train, y2_val = train_test_split(x,y1,y2,\n",
    "                           train_size = 0.7,shuffle = False, random_state = 0)\n",
    "\n",
    "# print(x_train.shape,x_val.shape) #(36724, 1, 8) (15740, 1, 8)\n",
    "print(y1_train.shape, y1_val.shape, y2_train.shape, y2_val.shape) #(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
    "x_train= x_train.reshape(36724*1, 10)\n",
    "x_val= x_val.reshape(15740*1, 10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "x_train= x_train.reshape(36724, 1, 10)\n",
    "x_val= x_val.reshape(15740, 1, 10)\n",
    "x_test= x_test.reshape(3888, 1, 10)\n",
    "# print(x_train.shape,x_val.shape,x_test.shape) #(36724, 1, 8) (15740, 1, 8) (3888, 1, 8)\n",
    "\n",
    "#  : Quantile loss definition\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "    err = (y_true - y_pred)\n",
    "    return K.mean(K.maximum(q*err, (q-1)*err), axis=-1)\n",
    "\n",
    "q = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "#2. \n",
    "\n",
    "def modeling() :\n",
    "    inputs = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    adam = Adam(learning_rate=0.003)\n",
    "    conv = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Flatten()(conv)\n",
    "\n",
    "    conv1 = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Flatten()(conv1)\n",
    "\n",
    "    conv2 = Conv1D(filters = 256,kernel_size=3,activation='relu',padding='same')(inputs)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 64,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 32,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Flatten()(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters = 256,kernel_size=1,activation='relu',padding='same')(inputs)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 64,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 32,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Flatten()(conv3)\n",
    "    \n",
    "    lstm = LSTM(256,activation='relu',return_sequences=True)(inputs)\n",
    "    lstm = LSTM(64,activation='relu',return_sequences=True)(lstm)\n",
    "    lstm = LSTM(16,activation='relu')(lstm)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    concat = concatenate([conv,conv1,conv2,conv3,lstm])\n",
    "    dense = Dense(128,activation='relu')(concat)\n",
    "    dense = Dense(64,activation='relu')(dense)\n",
    "    dense = Dense(32,activation='relu')(dense)\n",
    "    dense = Dense(8,activation='relu')(dense)\n",
    "    outputs = Dense(1,activation='relu')(dense)\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model\n",
    "# 3. , \n",
    "#####========,  , predict\n",
    "#y1, y2  \n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 30)\n",
    "lr = ReduceLROnPlateau(monitor = 'val_loss', patience = 20, factor = 0.9, verbose = 1)\n",
    "epochs = 500\n",
    "bs = 16\n",
    "\n",
    "# !!\n",
    "x = []\n",
    "for i in q:\n",
    "    if q!=0.5:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models3/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = lambda y_true,y_pred: quantile_loss(i,y_true,y_pred), optimizer = 'adam', metrics = [lambda y,y_pred: quantile_loss(i,y,y_pred)])\n",
    "        model.fit(x_train,y1_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y1_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "    else:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models3/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = 'mae', optimizer = 'adam')\n",
    "        model.fit(x_train,y1_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y1_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "df_temp1 = pd.concat(x, axis = 1)\n",
    "df_temp1[df_temp1<0] = 0\n",
    "num_temp1 = df_temp1.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = num_temp1\n",
    "\n",
    "x = []\n",
    "# !!\n",
    "for i in q:\n",
    "    if q!=0.5:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models3/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = lambda y_true,y_pred: quantile_loss(i,y_true,y_pred), optimizer = 'adam', metrics = [lambda y,y_pred: quantile_loss(i,y,y_pred)])\n",
    "        model.fit(x_train,y2_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y2_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "    else:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models3/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = 'mae', optimizer = 'adam')\n",
    "        model.fit(x_train,y2_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y2_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "df_temp2 = pd.concat(x, axis = 1)\n",
    "df_temp2[df_temp2<0] = 0\n",
    "num_temp2 = df_temp2.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = num_temp2\n",
    "\n",
    "##??????????????     ?????????   \n",
    "submission.to_csv('models3/upgrade_sub_0125__2.csv', index = False)\n",
    "##  135  -- 1.94653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ~~~!!!!! >o< ^o^\n",
    "###T-Td  (GHI )\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "import tensorflow.keras.backend as K\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model,Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPool1D,Flatten, Reshape, LSTM,Input,concatenate,BatchNormalization,LayerNormalization,GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from lightgbm import LGBMRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#  \n",
    "train = pd.read_csv('./train/train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "#1. DATA\n",
    "\n",
    "# GHI column  \n",
    "def Add_features(data):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "# train data column\n",
    "def preprocess_data(data, is_train=True):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "    temp = data.copy()\n",
    "    temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "    if is_train==True:          \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')   #  Target\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill') #  Target\n",
    "        temp = temp.dropna()    #  \n",
    "        return temp.iloc[:-96]  #   . (   )\n",
    "\n",
    "    elif is_train == False:\n",
    "        temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "        return temp.iloc[-48:, :]\n",
    "\n",
    "df_train = preprocess_data(train)\n",
    "# print(df_train.shape)   # (52464, 10)\n",
    "# print(df_train[:48])\n",
    "\n",
    "#-------------test \n",
    "df_test = []\n",
    "for i in range(81):\n",
    "    file_path = 'test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    df_test.append(temp)\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "test = np.array(df_test)\n",
    "print(df_test.shape) # (3888, 8)\n",
    "x_test = df_test.to_numpy()\n",
    "\n",
    "###=====================================================================\n",
    "#  () --> y1, y2    \n",
    "def split_xy(dataset, time_steps) :\n",
    "    x, y1, y2 = [],[],[]\n",
    "    for i in range(len(dataset)) :\n",
    "        x_end = i + time_steps\n",
    "        if x_end > len(dataset) :\n",
    "            break\n",
    "        tmp_x = dataset[i:x_end, :-2] # ['Hour' ~ 'T']\n",
    "        tmp_y1 = dataset[x_end-1:x_end,-2] # Target1\n",
    "        tmp_y2 = dataset[x_end-1:x_end,-1]   # Target2\n",
    "        x.append(tmp_x)\n",
    "        y1.append(tmp_y1)\n",
    "        y2.append(tmp_y2)\n",
    "    return np.array(x), np.array(y1), np.array(y2)\n",
    "\n",
    "X = df_train.to_numpy()\n",
    "# print(X.shape)      # (52464, 10)\n",
    "x,y1,y2 = split_xy(X,1)\n",
    "print(x.shape, y1.shape, y2.shape) #(52464, 1, 8) (52464, 1) (52464, 1)\n",
    "\n",
    "#####===========\n",
    "x_train, x_val, y1_train, y1_val, y2_train, y2_val = train_test_split(x,y1,y2,\n",
    "                           train_size = 0.7,shuffle = False, random_state = 0)\n",
    "\n",
    "# print(x_train.shape,x_val.shape) #(36724, 1, 8) (15740, 1, 8)\n",
    "print(y1_train.shape, y1_val.shape, y2_train.shape, y2_val.shape) #(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
    "x_train= x_train.reshape(36724*1, 10)\n",
    "x_val= x_val.reshape(15740*1, 10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "x_train= x_train.reshape(36724, 1, 10)\n",
    "x_val= x_val.reshape(15740, 1, 10)\n",
    "x_test= x_test.reshape(3888, 1, 10)\n",
    "# print(x_train.shape,x_val.shape,x_test.shape) #(36724, 1, 8) (15740, 1, 8) (3888, 1, 8)\n",
    "\n",
    "#  : Quantile loss definition\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "    err = (y_true - y_pred)\n",
    "    return K.mean(K.maximum(q*err, (q-1)*err), axis=-1)\n",
    "\n",
    "q = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "#2. \n",
    "\n",
    "def modeling() :\n",
    "    inputs = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    adam = Adam(learning_rate=0.003)\n",
    "    conv = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Flatten()(conv)\n",
    "\n",
    "    conv1 = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Flatten()(conv1)\n",
    "\n",
    "    conv2 = Conv1D(filters = 256,kernel_size=3,activation='relu',padding='same')(inputs)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 64,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 32,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Flatten()(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters = 256,kernel_size=1,activation='relu',padding='same')(inputs)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 64,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 32,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Flatten()(conv3)\n",
    "    \n",
    "    lstm = GRU(256,activation='relu')(inputs)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    concat = concatenate([conv,conv1,conv2,conv3,lstm])\n",
    "    dense = Dense(128,activation='relu')(concat)\n",
    "    dense = Dense(64,activation='relu')(dense)\n",
    "    dense = Dense(32,activation='relu')(dense)\n",
    "    dense = Dense(8,activation='relu')(dense)\n",
    "    outputs = Dense(1,activation='relu')(dense)\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# 3. , \n",
    "#####========,  , predict\n",
    "#y1, y2  \n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "lr = ReduceLROnPlateau(monitor = 'val_loss', patience = 5, factor = 0.9, verbose = 1)\n",
    "epochs = 200\n",
    "bs = 512\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = []\n",
    "for i in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "    filepath_cp = f'models3/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "    model = load_model(filepath_cp,compile=False)\n",
    "    pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "    x.append(pred)\n",
    "df_temp1 = pd.concat(x, axis = 1)\n",
    "df_temp1[df_temp1<0] = 0\n",
    "num_temp1 = df_temp1.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = num_temp1\n",
    "\n",
    "x = []\n",
    "# !!\n",
    "for i in q:\n",
    "    print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "    filepath_cp = f'models3/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "    model = load_model(filepath_cp,compile=False)\n",
    "    pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "    x.append(pred)\n",
    "df_temp2 = pd.concat(x, axis = 1)\n",
    "df_temp2[df_temp2<0] = 0\n",
    "num_temp2 = df_temp2.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = num_temp2\n",
    "submission.to_csv('models3/upgrade_sub_0125_.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ~~~!!!!! >o< ^o^\n",
    "###T-Td  (GHI )\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "import tensorflow.keras.backend as K\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model,Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPool1D,Flatten, Reshape, LSTM,Input,concatenate,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from lightgbm import LGBMRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#  \n",
    "train = pd.read_csv('./train/train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "#1. DATA\n",
    "\n",
    "# GHI column  \n",
    "def Add_features(data):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "# train data column\n",
    "def preprocess_data(data, is_train=True):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "    temp = data.copy()\n",
    "    temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "    if is_train==True:          \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')   #  Target\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill') #  Target\n",
    "        temp = temp.dropna()    #  \n",
    "        return temp.iloc[:-96]  #   . (   )\n",
    "\n",
    "    elif is_train == False:\n",
    "        temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "        return temp.iloc[-48:, :]\n",
    "\n",
    "df_train = preprocess_data(train)\n",
    "# print(df_train.shape)   # (52464, 10)\n",
    "# print(df_train[:48])\n",
    "\n",
    "#-------------test \n",
    "df_test = []\n",
    "for i in range(81):\n",
    "    file_path = 'test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    df_test.append(temp)\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "test = np.array(df_test)\n",
    "print(df_test.shape) # (3888, 8)\n",
    "x_test = df_test.to_numpy()\n",
    "\n",
    "###=====================================================================\n",
    "#  () --> y1, y2    \n",
    "def split_xy(dataset, time_steps) :\n",
    "    x, y1, y2 = [],[],[]\n",
    "    for i in range(len(dataset)) :\n",
    "        x_end = i + time_steps\n",
    "        if x_end > len(dataset) :\n",
    "            break\n",
    "        tmp_x = dataset[i:x_end, :-2] # ['Hour' ~ 'T']\n",
    "        tmp_y1 = dataset[x_end-1:x_end,-2] # Target1\n",
    "        tmp_y2 = dataset[x_end-1:x_end,-1]   # Target2\n",
    "        x.append(tmp_x)\n",
    "        y1.append(tmp_y1)\n",
    "        y2.append(tmp_y2)\n",
    "    return np.array(x), np.array(y1), np.array(y2)\n",
    "\n",
    "X = df_train.to_numpy()\n",
    "# print(X.shape)      # (52464, 10)\n",
    "x,y1,y2 = split_xy(X,1)\n",
    "print(x.shape, y1.shape, y2.shape) #(52464, 1, 8) (52464, 1) (52464, 1)\n",
    "\n",
    "#####===========\n",
    "x_train, x_val, y1_train, y1_val, y2_train, y2_val = train_test_split(x,y1,y2,\n",
    "                           train_size = 0.7,shuffle = False, random_state = 0)\n",
    "\n",
    "# print(x_train.shape,x_val.shape) #(36724, 1, 8) (15740, 1, 8)\n",
    "print(y1_train.shape, y1_val.shape, y2_train.shape, y2_val.shape) #(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
    "x_train= x_train.reshape(36724*1, 10)\n",
    "x_val= x_val.reshape(15740*1, 10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "x_train= x_train.reshape(36724, 1, 10)\n",
    "x_val= x_val.reshape(15740, 1, 10)\n",
    "x_test= x_test.reshape(3888, 1, 10)\n",
    "# print(x_train.shape,x_val.shape,x_test.shape) #(36724, 1, 8) (15740, 1, 8) (3888, 1, 8)\n",
    "\n",
    "#  : Quantile loss definition\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "    err = (y_true - y_pred)\n",
    "    return K.mean(K.maximum(q*err, (q-1)*err), axis=-1)\n",
    "\n",
    "q = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "#2. \n",
    "\n",
    "def modeling() :\n",
    "    inputs = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    adam = Adam(learning_rate=0.003)\n",
    "    conv = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Flatten()(conv)\n",
    "\n",
    "    conv1 = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Flatten()(conv1)\n",
    "\n",
    "    conv2 = Conv1D(filters = 256,kernel_size=3,activation='relu',padding='same')(inputs)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 64,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 32,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Flatten()(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters = 256,kernel_size=1,activation='relu',padding='same')(inputs)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 64,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 32,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Flatten()(conv3)\n",
    "    \n",
    "    lstm = LSTM(256,activation='relu',return_sequences=True)(inputs)\n",
    "    lstm = LSTM(64,activation='relu',return_sequences=True)(lstm)\n",
    "    lstm = LSTM(16,activation='relu')(lstm)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    \n",
    "    lstm1 = LSTM(256,activation='relu',return_sequences=True)(inputs)\n",
    "    lstm1 = LSTM(16,activation='relu')(lstm1)\n",
    "    lstm1 = LayerNormalization()(lstm1)\n",
    "    \n",
    "    lstm2 = LSTM(16,activation='relu')(inputs)\n",
    "    lstm2 = LayerNormalization()(lstm2)\n",
    "    \n",
    "    \n",
    "    concat = concatenate([conv,conv1,conv2,conv3,lstm,lstm1,lstm2])\n",
    "    dense = Dense(128,activation='relu')(concat)\n",
    "    dense = Dense(64,activation='relu')(dense)\n",
    "    dense = Dense(32,activation='relu')(dense)\n",
    "    dense = Dense(8,activation='relu')(dense)\n",
    "    outputs = Dense(1,activation='relu')(dense)\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model\n",
    "# 3. , \n",
    "#####========,  , predict\n",
    "#y1, y2  \n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 30)\n",
    "lr = ReduceLROnPlateau(monitor = 'val_loss', patience = 20, factor = 0.9, verbose = 1)\n",
    "epochs = 500\n",
    "bs = 16\n",
    "\n",
    "# !!\n",
    "x = []\n",
    "for i in q:\n",
    "    if q!=0.5:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models4/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = lambda y_true,y_pred: quantile_loss(i,y_true,y_pred), optimizer = 'adam', metrics = [lambda y,y_pred: quantile_loss(i,y,y_pred)])\n",
    "        model.fit(x_train,y1_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y1_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "    else:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models4/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = 'mae', optimizer = 'adam')\n",
    "        model.fit(x_train,y1_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y1_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "df_temp1 = pd.concat(x, axis = 1)\n",
    "df_temp1[df_temp1<0] = 0\n",
    "num_temp1 = df_temp1.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = num_temp1\n",
    "\n",
    "x = []\n",
    "# !!\n",
    "for i in q:\n",
    "    if q!=0.5:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models4/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = lambda y_true,y_pred: quantile_loss(i,y_true,y_pred), optimizer = 'adam', metrics = [lambda y,y_pred: quantile_loss(i,y,y_pred)])\n",
    "        model.fit(x_train,y2_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y2_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "    else:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models4/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = 'mae', optimizer = 'adam')\n",
    "        model.fit(x_train,y2_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y2_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "df_temp2 = pd.concat(x, axis = 1)\n",
    "df_temp2[df_temp2<0] = 0\n",
    "num_temp2 = df_temp2.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = num_temp2\n",
    "\n",
    "##??????????????     ?????????   \n",
    "submission.to_csv('models4/upgrade_sub_0125__2.csv', index = False)\n",
    "##  135  -- 1.94653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ~~~!!!!! >o< ^o^\n",
    "###T-Td  (GHI )\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "import tensorflow.keras.backend as K\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model,Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPool1D,Flatten, Reshape, LSTM,Input,concatenate,BatchNormalization,LayerNormalization,GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from lightgbm import LGBMRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#  \n",
    "train = pd.read_csv('./train/train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "#1. DATA\n",
    "\n",
    "# GHI column  \n",
    "def Add_features(data):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "# train data column\n",
    "def preprocess_data(data, is_train=True):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "    temp = data.copy()\n",
    "    temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "    if is_train==True:          \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')   #  Target\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill') #  Target\n",
    "        temp = temp.dropna()    #  \n",
    "        return temp.iloc[:-96]  #   . (   )\n",
    "\n",
    "    elif is_train == False:\n",
    "        temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "        return temp.iloc[-48:, :]\n",
    "\n",
    "df_train = preprocess_data(train)\n",
    "# print(df_train.shape)   # (52464, 10)\n",
    "# print(df_train[:48])\n",
    "\n",
    "#-------------test \n",
    "df_test = []\n",
    "for i in range(81):\n",
    "    file_path = 'test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    df_test.append(temp)\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "test = np.array(df_test)\n",
    "print(df_test.shape) # (3888, 8)\n",
    "x_test = df_test.to_numpy()\n",
    "\n",
    "###=====================================================================\n",
    "#  () --> y1, y2    \n",
    "def split_xy(dataset, time_steps) :\n",
    "    x, y1, y2 = [],[],[]\n",
    "    for i in range(len(dataset)) :\n",
    "        x_end = i + time_steps\n",
    "        if x_end > len(dataset) :\n",
    "            break\n",
    "        tmp_x = dataset[i:x_end, :-2] # ['Hour' ~ 'T']\n",
    "        tmp_y1 = dataset[x_end-1:x_end,-2] # Target1\n",
    "        tmp_y2 = dataset[x_end-1:x_end,-1]   # Target2\n",
    "        x.append(tmp_x)\n",
    "        y1.append(tmp_y1)\n",
    "        y2.append(tmp_y2)\n",
    "    return np.array(x), np.array(y1), np.array(y2)\n",
    "\n",
    "X = df_train.to_numpy()\n",
    "# print(X.shape)      # (52464, 10)\n",
    "x,y1,y2 = split_xy(X,1)\n",
    "print(x.shape, y1.shape, y2.shape) #(52464, 1, 8) (52464, 1) (52464, 1)\n",
    "\n",
    "#####===========\n",
    "x_train, x_val, y1_train, y1_val, y2_train, y2_val = train_test_split(x,y1,y2,\n",
    "                           train_size = 0.7,shuffle = False, random_state = 0)\n",
    "\n",
    "# print(x_train.shape,x_val.shape) #(36724, 1, 8) (15740, 1, 8)\n",
    "print(y1_train.shape, y1_val.shape, y2_train.shape, y2_val.shape) #(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
    "x_train= x_train.reshape(36724*1, 10)\n",
    "x_val= x_val.reshape(15740*1, 10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "x_train= x_train.reshape(36724, 1, 10)\n",
    "x_val= x_val.reshape(15740, 1, 10)\n",
    "x_test= x_test.reshape(3888, 1, 10)\n",
    "# print(x_train.shape,x_val.shape,x_test.shape) #(36724, 1, 8) (15740, 1, 8) (3888, 1, 8)\n",
    "\n",
    "#  : Quantile loss definition\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "    err = (y_true - y_pred)\n",
    "    return K.mean(K.maximum(q*err, (q-1)*err), axis=-1)\n",
    "\n",
    "q = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "#2. \n",
    "\n",
    "def modeling() :\n",
    "    inputs = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    adam = Adam(learning_rate=0.003)\n",
    "    conv = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Flatten()(conv)\n",
    "\n",
    "    conv1 = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Flatten()(conv1)\n",
    "\n",
    "    conv2 = Conv1D(filters = 256,kernel_size=3,activation='relu',padding='same')(inputs)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 64,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 32,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Flatten()(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters = 256,kernel_size=1,activation='relu',padding='same')(inputs)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 64,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 32,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Flatten()(conv3)\n",
    "    \n",
    "    lstm = GRU(256,activation='relu')(inputs)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    concat = concatenate([conv,conv1,conv2,conv3,lstm])\n",
    "    dense = Dense(128,activation='relu')(concat)\n",
    "    dense = Dense(64,activation='relu')(dense)\n",
    "    dense = Dense(32,activation='relu')(dense)\n",
    "    dense = Dense(8,activation='relu')(dense)\n",
    "    outputs = Dense(1,activation='relu')(dense)\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# 3. , \n",
    "#####========,  , predict\n",
    "#y1, y2  \n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "lr = ReduceLROnPlateau(monitor = 'val_loss', patience = 5, factor = 0.9, verbose = 1)\n",
    "epochs = 200\n",
    "bs = 512\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = []\n",
    "for i in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "    filepath_cp = f'models4/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "    model = load_model(filepath_cp,compile=False)\n",
    "    pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "    x.append(pred)\n",
    "df_temp1 = pd.concat(x, axis = 1)\n",
    "df_temp1[df_temp1<0] = 0\n",
    "num_temp1 = df_temp1.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = num_temp1\n",
    "\n",
    "x = []\n",
    "# !!\n",
    "for i in q:\n",
    "    print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "    filepath_cp = f'models4/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "    model = load_model(filepath_cp,compile=False)\n",
    "    pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "    x.append(pred)\n",
    "df_temp2 = pd.concat(x, axis = 1)\n",
    "df_temp2[df_temp2<0] = 0\n",
    "num_temp2 = df_temp2.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = num_temp2\n",
    "submission.to_csv('models4/upgrade_sub_0125_.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ~~~!!!!! >o< ^o^\n",
    "###T-Td  (GHI )\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "import tensorflow.keras.backend as K\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model,Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPool1D,Flatten, Reshape, LSTM,Input,concatenate,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from lightgbm import LGBMRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#  \n",
    "train = pd.read_csv('./train/train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "#1. DATA\n",
    "\n",
    "# GHI column  \n",
    "def Add_features(data):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "# train data column\n",
    "def preprocess_data(data, is_train=True):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "    temp = data.copy()\n",
    "    temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "    if is_train==True:          \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')   #  Target\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill') #  Target\n",
    "        temp = temp.dropna()    #  \n",
    "        return temp.iloc[:-96]  #   . (   )\n",
    "\n",
    "    elif is_train == False:\n",
    "        temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "        return temp.iloc[-48:, :]\n",
    "\n",
    "df_train = preprocess_data(train)\n",
    "# print(df_train.shape)   # (52464, 10)\n",
    "# print(df_train[:48])\n",
    "\n",
    "#-------------test \n",
    "df_test = []\n",
    "for i in range(81):\n",
    "    file_path = 'test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    df_test.append(temp)\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "test = np.array(df_test)\n",
    "print(df_test.shape) # (3888, 8)\n",
    "x_test = df_test.to_numpy()\n",
    "\n",
    "###=====================================================================\n",
    "#  () --> y1, y2    \n",
    "def split_xy(dataset, time_steps) :\n",
    "    x, y1, y2 = [],[],[]\n",
    "    for i in range(len(dataset)) :\n",
    "        x_end = i + time_steps\n",
    "        if x_end > len(dataset) :\n",
    "            break\n",
    "        tmp_x = dataset[i:x_end, :-2] # ['Hour' ~ 'T']\n",
    "        tmp_y1 = dataset[x_end-1:x_end,-2] # Target1\n",
    "        tmp_y2 = dataset[x_end-1:x_end,-1]   # Target2\n",
    "        x.append(tmp_x)\n",
    "        y1.append(tmp_y1)\n",
    "        y2.append(tmp_y2)\n",
    "    return np.array(x), np.array(y1), np.array(y2)\n",
    "\n",
    "X = df_train.to_numpy()\n",
    "# print(X.shape)      # (52464, 10)\n",
    "x,y1,y2 = split_xy(X,1)\n",
    "print(x.shape, y1.shape, y2.shape) #(52464, 1, 8) (52464, 1) (52464, 1)\n",
    "\n",
    "#####===========\n",
    "x_train, x_val, y1_train, y1_val, y2_train, y2_val = train_test_split(x,y1,y2,\n",
    "                           train_size = 0.7,shuffle = False, random_state = 0)\n",
    "\n",
    "# print(x_train.shape,x_val.shape) #(36724, 1, 8) (15740, 1, 8)\n",
    "print(y1_train.shape, y1_val.shape, y2_train.shape, y2_val.shape) #(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
    "x_train= x_train.reshape(36724*1, 10)\n",
    "x_val= x_val.reshape(15740*1, 10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "x_train= x_train.reshape(36724, 1, 10)\n",
    "x_val= x_val.reshape(15740, 1, 10)\n",
    "x_test= x_test.reshape(3888, 1, 10)\n",
    "# print(x_train.shape,x_val.shape,x_test.shape) #(36724, 1, 8) (15740, 1, 8) (3888, 1, 8)\n",
    "\n",
    "#  : Quantile loss definition\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "    err = (y_true - y_pred)\n",
    "    return K.mean(K.maximum(q*err, (q-1)*err), axis=-1)\n",
    "\n",
    "q = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "#2. \n",
    "\n",
    "def modeling() :\n",
    "    inputs = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    adam = Adam(learning_rate=0.003)\n",
    "    conv = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Flatten()(conv)\n",
    "\n",
    "    conv1 = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = LSTM(32,activation='relu')(conv1)\n",
    "\n",
    "    conv2 = Conv1D(filters = 256,kernel_size=3,activation='relu',padding='same')(inputs)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 64,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 32,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = LSTM(32,activation='relu')(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters = 256,kernel_size=1,activation='relu',padding='same')(inputs)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 64,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 32,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = LSTM(32,activation='relu')(conv3)\n",
    "    \n",
    "    lstm = LSTM(256,activation='relu',return_sequences=True)(inputs)\n",
    "    lstm = LSTM(64,activation='relu',return_sequences=True)(lstm)\n",
    "    lstm = LSTM(16,activation='relu')(lstm)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    \n",
    "    lstm1 = LSTM(256,activation='relu',return_sequences=True)(inputs)\n",
    "    lstm1 = LSTM(16,activation='relu')(lstm1)\n",
    "    lstm1 = LayerNormalization()(lstm1)\n",
    "    \n",
    "    lstm2 = LSTM(16,activation='relu')(inputs)\n",
    "    lstm2 = LayerNormalization()(lstm2)\n",
    "    \n",
    "    concat = concatenate([conv,conv1,conv2,conv3,lstm,lstm1,lstm2])\n",
    "    dense = Dense(128,activation='relu')(concat)\n",
    "    dense = Dense(64,activation='relu')(dense)\n",
    "    dense = Dense(32,activation='relu')(dense)\n",
    "    dense = Dense(8,activation='relu')(dense)\n",
    "    outputs = Dense(1,activation='relu')(dense)\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model\n",
    "# 3. , \n",
    "#####========,  , predict\n",
    "#y1, y2  \n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 30)\n",
    "lr = ReduceLROnPlateau(monitor = 'val_loss', patience = 20, factor = 0.9, verbose = 1)\n",
    "epochs = 500\n",
    "bs = 16\n",
    "\n",
    "# !!\n",
    "x = []\n",
    "for i in q:\n",
    "    if q!=0.5:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models5/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = lambda y_true,y_pred: quantile_loss(i,y_true,y_pred), optimizer = 'adam', metrics = [lambda y,y_pred: quantile_loss(i,y,y_pred)])\n",
    "        model.fit(x_train,y1_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y1_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "    else:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models5/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = 'mae', optimizer = 'adam')\n",
    "        model.fit(x_train,y1_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y1_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "df_temp1 = pd.concat(x, axis = 1)\n",
    "df_temp1[df_temp1<0] = 0\n",
    "num_temp1 = df_temp1.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = num_temp1\n",
    "\n",
    "x = []\n",
    "# !!\n",
    "for i in q:\n",
    "    if q!=0.5:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models5/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = lambda y_true,y_pred: quantile_loss(i,y_true,y_pred), optimizer = 'adam', metrics = [lambda y,y_pred: quantile_loss(i,y,y_pred)])\n",
    "        model.fit(x_train,y2_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y2_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "    else:\n",
    "        print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "        model = modeling()\n",
    "        filepath_cp = f'models5/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\n",
    "        model.compile(loss = 'mae', optimizer = 'adam')\n",
    "        model.fit(x_train,y2_train,epochs = epochs, batch_size = bs, validation_data = (x_val,y2_val),callbacks = [es,cp,lr])\n",
    "        pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "        x.append(pred)\n",
    "df_temp2 = pd.concat(x, axis = 1)\n",
    "df_temp2[df_temp2<0] = 0\n",
    "num_temp2 = df_temp2.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = num_temp2\n",
    "\n",
    "##??????????????     ?????????   \n",
    "submission.to_csv('models5/upgrade_sub_0125__2.csv', index = False)\n",
    "##  135  -- 1.94653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ~~~!!!!! >o< ^o^\n",
    "###T-Td  (GHI )\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "import tensorflow.keras.backend as K\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model,Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPool1D,Flatten, Reshape, LSTM,Input,concatenate,BatchNormalization,LayerNormalization,GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from lightgbm import LGBMRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#  \n",
    "train = pd.read_csv('./train/train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "#1. DATA\n",
    "\n",
    "# GHI column  \n",
    "def Add_features(data):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "# train data column\n",
    "def preprocess_data(data, is_train=True):\n",
    "    c = 243.12\n",
    "    b = 17.62\n",
    "    gamma = (b * (data['T']) / (c + (data['T']))) + np.log(data['RH'] / 100)\n",
    "    dp = ( c * gamma) / (b - gamma)\n",
    "    data.insert(1,'Td',dp)\n",
    "    data.insert(1,'T-Td',data['T']-data['Td'])\n",
    "    data.insert(1,'GHI',data['DNI']+data['DHI'])\n",
    "    sunup=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i,-1]\n",
    "        if temp>0.0:\n",
    "            sunup.append(1)\n",
    "        else :\n",
    "            sunup.append(0)\n",
    "    data['Sun_up']=sunup\n",
    "    # Time \n",
    "    Time=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = 60*data.iloc[i,1]+data.iloc[i,2]\n",
    "        Time.append(temp)\n",
    "    data['Time']=Time\n",
    "    temp = data.copy()\n",
    "    temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "    if is_train==True:          \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')   #  Target\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill') #  Target\n",
    "        temp = temp.dropna()    #  \n",
    "        return temp.iloc[:-96]  #   . (   )\n",
    "\n",
    "    elif is_train == False:\n",
    "        temp = temp[['Hour','TARGET','T-Td','DHI','DNI','WS','RH','T','Time','Sun_up']]\n",
    "\n",
    "        return temp.iloc[-48:, :]\n",
    "\n",
    "df_train = preprocess_data(train)\n",
    "# print(df_train.shape)   # (52464, 10)\n",
    "# print(df_train[:48])\n",
    "\n",
    "#-------------test \n",
    "df_test = []\n",
    "for i in range(81):\n",
    "    file_path = 'test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    df_test.append(temp)\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "test = np.array(df_test)\n",
    "print(df_test.shape) # (3888, 8)\n",
    "x_test = df_test.to_numpy()\n",
    "\n",
    "###=====================================================================\n",
    "#  () --> y1, y2    \n",
    "def split_xy(dataset, time_steps) :\n",
    "    x, y1, y2 = [],[],[]\n",
    "    for i in range(len(dataset)) :\n",
    "        x_end = i + time_steps\n",
    "        if x_end > len(dataset) :\n",
    "            break\n",
    "        tmp_x = dataset[i:x_end, :-2] # ['Hour' ~ 'T']\n",
    "        tmp_y1 = dataset[x_end-1:x_end,-2] # Target1\n",
    "        tmp_y2 = dataset[x_end-1:x_end,-1]   # Target2\n",
    "        x.append(tmp_x)\n",
    "        y1.append(tmp_y1)\n",
    "        y2.append(tmp_y2)\n",
    "    return np.array(x), np.array(y1), np.array(y2)\n",
    "\n",
    "X = df_train.to_numpy()\n",
    "# print(X.shape)      # (52464, 10)\n",
    "x,y1,y2 = split_xy(X,1)\n",
    "print(x.shape, y1.shape, y2.shape) #(52464, 1, 8) (52464, 1) (52464, 1)\n",
    "\n",
    "#####===========\n",
    "x_train, x_val, y1_train, y1_val, y2_train, y2_val = train_test_split(x,y1,y2,\n",
    "                           train_size = 0.7,shuffle = False, random_state = 0)\n",
    "\n",
    "# print(x_train.shape,x_val.shape) #(36724, 1, 8) (15740, 1, 8)\n",
    "print(y1_train.shape, y1_val.shape, y2_train.shape, y2_val.shape) #(36724, 1) (15740, 1) (36724, 1) (15740, 1)\n",
    "x_train= x_train.reshape(36724*1, 10)\n",
    "x_val= x_val.reshape(15740*1, 10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "x_train= x_train.reshape(36724, 1, 10)\n",
    "x_val= x_val.reshape(15740, 1, 10)\n",
    "x_test= x_test.reshape(3888, 1, 10)\n",
    "# print(x_train.shape,x_val.shape,x_test.shape) #(36724, 1, 8) (15740, 1, 8) (3888, 1, 8)\n",
    "\n",
    "#  : Quantile loss definition\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "    err = (y_true - y_pred)\n",
    "    return K.mean(K.maximum(q*err, (q-1)*err), axis=-1)\n",
    "\n",
    "q = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "#2. \n",
    "\n",
    "def modeling() :\n",
    "    inputs = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    adam = Adam(learning_rate=0.003)\n",
    "    conv = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Flatten()(conv)\n",
    "\n",
    "    conv1 = Conv1D(filters = 256,kernel_size=2,activation='relu',padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 128,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 64,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters = 32,kernel_size=2,activation='relu',padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Flatten()(conv1)\n",
    "\n",
    "    conv2 = Conv1D(filters = 256,kernel_size=3,activation='relu',padding='same')(inputs)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 128,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 64,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters = 32,kernel_size=3,activation='relu',padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Flatten()(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters = 256,kernel_size=1,activation='relu',padding='same')(inputs)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 128,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 64,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv1D(filters = 32,kernel_size=1,activation='relu',padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Flatten()(conv3)\n",
    "    \n",
    "    lstm = GRU(256,activation='relu')(inputs)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    concat = concatenate([conv,conv1,conv2,conv3,lstm])\n",
    "    dense = Dense(128,activation='relu')(concat)\n",
    "    dense = Dense(64,activation='relu')(dense)\n",
    "    dense = Dense(32,activation='relu')(dense)\n",
    "    dense = Dense(8,activation='relu')(dense)\n",
    "    outputs = Dense(1,activation='relu')(dense)\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# 3. , \n",
    "#####========,  , predict\n",
    "#y1, y2  \n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "lr = ReduceLROnPlateau(monitor = 'val_loss', patience = 5, factor = 0.9, verbose = 1)\n",
    "epochs = 200\n",
    "bs = 512\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = []\n",
    "for i in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "    filepath_cp = f'models5/dacon_y1_quantile_{i:.1f}.hdf5'\n",
    "    model = load_model(filepath_cp,compile=False)\n",
    "    pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "    x.append(pred)\n",
    "df_temp1 = pd.concat(x, axis = 1)\n",
    "df_temp1[df_temp1<0] = 0\n",
    "num_temp1 = df_temp1.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = num_temp1\n",
    "\n",
    "x = []\n",
    "# !!\n",
    "for i in q:\n",
    "    print(\"\\n\\n \",i,\"\\n\\n\")\n",
    "    filepath_cp = f'models5/dacon_y2_quantile_{i:.1f}.hdf5'\n",
    "    model = load_model(filepath_cp,compile=False)\n",
    "    pred = pd.DataFrame(model.predict(x_test).reshape(3888,1).round(2))\n",
    "    x.append(pred)\n",
    "df_temp2 = pd.concat(x, axis = 1)\n",
    "df_temp2[df_temp2<0] = 0\n",
    "num_temp2 = df_temp2.to_numpy()\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = num_temp2\n",
    "submission.to_csv('models5/upgrade_sub_0125_.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
